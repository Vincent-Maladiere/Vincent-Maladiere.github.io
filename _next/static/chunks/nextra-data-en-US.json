{"/about":{"title":"About","data":{"about-me#About me":"Hello and welcome to my website!I‚Äôm Vincent, a machine learning engineer with over five years of expertise spanning healthcare and fintech sectors.Olivier Grisel and I during our poster presentation at EuroScipy 2023Current endeavors:At Inria Saclay, I'm part of the Soda team, contributing to open-source libraries like scikit-learn, skrub, and hazardous. On scikit-learn, I focused on optimizing pairwise distance for sparse matrices, which resulted in a 20x speedup for algorithms like kNN and tSNE.I'm also a maintainer of skrub, a data wrangling project led by Ga√´l Varoquaux. Skrub mission is to enhance data preprocessing with machine learning, ‚Äúless data wrangling, more machine learning.‚ÄùResearch & Contributions:Together with Olivier Grisel, we created hazardous, aiming to make survival analysis and competing risks more accessible to the Python community. We then wrote the paper \"Teaching Models To Survive: Proper Scoring Rule and Stochastic Optimization with Competing Risks\" with Julie Albert.Speaking Engagements:I've been lucky enough to present our work at JupyterCon 2023, EuroScipy 2022 & 2023, and the Axa Dev Summit 2023, sharing insights on survival analysis and introducing skrub to a diverse audience.Educational Pursuits:I taught Python for Data Science at Polytechnique and survival analysis at Dauphine University, where I used Kaggle and Jupyterlite to ensure an interactive learning experience.Past Experiences:Before joining Inria, I was a ML engineer at Spendesk, a leading European startup. My projects ranged from transactions and revenue forecasting to real-time detection of duplicated transactions and churn probability estimation. I also co-founded Chasseur Particulier, a prominent flat-hunter agency in Paris, where I developed the backend scraper.Academic Credentials:I hold an MSc in Engineering from Grenoble Alpes University, INP Phelma, specializing in Signal Processing, Computer Science, and Machine Learning.Connect with Me:Feel free to engage with me on LinkedIn, Github, and Kaggle.üìù See my full Resume"}},"/apply-2022/accelerating-deployment-velocity":{"title":"15. Accelerating model deployment velocity, Emmanuel Ameisen, Stripe","data":{"":"https://www.youtube.com/watch?v=tClDQk7DqlY&ab_channel=Tecton\nSpeeding up the art of ML model deployment\nThe value of redeployed models\nModelling and eng part to ship it ‚áí what happens when we have new features, or drift, etc\nAny model that you train today will be obsolete tomorrow. By how much will it be obsolete?\nDomain shift\nBottleneck when it comes to production\nSkill set for regular ML deployments\nNeed to be 10x DS?\nvery operational work to bring value\nImproving model release processes\nautomate the majority of the pipeline\nanother trick to de-risk the pipeline is to leverage shadow mode\ndeploy our prod model and also shadow, to observe how it behaves in production\nSchedule it\n‚áí Stripe radar technical guide"}},"/apply-2022/applied-ml":{"title":"17. Engineering for applied ML, Yuchen Wu, Instacard","data":{"":"[Note: Super high-level discussion without slides, pretty hard to extract any insights at all from it, sorry ü§∑‚Äç‚ôÇÔ∏è]https://www.youtube.com/watch?v=F_RXAHsgbRE&ab_channel=Tecton\nML for a wide range of marketplace\nDelivery prediction: how fast someone can drive in a store, and gets those deliver\nForecasting: ensure our users can order whenever they want"}},"/apply-2022/data-observability":{"title":"22. Data observability for ML team, Kyle Kirwan, Bigeye","data":{"":"https://www.youtube.com/watch?v=GKpflt3fvKw&ab_channel=Tecton\nWhat is data observability?\nIt‚Äôs the ability to understand what‚Äôs going on with the data all along the pipeline, the data content itself, the dataset we‚Äôre working with\nIf we have an issue with click logs, the inference will break, data obs is about discovering this issue\nHow does it work? 3 ways to understand what‚Äôs going on\nmetadataquery logs, info schema ‚áí understand in our schema: did we mess with some columns that break our pipeline?can also be a problem with freshness, did we miss an update?\nmetricsdetecting duplicate data, format, null values, outliers, change in distributions\nlineagemap of dependency between transformations\nWhy should ML team care?Can use data obs to catch problems in our pipeline before breaking thingsex: car self-driving company, retrain their model (pretty expensive) and some metrics plummeted. Turns out that 10k images didn‚Äôt have any bounding box labels from a third party. Data obs allow identifying that\n3 categories to get started\nVendor: Bigeye\nOpen source: great_expectations\nBuild your own"}},"/apply-2022/customer-preference":{"title":"29. Intelligent Customer Preference Engine with Real-time ML systems, Manoj & Praveen, Walmart Lab","data":{"":"https://www.youtube.com/watch?v=WPKWRXU0bOQ&ab_channel=Tecton\nCustomers are increasingly omnichannelWe need to understand behaviour across the whole channelsPersonalization plays a key role throughout the customer journey, and also after the purchase in case of recommendationDozen of recommenders in play in the background, to make baskets very quickly. Adding each item can be time-consuming, so personalisation is right from the home page.Product discovery is another recommender use case.Aside from product impressions, we also prioritize non-product impressions like banners, to show that we‚Äôre to help find items they‚Äôre looking for.\n2 types of recommendations: baskets and checkout/last-call (candies, batteries etc).For last-call, we don‚Äôt want customers going back to the discovery again, but go ahead and checkout\nFlywheel\n1:1 micro intent: for a given time, for a given customerex of micro intent: Party supplies, then ice cream, then gift cardsdetecting micro-intent with the inference engine\nFeaturesVariable time windows for online predictions\nRanking using some contextIf a user already has an iPhone and search for AirPods, we‚Äôll propose Apple ones first.\nRecommender flow\nFeature and model storeModel store allows to A/B test and to iterate quickly on different models\nOnline inferencing platformMulti-model, DS can bring their framework and model on it and spin up a microservice easilyCan tie different models together in an ensemble fashionInference graph is very high-level: expose human-readable API to our clients. Most of the time, embedding and transformations are only understandable by the fews who built the model. Given a feature, it can bundle features together using the feature store.Post-processing is a filtering mechanism of the recall set, response to the clientReal-time platform is both async and real-time for different use-cases.\nQ&A\nDoes the feature store contain pre-computed features for online input? Or differents need to speed up inference.Yes precomputed. Feature store has many purposes: saving compute costs, storing final and intermediate features for reusability purposes."}},"/apply-2022/dagger-and-feast":{"title":"21. Feature engineering at scale with Dagger and Feast, Ravi Suhag, Gojek","data":{"":"https://www.youtube.com/watch?v=9B9qqqJVm4M&t=48s&ab_channel=Tectonüëâ¬†Slides\nFeature engineering: transforming raw data into high-quality model inputs and feature vectorPut it in a features store\nBetter features provide better flexibility for modelling and simpler models. also better models\nChallenges of features engineering\nInconsistency between training and servingEspecially in real-time, you want the same transformation to happen as with batches transformation\nDS don‚Äôt want to manage data pipelines, focus more on modelling\nScaling data infraHard for DS to manage Flink, Spark and Kubes cluster\nReal-time features need skilled data eng, to write more optimized jobs\nGoals\nUnify processing in real-time and batch. The logic needs to be the same. Self-service platform\nElastic infra, every time a DS adds a job, the infra can scale up\nStandard and reusable transformations, a standard way to define that\nNo extra skills required from DS to create real-time features\nDagger is our solution, on top of Flink for stateful processing of data\nPipeline with dagger and feast\nDagger architecture\nFeatures:\nSQL first: query, writing and templating queries\nFlexible, various ways to extend it using UDFs processor/post-processor\nStream enrichment from DB sources or API to bring context data for real-time processing\nSelf-service:\nDS shouldn‚Äôt be managing data pipelines\nonly specify their query and get the job running, and monitor it, have alerts, some templates\nManaging with GitOps\nCan specify deployment and version control with yaml\nSQL first\nNot turning complete, thus it will always complete, even if it fails, so won‚Äôt mobilize all the compute power in a data center\nUDFsUser define function can be used in SQL (like geohash)\nSELECT\n\tdata_location.longitude AS long\n\tdata_location.latitude AS lat\n\t**GeoHash**(\n\t\t**data_location.longitude,\n\t\tdata_location.latitude,\n\t\t6\n\t) AS geohashPickup,**\n\tTUMBLE_END(rowtime, INTERVAL '60' SECOND) AS window_timestamp\nFROM\n\tdata_stream\nGROUP BY\n\tTUMBLE(rowtime, INTERVAL '60' SECOND),\n\tdata_location.longitude,\n\tdata_location.latitude\nData maskingencryption on sensitive data, hashing\nSELECT\n\tevent_timestamp,\n\ttest_data\nFROM\n\tdata_stream\n******* Post processor config: ******\n{\n\t\"internal_source\": [{\n\t\t\"output_field\": \"*\",\n\t\t\"value\": \"*\",\n\t\t\"type\": \"sql\"}],\n\t\"transformers\": [{\n\t\t\"Transformation_class\": \"io.cdpf.dagger.functions.transformer\",\n\t\t**\"transformation_arguments\": {\n\t\t\t\"maskColumns\": [\"test_data.data1\"]**\n\t}}]\n\t\t\n}\nHybrid data sourcesManaging logic in 2 places are an issue, if you‚Äôre doing real-time streaming and batch processing with static and historical dataDagger allow to use data simultaneously from different sources\n[{\n\t\"INPUT_SCHEMA_TABLE\": \"booking_log_stream\",\n\t...\n\t\"SOURCE_DETAILS\": [\n\t\t{\"SOURCE_TYPE\": \"BOUNDED\", \"SOURCE_NAME\": \"PARQUET_SOURCE\"},\n\t\t{\"SOURCE_TYPE\": \"UNBOUNDED\", \"SOURCE_NAME\": \"KAFLA_SOURCE\"}\n]\n}]\nIn this logic, you can backfill historical data using streaming source, and join data across multiple streams\nStream enrichmentUse external data source (service endpoint, object stores, cache) to enrich real-time event and add context\n[{\"internal_source\": [{\n\t\"output_field\": \"booking_log\",\n\t\"type\": \"sql\",\n\t\"value\": \"*\"\n  }]},\n **{\"external_source\": {\n\t\t\"es\": [{\n\t\t\t\"host\": \"127.0.0.1\",\n\t\t\t\"port\": \"9200\",**\n\t\t\t\"endpoint_pattern\": \"/customers/customer/%s\",\n\t\t\t\"endpoint_variables\": \"customer_id\",\n\t\t\t\"stream_timeout\": \"5000\",\n\t\t\t\"output_mapping\": {\n\t\t\t\t\"customer_profile\": {\"path\": \"$._source\"}\n\t\t\t}\n\t}]\n }]\nStream inferenceAttach prediction to the same input event, in the streaming infraIn details\nEx: attach pricing prediction, for multiple consumers\nAll these predictions log are now in the warehouse, which every one can now use\nStreaming are very self-service from a DS point of view\nDagger adoption at Gojek\n300+ dagger jobs for feature engineering\n50+ data scientists creating dagger jobs\n10TB+ data processed every day\nDagger is part of data ops foundation, with an experience first approachDagger open source from 1 year\nQ&A\nHow feature eng can be viewed regarding autoML solutions?Not tackling this issue right now, but it can be explored\nDagger vs Spark?Dagger under the hood uses Fling, in batch modeDagger adds an abstraction layer for data scientists, allow them to have a more use friendly experience\nWhen ML Platform proposes new things, always some friction during user adoption. How did you approach 300 DS with dagger?Let make sure DS don‚Äôt need to learn new frameworkDon‚Äôt need to manage anything elseOnboarding process for DS to play around with their feature is shorter"}},"/apply-2022/declarative-ml-systems":{"title":"14. Declarative ML Systems and Ludwig, Pierre Molino & Travis Addair, Predibase","data":{"":"https://www.youtube.com/watch?v=74hqlj5k4Zg&ab_channel=Tecton\nOrganizations take inefficient ML approach\nEach project takes too long to bring value\nBespoke solution are hard to maintain and bring tech debt\nOrganization can‚Äôt hire enough ML engineers\nIntroducing declarative ML system\nhigher abstraction, ease of use\nopen the door to non experts for ML\nPioneer project with Ludwig (Uber) and Overton (Apple)\nHow does Ludwig works? a configuration system with yaml\nEnd to end deep learning architecture\nTask flexibility\nHow to scale this concept and work with bigger amount of data?\nScalable backend over Ray\nDoesn‚Äôt require you to provision heavy weighted infra, like a spark cluster, everything on the same layer\nPredibase on top of Ludwig:\nTake a look of the end-to-end problem of data flow in ML model to put it in production\nBoth batch and real-time production\nLow code\nWorkflow‚áí Check their paper about declarative ML"}},"/apply-2022/effective-stack":{"title":"11. How to draw an owl and build effective ML stacks, Sarah Catanzaro, Amplify","data":{"":"https://www.youtube.com/watch?v=wbExGrRBDvI&ab_channel=Tectonüëâ¬†Slides\nWe need a common language to describe the key elements of the ML stacks and workflows\n3 layers\nData management\nModel and deployment\nModel operations"}},"/apply-2022/faire-journey":{"title":"19. Faire‚Äôs journey toward modern data and ML stack, Daniele Perito, Faire","data":{"":"https://www.youtube.com/watch?v=hTR-YSrN60Q&ab_channel=Tecton\nFaire is a B2B marketplace helping independent retailers\nML to connect brands to retailers\nChallenges include: search discovery & ranking, risk & fraud, seller success (lead scoring), incentive optimization\nYear 1 (data team size: 1 ‚Üí 1)\nMain decisions\nOnline DB to use\nBI tool\nData warehouse\nEvent recording\nFirst real-time feature store for fraud\nData structure decisions can be very sticky, not easy to evaluate in the short term, a lot of critical and far-reaching decisionsMake sure to have cofounders or advisers that have worked with data systems\n2 choices of DB: MongoDB vs MySQLMongoDB pro: fast to set up and runMySQL pro: data replication, consistency\nYear 2 (data team size: 1 ‚Üí 3)\nMain decisions\nImproved search system\nKubeflow vs Airflow for simple orchestration\nFirst standardized core tables for analytics\nSo far, usage of GCP and Kubeflow seemed the most natural moveWhen scaling, decision making needs a more framework: create a rubric\nYear 3 (data team size: 3 ‚Üí 6)\nMain decisions\nUnify feature store\nMigration to Airflow\nData quality monitoring\nExperimentation platform\nEvents platform\nBuild / Buy experimentWe had a basic experiment system and we didn‚Äôt know what we were missing, so we tried to use an external solutionTook longer than expected and ultimately failed because of sync issues between our warehouse and our suppliers. Difficulty to map both data models.\nLearnings: there is a cost in maintaining data consistency with external providers. Also, make sure that data infra decisions are tailored to your need.\nYear 4 (data team size: 6 ‚Üí 12)\nMain decisions\nReal-time ranking and low latency inference\nMigration to Snowflake\nTerraform everywhere\nSnowflake migration from RedshiftsThousands of tables to migrate, very painful over 6 monthsBig data infra changes are hard, and necessitate a lot of coordination and project management to bring the whole company with youNo migration will be perfect, so set up an accuracy threshold to consider the job finished\nYear 5 (2021, data team size: 12 ‚Üí 30)\nMain decisions\nML platform unification\nAnalytics unification\nMake sure you have seasoned technical leaders to understand when it‚Äôs time to make big investmentsWithout feedback from DS and MLE and their workflow, you might not understand that there is a need to invest in analytics or ML platformThese DS projects are critical for the future\nSummary\nMake sure to have experimented folks in the early days\nClose collaboration between eng and data team during the early days is crucial\nEstablish rubrics for decision making\nQ&A\nA concrete example of a data decision rubric?Recently: does the data catalogue integrate with the BI tool? Can people follow tables to foster collaboration (‚Äùhey I notice you are an expert with this table‚Ä¶‚Äù)?\nWhat data are you using?Essentially customers‚Äô behaviour and interaction with the marketplace\nAnalytics platform vs ML platform?ML platform = unified feature store, unified ML training pipeline, model registry for later used, model serving (batch & real-time)Analytics platform = how are we adding decisions, and who are adding them? All of these things need to be defined at once, in a precise place. Every single fact of the business is defined once, and everyone is using it.\nReal-time feature store?How grown, ranking and fraud systemVery interested in a unified feature store and understanding when to make the switchFeature computation in real-time is a really hard topic, the cost of sync with an external provider is very real\nBiggest growth challenges?Hundred of small changes in the data analytics platform during internationalization, different currencies, when to make the conversion before computing the revenue?"}},"/apply-2022/effective-system-ml-dev":{"title":"10. Effective system ML development, Leonard Aukea, Volvo","data":{"":"https://www.youtube.com/watch?v=Tb_IKFvlFo8&ab_channel=Tectonüëâ¬†Slides\nML is (data-intensive) software: let‚Äôs not forget it\nUncertainty is a feature of ML, also sometimes a bug\nHarder to test: test model + test data\nSome low hanging fruits\ncare about system design\nNot being done properly in practice\nadopt a branching strategy\nreview process: code and analysis, ensure quality & distribute knowledge across the team\nwrite tests, statistical tests as well. adopt this mentality\ndocumentation is paramount, your approach, your analysis, and your codebase in general\nmonitoring and alerting, beware of silent errors\nautomation: learn how to use git properly to use CI/CD\nplan for disaster: prepare a disaster recovery plan. start simple and iterate on it\nQ&A\nElaborate on branching and review process?‚áí git flow is quite simple for branching strategy. build a solid foundation to collaborate. running integration needs to have a branching strategy.\n‚áí the review process needs different perspectives: some software dev and also senior MLE or DS\nmake it more interactive to look at plot / dist, and ensure quality. not merge unless it‚Äôs been reviewed‚áí we haven‚Äôt cared about these things at all in the ML sphere, we need to shape up\nwhat disaster ML technique have practitioners used?‚áí should be discussed during the design process, setting up requirements, and which scope it should function. running some type of stress testing to estimate the worst-case scenario in production, things that might be exposed to the user."}},"/apply-2022/diy-feature-store":{"title":"5. DIY minimal feature store, Jo√£o Santiago, Billie","data":{"":"https://www.youtube.com/watch?v=q4bZ0ixdUKk&ab_channel=Tecton\nRequirements\nEasily maintainable\nSame feature implementation during training and inference\nUse past data in real-time models\nImplement a feature once\nBeamter drawbacks\nCool but no real-time data caching\nSync versions between project\nUse Snowflake streams and tasks, define features as SQL function\nNo kafka, no spark, no new things to learn\nFivetran delays here are a bottleneck, will be replaced with some events using Kafka\nIn summary, you can build features store with 3 components: Snowflake, Lambdas and Redis\nGoing further in the discussion: unlocking our data with a feature store"}},"/apply-2022/feast":{"title":"25. Lessons learned from working on Feast, Willem Pienaar, Tecton","data":{"":"https://www.youtube.com/watch?v=o787CSFKaXU&ab_channel=Tecton\nA lot has happened last year with Feasthigh-quality contributions:\nhigh-performances feature serving\ndata quality and monitoring\nfeast UI\nWhich technology do you use for online feature store?Why Postgres/MySQL? Simplicity?\nWhich technology for your offline feature storeOperational DBs present again, it is curious\nAre operational DBs necessary for ML?Operational DBs are shrinking, analytic stacks are increasingly being used for production ML\nIs everyone using the modern data stack for production ML? What tools do you use for data transformationSpark and pandas dominate, but these are not modern stackGood representation of dbt with data warehouse\nIs dbt enough to do ML?dbt user always use another tool to transform data, important but not sufficientModern data stack is not yet enough for production ML, doesn't address yet:\nStreaming, still the domain of Spark and Fling\nOn demand feature engineer, on the fly\nHow do you implement Feast for online feature retrievalSurprising because python is slower than Java or Go, because understanding is easierUsers want to use Feast as a library, as opposed to deploying a new serviceEven with a service, users still prefer Python, usability is the main argument\nWhy are teams adopting python in prod?\nTeam are optimizing for delivery over performance\nPython is just writing another language (C/C++, Rust)\nPython ML ecosystem is vast and growing rapidly"}},"/apply-2022/home":{"title":"apply conf(2022)","data":{"":"Created: May 25, 2022 9:53 AMApply(conf) is a two-days online MLOps conference, focusing on engineering challenges like real-time systems and infrastructure scaling. Panelists are engineers sharing wisdom and tricks for building and maintaining ML platforms accross a wide range of use-cases.This guide gives a broad outlook of the state of MLOps in 2022. Even though the MLOps sphere has kept evolving quickly ever since, many tools and concepts introduced here rely on first-principles that will last.","day-1#Day 1":"Full recording (6h) üëâ¬†https://www.youtube.com/watch?v=jbnbjNkeBKk&t=170s&ab_channel=Tecton","day-2#Day 2":"Full recording (6h) üëâ¬†https://www.youtube.com/watch?v=wOqBj1hst3M&ab_channel=Tecton"}},"/apply-2022/is-production-rl-at-tipping-point":{"title":"13. Is production RL at a tipping point? Waleed Kadous, Anyscale","data":{"":"https://www.youtube.com/watch?v=ufdhVCj-tpg&ab_channel=Tecton\nThere‚Äôs a very standard way of doing supervised ML, but reinforcement learning (RL) challenges that\nResearch has shown that RL does super well on real-world tasks\nYet we rarely see RL in prod. Why? Sometimes we get freaked out by RL because it's new, but we‚Äôll show that it‚Äôs a natural extension.\nI‚Äôll give some tips and traps to watch out for\nUsing RLlib, popular open-source distributed library\nUnderstanding RL structure\nHow to escalate from Bandit\nDeployment will be discussed\nBandit\nYou have slots machines giving a payout based on an unknown proba. A good example if UI treatment: 5 different ways of showing UI. You don‚Äôt want to test them all uniformly\nThe challenge with Bandit in production is the explore/exploit tradeoff. How to balance it? Epsilon greedy algorithm is a tradeoff. 50% of the time you act random and 50% you maximize your gain based on your knowledge\nContextual bandit leverage metadata: is it sunny? A very natural extension of bandit. based on this user profile, and watch this episode last, I‚Äôll suggest this user watch this\nRL is bandit with states (or sequential)\nOrder of actions in chess\nA sequence of steps to a payout, how to distribute the reward to the last move?\nYou need a temporal credit assignment.\nThere is also a delay problem. What happens if the reward is delayed? Even more complex, like a sacrifice move in chess, negative reward in the short term, but a reward 50 moves later.\nIf we expand the state and action spaces\nInstead of 4 bandit machines, I have 32\nState-space grows exponentially\nFor so many real-world applications, I got a log (here‚Äôs where people clicked yesterday), learning RL policy, how to run it without experiment?\nWhen you move to offline RL, you are stuck with the historic.\nMulti-agent RL (MRL)\nHow do you share probability between different users?\nIs it a cooperative or competitive scenario?\nThe stock market is a sum of very small players\nMRL get way more complex\nHow do you model reward between all actors\nIs RL at a tipping point?\nAll companies use RL for recommenders, so it‚Äôs started to cross the boundary, but why not more popular? 4 factors before prod ready. Recent progress in each\nHuge amount of training: alpha go played 5 million games. Only huge tech can afford that. We started to see transfer learning, you don‚Äôt always have to start to scratch. Imitation learning mimics human behaviour.\nThe default implementation is online: it‚Äôs designed to run live. Changing a dynamic model in production is pretty scary. Hard to get data to train them. Offline learning can help.\nTemporal credit assignment: which action to reward? Contextual bandit is RL without the temporal credit assignment, limited but simple to deploy and start getting adopted\nLarge action and state space. Recently, high fidelity simulator, deep learning approach to learning the state space, embedding approaches to learn action space (candidate selection then ranking), offline learning doesn‚Äôt require relearning\n3 Common patterns in successful RL applications\na good simulator is 50% of the workrunning a lot of simulations at once using distributed RL (RLlib)batching by merging results for many experimentsgetting close with a simulator, then fine-tuning in real-worldex1: games are good simulators!ex2: markets. simulations don‚Äôt need to be perfect.\nlow temporality: do you really need temporal assignment?ex1: last played game + user profile (the contextual part)what if millions of users and hundred games? use embedding to reduce the dimensionality of users and embedding to find the game\noptimization: the next generation. Linear programming can be used with RL.RL is optimization in a data-driven way, does not require modelling, but many experiments. Obviously, it takes a lot more computation but often plug and play with optimization\n2 tips for production RL\nKeep it simple.\nstart with stateless, then add context and up forward.\nonline vs offline\nsmall discrete state and action spaces vs large and continuous\nsingle-agent vs multi-agent shared policy vs true multi-agent\nRLOps?\nWorkflow is different\nValidate? Update? Monitoring? Retrain when it's offline data? Real problems with RL in production\nConclusion: a tipping point in some areas, some early adopters.\nQ&A\nHow much training data does a simple recommender system need?‚áí Think about embeddings: you don‚Äôt need RL to build them. 10k examples can be enough (ideally 100k or 1m).\nContextual bandit is quite off the shelf now."}},"/apply-2022/lakehouse":{"title":"2. Lakehouse: a new kind of platform, Matei Zaharia, Databricks","data":{"":"https://www.youtube.com/watch?v=6j0MazSTLHg&ab_channel=Tectonüëâ¬†SlidesHistorically, Data warehouses weren‚Äôt designed for data science, high cost for huge datasets.2010: datalakes: non tabular data, store all raw data with a single method. Open data format like Parquet accessible directly by DSProblem with 2 tier architecture: cheap to store all data but complex\nData reliability suffers from multiple storage systems, different ETL jobs\nTimeliness suffers from extra steps before data can be used in warehouses\nHigh cost from parallelisation and duplication\nKey technology enabling lakehouse:","1-metadata-for-data-lakes#1. metadata for data lakes":"track which files are part of table version to offer feature management (query files from a table)\nversions of files using delta lakes, to avoid crashing jobs when updating data\nall version of tables, you can time travel and stream changes","2-lakehouse-engine-designs-performant-sql#2. Lakehouse engine designs: performant SQL":"4 optimisation tricks:\nauxiliary data structure (statistiques, always consistant)For each parquet files, statistics like min, max year, uuid.\nWhen you read your snapshot of the table, you also read the statistics by using them during SQL query filtering\ncaching\nvectorisation (Databricks photon, using the Parquet format)\nNew query engines like databricks use these techniques","3-declarative-io-format-for-ml#3. Declarative I/O format for ML":"ML using Warehouse is painful because ML tools don‚Äôt query in SQL format, add new jobs for ETL\nML over Lakehouse can use Parquet, and spark can do query optimisation\nMLflow can also help with the ML lifecycle and data version trackingConclusion: Lakehouse combines the best of DWs and lakes","qa#Q&A":"Data quality?\nAutomatic test and table versioning\nEnd to end sanity checks on tables\nSnowpark vs Databricks?\nSnowpark is a proprietary APIs to run java and python workflows, can‚Äôt use existing Spark methods, Databricks support open-source API (PyTorch, Keras, XGBoost distributed)\nData mesh vs Data architecture platform: different teams can manage storage, easier to have a decentralised ownership of data"}},"/apply-2022/large-scale-recsys":{"title":"31. Training large scale recommendation models with TPU, Aymeric Damien, Snap","data":{"":"https://www.youtube.com/watch?v=iBfNfJHr9mE&ab_channel=Tecton\nSnap‚Äôs ad ranking environmentCheck if users engaged with ads or not, and generate labels based on thatRankers pretty straightforwardMore details: https://eng.snap.com/machine-learning-snap-ad-ranking\nChallengesThousand of featuresArchitecture: DCN, PLEHardware: training CPU + TPUv3, inference CPU or CPU + GPU (Nvidia T4)See: https://eng.snap.com/applying_gpu_to_snap\nChallenges of RecSys\nLot of categorical features: very large embeddings, so lookups are pretty memory intensive\nEmbedders have been little investigated by scholars, because of a lack of dataset, not as much focus as CV or NLP\nSpecialized hardware has focused on matrix operation but RecSys have specific unaddressed use cases: fast embeddings lookup, large memory, high memory bandwidth\nTPU integration and benchmark\nGoogle developed ASICS for DL, 4 generations (v5 in 2022)\nWhy for RecSys? Large memory, high bandwidth. Good embedding TPU API, fast and efficient lookups. Support sharding, embedding tables can be shared in different devices.\nBF16 matmul acceleration under the hood, unlike GPU where you need to specify type precisely.\nTensorflow compatible pipelines\nIntegration\nRefactor some TF code base (TPUEstimator, TPU distribute strategy), I/O pipeline optimization for high throughput\nChallenges: not as straightforward as using GPU. Optimizing GPU is easier. Some ops are not supported, need parameter tuning and some async & scaling issues for large amounts of cores\nBenchmarks: TPU over GPUcurrently working on benchmarking Nvidia's recent HugeCTR TF plugin, up to 7.8x speed up with their embedding optimisation compared to vanilla Tensorflow\nTPU scaling issuesSynchronous training and computer vision in mind (batch of same size) but not true for RecSys. Each embedding has different and many more lookups, and more computation, slowing down the whole pipeline\nA ton of new exciting ML hardware coming, leverage the graphic core to speed up matmul and improvement of memory, so good for RecSys.\nGoogle TPU v5\nIntel  SapphireRapid\nNvidia H100\nHabana Gaudi2"}},"/apply-2022/managing-the-flywheel":{"title":"1. Managing the flywheel, Mike del Balso, Tecton","data":{"":"https://www.youtube.com/watch?v=95p0JR6H-fM&ab_channel=Tectonüëâ¬†Slides"}},"/apply-2022/ml-meet-sql":{"title":"23. ML meet SQL, Dan Sullivan, 4 Mile Analytics","data":{"":"https://www.youtube.com/watch?v=T1StmzI0RbQ&ab_channel=Tecton\nBig query is helpful for serverless warehouseA lot of features for a data analytics platform: big query ML, BI engine, GIS, Omni (outside of Google Cloud, part of the Kubernetes ecosystem)\nBig query ML allow to create ML model in SQLLinear Regression, Matrix factorization, Boosting tree, Tensorflow, AutoML (params search and algo search for you with good performances)Hyperparams tuning is much easierNo need to export data, no need to be proficient with Python or Java\nCreate model\nCREATE MODEL `our_model`\nOPTIONS (\n\t(model_type='linear_reg',\n\tinput_label_cols=['weight_pounds']\n) AS\nSELECT\n\tweight_pounds,\n  feature_1,\n\tfeature_2,\n  ...\nFROM big_query.dataset\nWHERE filter ...\nPredictions\nSELECT\n\tpredicted_weight_pounds\nFROM\n\tML.PREDICT(\n\t\tMODEL `our_model`,\n\t\t(\n\t\t\tSELECT\n\t\t\t\tis_male,\n\t\t\t\tgestation_weeks,\n\t\t\t\tmother_age,\n\t\t\t\t...\n\t\t\tFROM big_query.dataset\n\t\t\tWHERE filter ...\n\t\t)"}},"/apply-2022/model-cards":{"title":"34. Making model cards, Chris Albon, Wikimedia","data":{"":"https://www.youtube.com/watch?v=t4GMq7MC7Js&ab_channel=Tecton\n150 models in prod, language translation, topic detection\nGoverned by the community and open funded by users, so pretty careful where money is spent on\nWidely transparent: all code is public, ML team internal chat is public, tickets are publicNo black boxes, no secret sauce, have participation in it. We need a way for users to read and understand the models\nModel cards are a single source of truth for an ML modelpublic-facing, discuss motivation, training, its aim, how to get the code, the data etcimportant for transparency\nSpent a lot of time speaking to the community, researchers, what do they looking for in a tool like thisIf people on french Wikipedia don‚Äôt want to use a tool, they can turn it off.\nPOC was pretty straightforward\nLanguage agnostic topic detection\nTalk about how the model should be used, all the story behind this. The least technical part of the discussion at the intro.\nOne model card\nData card"}},"/apply-2022/monitoring":{"title":"24. Learning from monitoring more than 30 ML use-cases, Lina Weichbrodt,","data":{"":"https://www.youtube.com/watch?v=Ki0es3bLsG8&ab_channel=Tecton\nSoftware monitoring basicsDetect errors asap and prioritize the severity4 signals to watch:\nlatency\ntraffic\nerrors\nsaturation\nFocus on symptoms for end-users\nHigher complexity in ML systems than in traditional ones\nMonitoring motivation: silent failures can have a huge commercial impact\nUnit changes\nData loss when calling an external client\nAggressive post-processing filters applied, seasonality drift made data scarcer\nSymptoms-based monitoring: focusing on output first\nPriority 1DS asked ‚Äúcan I monitor evaluation metric in prod?‚Äù it depends on the target feedback loop latency.Focus on what stakeholders want to avoidML app needs trust: need to reassure business and product stakeholders\nPriority 2A lot of ML monitoring is done with evaluation metricsEvaluation metrics are often not available in real-time (Recall, FBeta score, etc)Monitoring metrics focus on detecting problems, not on evaluating data quality and are easier to implement than evaluation metricsMeasure response distribution and use simple stats tests (heuristics), or KS sample test, D1 distance, stability index\nPriority 3Useful to understand how input change\nDo I need an MLOps observability tool?\nfocus on monitoring and explainability with Aporia or Superwise\nmonitoring as part of a full-featured tool like Seldon or Sagemaker\nadd loggings to inference, display them on dashboard and create alerts\nyou often don‚Äôt need a new tool"}},"/apply-2022/online-prediction":{"title":"3. ML for online prediction, Chip Huyen, Claypot AI","data":{"":"https://www.youtube.com/watch?v=P5uBWGqzogs&ab_channel=Tectonüëâ¬†Slides","1-online-prediction#1. Online prediction":"Batch prediction compute periodically vs online prediction compute on demand.Batch is not adaptive, before request arrive. Ok for recommender system, but it is costly. A prediction for all possible request out there.Online prediction issue is latency, need a high throughput.\nDatabricks has spark streaming\nFeature service (or feature store) must\nConnect to different to different data sources and streaming sources (Kafka).\nFeature catalog: it also needs to store feature definition. It can either be in SQL or Df.\nNext requirement is feature computation, vectorisation. Databricks come from a batch computation approach, not adapted for real-time\nServing consistency Reuse the computation for later prediction or retrained the model. Feature is more like a datamart. Ensure the consistency of the feature during prediction. Hard to achieve consistency for features computed outside of the feature store.\nMonitoring: online feature must be tested and don‚Äôt break into pipelines","2-monitoring#2. Monitoring":"What to monitor? Business metrics. F1 accuracy, CTR. Bottleneck by label availability. Companies try to collect as much feedback as possible (with user feedback, a proxy for labels). Clicks are not a strong signal, purchases are but much sparser.Prediction and features are monitored instead: shifts in distribution decrease business metrics.\nHow to determine it that 2 distributions are different? (train vs production, or yesterday vs today, source vs target)\nCompare statistics ‚Üí distribution dependent, inconclusive\nUsing sample testing (K-S or MMD) ‚Üí generally don‚Äôt work with high dim\nNot all shifts are equal\nslow and steady vs fast\nspatial shift vs temporal shift\nspatial = desktop vs mobile phone shift\ntemporal = behaviour has changed\nChoosing the right window is hard\nCan merge shorter time window (hourly) into a bigger one (daily)\nAuto detect using RCA with various windows size\nMonitoring predictions\nLow dimensional, easy to do simple hypothesis test on\nChange in prediction dist means change in input dist (can also be due by new model rollout progressively)\nMonitoring features\nSanity checks with precomputed statistics (min, max, avg)\nBut costly and slow\nAlert fatigue (most expectation violation are benign)\nUpdate needed overtime","3-continual-learning#3. Continual learning":"","qa#Q&A":"Feature store for small teams?\nSimple feature: lambda computation\nComplex features: no feature store, integration heavy\nScaling serving with the number of model serving? And how to monitor drift?‚Üí Embeddings? prebuild, pretrained models?‚Üí Are you deploying these models separately or a single containers?B2B companies have a separate model for each customer, requires managing features and training"}},"/apply-2022/nlp-modeling-and-inference":{"title":"32. Streamlining NLP model creation and inference, Cary & Phillip, Primer.ai","data":{"":"https://www.youtube.com/watch?v=jBN_PTKcOmE&ab_channel=Tecton\nWhat does Primer do?\nAggregation tool helping analysts get data within the global news\nReal-time stream from social media, watch events that are developing quickly\nCustom entity extraction on top of news, a bespoke case of workflow and create a NLP solution for it\nChallenges for NLP company\nMostly ingesting streaming data: fast, high reliability, autoscale, cost-effective (GPU), flexible enough to support any model framework for DS.\nNo simple way to solve all requirements, a complex solution. Complexity can become overwhelming and a lot of tooling\nTraining at scale issue: cross-validation, large scale validation, integration with serving solution, a lot of complexity\nGoal: a single API to access hardware for DS not to worry about the infra (access to batches, streaming etc)A DS trained and dev a model, how to embed that into the pipeline? Doesn‚Äôt provision all the underlying pieces like Kafka or Redis\nKubernetes custom resources\ncan deploy training/serving resources alongside everything else\ndeclarative by design, extend API natively to new resources type\nhides ephemeral and intermediate resources in k8s from users, lower technical barrier to get into prod NLP\nDeploying the model\nAuthors simply state what they want instead of how to do it\nEasy training\nAutoML declarative solution with k8s, users just specify the data and check that it conforms to model type, they can get on to training\nInternally handle cross-validation, hyper-parameters tuning\nOutput includes a summary of performances from cross-validation and config file for serving\nOne pro of being declarative is to deploy in generic env"}},"/apply-2022/open-source-feature-store":{"title":"7. Extending Open Source feature Store, Joost & Thijs, Adyen","data":{"":"https://www.youtube.com/watch?v=Yidl2VmrgVs&ab_channel=Tecton\nUse cases\nFraud classification\nPredict best time to retry a failed payment\nDetect illegal activity on the platform\nOpen source feast-spark-offline-store and move it into the feast project\nVery custom solution\nIn house build package, in early versions Spark was supported but it became incompatible, hence the library.\nOffline features are mainly aggregations over day ranges\nA lot of internal ETL pipelines use PySpark\nSparkOfflineStore as the interface to query existing feature tables, following BigQuery and Redshift template"}},"/apply-2022/panel-1":{"title":"28. Panel: Alexander Ratner & Aparna Dhinakaran & Ketan Umare & Biswaroop Palit","data":{"":"https://www.youtube.com/watch?v=m-qcjCE7AHU\nWhat defines success for an ML team?‚áí We live in an unstructured and unlabelled data world. The capacity of the team to handle it. A baseline, a metric and a clear understanding aligned with the business team is the first thing to look for. First-mile thing: what is the status of getting the data, and shaping it well?These fundamentals sound boring but it‚Äôs what matters.‚áí The ML team velocity. How many shots and goals can this team can have to get ML system in prod. Is it 10 months of research? Shorter?How do you measure performances and quickly iterate? Successful product and eng teams try to get MVP quickly and have tools in place to iterate.\nWhat about the ML flywheel‚áí Tech is never good the first time, but you got to have something out and set up observability, with a clear line of ownership. Lots of parallels to the software world.\nWhich principles do successful ML teams adopt? Patterns?‚áí Focusing on data quality in terms of reliability, testing the data, and observability, are fundamental. ML team need to care about those. Pipeline complexity has increased, data transformation and processes have also increased. Automation around the data pipelines and ML model themselvesWe observe that stakeholder buy-in helps a lot with a successful ML team. Understand business KPIs in depth‚áí Always start with the simplest things.Ownership across the entire team: when DS ship a notebook and ML engineers need to rewrite it. Folks need to work together. Tricky to have a single owner.‚áí Uber vs Google approach: totally separated DS vs MLE teams and full single ownershipBoth teams need to own the success of the project.\nWhich successful ML pattern set team from the rest‚áí You can‚Äôt own everything. Collaboration needs to be the focus. Also, what is the process between these teams? Good relationships? How often do people meet, communicate and measure success?You shouldn‚Äôt rely on some out-of-the-loop expert knowledge for any aspects of data modelling or feature extraction. Instead, have this talent in your team day-to-day.Being very disciplined to not fall for the last shiny model, focus instead on data quality, causality and observability.When you start a mature software eng project, you think about maintainability, governance etc but data eng teams, surprisingly, don‚Äôt often take this approach. Avoid rushing to the solution before sketching some basic routines and long term objectives.Have a leaderboard, a metrics or it will fail. But team that are too reductive and glued to that number without having a product/business vision will also fail. You need both.\nWhat type of structure successful ML team have?‚áí A lot of individual ML teams, owning different features, and then there are also central ML teams, like platform. Central ML teams try to really understand their customers and their business stakes. Define who owns the infrastructure.‚áí ML platform is becoming the norm, we need to push them to be product team, and multidisciplinary. We need to mix MLE and DS with product teams.‚áí Teams need to be like application services, being able to communicate easily, like the RPC protocol, a simplified interface for everyone to interact (during human communication but also on implementing data project)\nHow do you ensure that your customers are really successful (if you‚Äôre a SaaS & data company)‚áí Successful teams tend to treat data as products themselves. A lot of implications in the cultural aspect of collaboration between teams. Data producer vs data consumer, a lot of clarity is driven regarding ownerships.We wrote a guide to share best practices. How to configure alerts and notifications to avoid alert fatigue, data quality best practices. We run every consumer through this guide, to arrive at better data quality."}},"/apply-2022/pytorch-tooling":{"title":"18. PyTorch‚Äôs next generation of data tooling, Donny Greenberg, PyTorch at Meta","data":{"":"https://www.youtube.com/watch?v=pAoV9rls1IY&ab_channel=Tecton\nShould we have data APIs at PyTorch?\nProblems in OSS AI data loading\nIntroducing Extensible Loading Standard\nData processing\nIntroducing Extensible Processing Standard"}},"/apply-2022/rapid-deployment-healthcare":{"title":"6. Enabling rapid model deployment in healthcare, Felix Brann, Vital","data":{"":"https://www.youtube.com/watch?v=HWD42AHrgZk&ab_channel=Tecton\nMotivating exemple: Wait time for patients.\nNo info except grabbing a nurse or a doctor\nML to help inform patients during their stay\nCold start problem: scarce data\nHospital expect accurate ML results from day one\nNo history when opening a new facility\nEmergency departments varies a lot\nData regime change (think Covid)\nSolution: a facility-agnostic model, predicting the wait percentile\nModel use raw feature and aggregated features at the facility level\nInstead of predicting in minutes, we predict in percentile of the historical wait CDF, of a given facility (see below that it can varies a lot for same percentile)\nImplementing using Tecton\nFeature service for all feature to normalize, indexed using a facility_id\nLambda periodically extract tecton features into Redshift\nRedshift is the data source"}},"/apply-2022/real-time-featurization":{"title":"33. Real-time, accuracy and lineage-aware featurization, Sarah Wooders, Sky (ralf)","data":{"":"https://www.youtube.com/watch?v=c4mTAMkq0N8&ab_channel=Tectonüëâ¬†Slides\nFeature store quick review\nThis talk is specifically about feature maintenance and how to keep features refreshed\nHow often to update data transformation? Infrequent batch vs streaming system in real-time?Unclear where to land for different apps\nA lot of features are not that important, might never be queried, power-law distribution on feature query. A waste of resources to keep them up-to-date\nConsider first the cost of feature maintenance and then how much quality (tradeoff)Improve the tradeoff by being smart about priorization\nWhat is data quality?Ratio of actual feature performance over the optimal/ideal featuresDefine feature-store regret = prediction error with actual feature - ideal feature\nIf able to measure the accuracy of models, able to approximate the regret:Can determine which feature leads to less regretLess important to update vs more important by tracking the cumulative regret overtime\nFor every feature-pair we track regret and take the key with the highest regret and update\nHigher accuracy for the same cost using this regret scheduling, different feature updates by timesteps, improving our tradeoffFreshness ‚â† accuracy, they are correlated but not equal so it is another reason to look at model error for feature quality\n‚Äúralf‚Äù is a declarative dataframe API for defining features, with fine-grained control over managing feature updates, built for ML operation (python/ray)\ntreat features as static dataframe\nincrement in real-time with upcoming event\neasy integration with existing ML system"}},"/apply-2022/real-time-ranking":{"title":"20. Cash App‚Äôs real-time ranking ML system, Meenal & Austin, Block Inc.","data":{"":"https://www.youtube.com/watch?v=8KsPtzj1wa8&ab_channel=Tecton\nCash App is an easy way to send and save money\nUse-cases\nUse-case\nRewards ranking\nSupport article ranking\nInfra\nThe intention is to take customer context during the sort phase, to make it more specific\nWe are going to focus on ranking\nRequirements\nEach event leads to n item being scored: show relevant reward when the user clicks on the ad\nCustomer-facing: fast and reliable, target is scoring 50 items < 100ms\nConfigurable system to support multi use-cases\nBasic ML system features\nA/B test support\nShadow mode to test in prod background\nLogging metrics and results\nOptions to work on a solution:\nA library: business logic is embedded in the use-case itselfpro: reduce latencycons: tricky for management\nSide-car: business logic run in a container along with the product servicesame pros and cons\nStandalone Service (Weaver) that the product service makes a call topro:\neasier to bootstrap\neasy to manage\neasy to onboard\ncons\navailability\nlatency\nArchitecture\nMultiple feature stores and model servers, depending on the use-case several are accessible to the user\nAlso caching metadata for making quick requests in an online environment\nChallenges\nFeature store service latencyimpacted by the backend (is it DynamoDB, Cache)\nModel servingimpacted by the network and model evaluation cost\nFanout problemparallel calls lead to a higher proba of more latency\nTesting and optimizations\nQuick and dirty solution: slow and unstableWe couldn‚Äôt scale, need to adapt all podes\nLater on, achieved stability\nLatency was optimized\nHardening the network pathLatency had spikes without obvious causes, turn out that the bottleneck was the egress gateway traffic flowJust avoided this bottleneck to bring latency down to 200ms‚áí You want to understand to whole network path\nServing from disk vs memorySwitching from DynamoDB to Redis (feature store support both), avg latency drop from 10ms to 5msReady critical when doing fane-out\nHedging request: reduce the impact of tail latencyIdea:\nsend a request\nif no response after a delay, send it again\nreturn the response that completes first while cancelling the other\nIf the system is overloaded, you might have unexpected bottlenecks\nImpact: 1% hedging request bring down the p99 by 20%\nThe issue with edging: latency can spiral out of control. a safety mechanism is needed to avoid these loops.\nWarming up the connection poolThe initial traffic needs to first connect, helpful to improve the stability of the latency\nConcurrency tuningTricky to find but is key to maximising performancesAre you doing mostly I/O or CPU? (multi-threading vs multi-processing)Here, removing threadpool reduced CPU usage by 5% and p99 latency by 5ms\nFurther enhancements\nMore efficient data structuresLess data to transmit and process, avoid duplication of keys or values\nProduce good enough results quickly as opposed to best results slowlyUse a timeout to drop scores that are too slow\nQ&A\nDid you replace DynamoDB with Redis or was it another layer?Replaced. We‚Äôre also thinking about having a caching layer\nRequest hedging: how do you determine the timeout?We monitor the latency of requests in the system so that we can chart an histogram.Chose the delay with the distribution.\nRedis vs DynamoDB operational cost (Redis is not hosted)?Yes, it required some additional investment"}},"/apply-2022/recsys-online-evaluation":{"title":"26. Evaluating RecSys in production, Federico, Tooso","data":{"":"https://www.youtube.com/watch?v=FfJKFbgdSSo\nIssue:RecSys are vital for the user, but classic evaluation metrics are not enoughGuessing when a recommendation is not relevant is tricky\nRecSys fails silently, with dramatic consequencesand pseudo-feedback is tricky\nDo your test capture these issues? Is there a way to evaluate RecSys?\nIntroducing behavioural testing\nDoesn‚Äôt focus on a single metrics\nInherent properties of uses cases, enforce input/output pairs\nSimilar items are symmetric\nComplementary items are asymmetric\nQuantify mistakes‚ÄúTerminator‚Äù is a much worse mistake than ‚ÄúGot Mail‚Äù as a recommendation\nSlice the dataImbalanced datapoint (power log distribution), good aggregate measure (like MRR) even if low represented data are worse\nSolution: RecList is a behavioural test at scaleMain abstractions\nRecDataset: wraps dataset\nRecModel: wrap predictions from models, allowing ‚Äúblack box‚Äù testing of APIs\nRecList: declarative style, made of RecTests\nfrom reclist.metrics.standard_metrics import statistics\nclass MyRecList(RecList):\n\t\n\t@rec_test(test_type=\"stats\")\n\tdef basic_stats(self):\n\t\treturn statistics(\n\t\t\tself._x_train,\n\t\t\tself._y_train,\n\t\t\tself._x_test,\n\t\t\tself._y_test,\n\t\t\tself._y_preds)\nLot of pre-made behavioural tests to use\nCan be use for research with a new model\nand also for prod system, using CI/CD (check if behavioral symmetry is verified)\nQ&A\nEnd game with reclist? a company?Keep it open-source, make it useful for research and also for production tool\nIs it for recommender system only?Based mainly on rec for now\nBehavioral testing of recommender systems with RecListhttps://arxiv.org/pdf/2111.09963.pdf"}},"/apply-2022/semantic-layers":{"title":"16. Semantic Layers & Feature stores, Drew Banin, dbt Labs","data":{"":"Semantic layers are in\nMain idea: define your dataset and metrics, map out their relationships, translate semantic query into SQL\nExample metrics: revenue per country, churn rate ‚Ä¶\nOne example\nin dbt:\nPrecision & consistency\nMany people, many teams but only one way to define revenue\nAvoid repeating work or copy-paste, and inconsistency can arise\nBridging the gap\nStandardisation: feature store for ML training and serving\nSemantic layers: feature store in the BI world, output for analytics\nDoing it once and correctly\nget reuse and consistency"}},"/apply-2022/signal-engineering":{"title":"8. Compass: Composable & Scalable Signal Engineering, Justin Young, Abnormal Security","data":{"":"https://www.youtube.com/watch?v=nyaZm04qWl8üëâ¬†Slides\nUse case: stop all email attacks\nNeed high recall\nHigh precision\nRapid iteration cycle, because attackers update their techniques\nMantra: move fast and don‚Äôt break things\nHow to build an ML platform?\nPrevent entanglement\nPrevent online / offline skew\nEntanglement\n‚ÄúChanging anything changes everything‚ÄùHard to hard signal and basically impossible to remove ones, with an effect anywhere later on.\nFunction composition: f(x) = h(x, g(x))Function composition are the primary responsibility of ML pipeline\nUnconstrained function composition is the root of entanglementTotal dependencies grow square with the total of signalsSo we need constrained function composition instead\nHow does the signal dag work in production?\n@compass.single_output_transformation(\"variable_len\")\ndef extract_len(input_variable: str) -> int:\n\treturn len(input_variable)\nKey features of dag:\nData structure append-only\nEasy to understand the pipeline: list dependencies and viz\nValidate properties of the pipeline: no cycle, unused branch, every signal produced exactly by one transformation\nPlatformize pipeline: propagate errors downstream, parallel execution of parallel Dags\nHow to execute this offline/online at scale?\nMost of the time offline and online transformation will use the same transformations\nKey takeaway: make sure your ML pipeline implements both a feature store and a function store!The system should be function centric"}},"/apply-2022/streaming":{"title":"9. Streaming is an implementation detail, Arjun Narayan, Materialize","data":{"":"https://www.youtube.com/watch?v=iR-iN2Mqi0A&ab_channel=Tectonüëâ¬†Slides\nStreaming is taking action on data points as they appear, without waiting for batch processing\nBut this isn‚Äôt just about going fast, it enables an entire range of product\nLet‚Äôs build a feature store!\nCredit card fraud: approve or deny the transaction in real-time\nOne account owner per account, with multiple accounts\nHypothesis: one a fraud is committed on an account, it will likely happen again\nFeature store objective: given an account ID, the feature store will return the number of verified fraudulent transactions against an account owner in the last 30 days, if too high the transaction is denied\nWhat does it look like?\nSome data processing in real-time, coupled with batch processing using schedulers\nA layer of caching to process rescheduling\n‚áí This is a lot of infrastructures: we‚Äôve built an entire database\nWhy stream processor is insufficient?\nHigh engineering expense\nSlow query: needs cache\nMissing primitive: joins have to be implemented manually and the state must be managed\nStreaming should be a feature\nStreaming is more like a B tree, but developers don‚Äôt think about them every day.\nMaterialize is a database powered by a stream processor, looks and feels like a regular database\nMaterialize\nYields incremental results, you create queries upfront and it runs it as data arrive\nKeep data in memory for fast indexing\nQuery and tail to fetch from the app\nA Feature store with a streaming DB?\nNo scheduling\nMaterialize update SQL views\nNo need for a separated caching layer\nsimply write SQL, without worrying about streaming"}},"/apply-2022/transformers":{"title":"30. Are transformers becoming the most impactful tech of the decade? Cl√©ment Delangue, Hugging Face","data":{"":"https://www.youtube.com/watch?v=tWaXLomb-zw&ab_channel=Tecton\nThink about Google usage: results from natural language search are much betterAuto completes on phone and emails are more and more accurate and usefulAuto-translate super usefulModeration for offensive contentCopilot feature on Github to go fastereven Uber ETA‚áí All powered by transformers and transfer learning architecture\nHow did that happen?2017: Attention is all you need, new architectures for ML2018: BERT, change the way to do ML.\nWhy did that happen?Compute power (affordable and available TPU and GPU) + large and open datasets + transfer learning (ability to fine-tune on smaller datasets)Transformers started to beat SOTA for every NLP task. Within a short period of time 69% ‚Üí 88% accuracy (2019), humans are lower than 88% accuracy.\nAmazing adoption of hugging face transformers\nMultiplication of models, being used by so many companiesNow hugging face vision is to become the github of ML1000+ ML model shared on the platform, 10k companies using usMore and more companies start with ML or transformers in mind\nIt is time to act on ethical AIThe most popular transformer model BERT is extremely biased when it comes to inference jobs per genderIt is the right time to vest heavily on AI ethicsModel cards for model reporting: a way to communicate about the biases of modelWhen practitioners use models the right way. Ex: hiring mustn‚Äôt be handled or filtered using BERT, because we know that it will be biased.\nAnother initiative: the data measurement toolAnalyse datasets and try to find limitations and biases, to make sure it is understood and to manage properly, to mitigate those biases.\nWe can make ML and transformers the most positive technology of the decade"}},"/apply-2022/what-engineers-should-know":{"title":"12. Panel: What engineers should know when building for data scientists?","data":{"":"https://www.youtube.com/watch?v=0lzTx5yEGRE&ab_channel=Tecton‚Äú‚áí‚Äù are different spoke person intervention\nHow are DS and Eng (SWE) different in problem-solving?‚áí DS is such an umbrella, hard to find 2 people to approach their work, stack and problem solving the same way\nAnalytics and ML engineer started popping out, depending on the scale and industry domain of the company\nPretty clear boundaries where my work and your work start/end. However sometimes you just explore a topic, seems more like a hacking thing together. In a perfect scenario, engineers are supporting a bunch of MLE or DS‚áí We‚Äôre in a data lifecycle, SWEs are data creators. They show DS insights to the end-users. there‚Äôs a key partnership and team effort.‚áí Coming from the UX side, the scale and resources of the organisation impact a lot what is available (more resources = more specialized). At Spotify, DS are insights generators.‚áí Cross func teams, PM + Backend + Frontend + Front. Hard for SWE to understand the experimental approach of DS. It‚Äôs a real adaptation of expectation, to face research uncertainty.\nMain challenges of working with SWE?‚áí SWE and DS think differently. Experimentation phase. SWE give end goal and give requirements, but on a DS side not as straightforward.\nWho‚Äôs responsible for models in prod?‚áí Every project has its own ownership and SREs are the main guard of everything.‚áí Realistically, clear chart of ownership. It‚Äôs nearly always a collaboration between DS and MLE to handle crisis‚áí As your increase complexity, pager duty becomes necessary. People at larger companies are responsible, accountable, consulted and then informed (RACI matrix). Create this matrix to draw boundaries of responsibilities, and see where gaps and bottlenecks are. Solve things early before conflicts.\nHow do you feel about productizing ML vs productizing software? Where do we go beyond DevOps?‚áí The major diff is data, larger chunks that make it different and act unpredictably‚áí 2 main diff: regular deployment process (one-off), and expectations of testing and coverage\nIs eng the path for most DS? Or should DS should focus where they should add the most value‚áí It comes down to data maturity. In startups, extra data eng skills are a superpower. Go back on the infra to support the DS model. In Spotify, this wouldn‚Äôt be the case, focus on the main thing adding value."}},"/apply-2022/why-is-ml-hard":{"title":"4. Why is ML hard? Tal Shaked, Snowflake","data":{"":"https://www.youtube.com/watch?v=ezqsNzwN0p4üëâ¬†SlidesSome history about ML at Google in 2005 and onwards"}},"/axa-2022":{"title":"Axa 2022","data":{"axa-summit-2023---skrub-prepping-tables-for-machine-learning#Axa Summit 2023 - skrub: Prepping tables for machine learning":"This talk is a 30‚Äô introduction to skrub given during the Axa Summit 2023, along with a scikit-learn ecosystem introduction by Fran√ßois Goupil.","slides#Slides":"full talkskrub talk","additional-materials#Additional materials":"Tabular data by Gael Varoquaux"}},"/contributing-scikit-learn/find-a-good-first-issue":{"title":"Find a good first issue","data":{"":"Now that you are all set, let‚Äôs find good first issues to start contributing!","1-docstring-numpydoc-validation#1. Docstring numpydoc validation":"https://github.com/scikit-learn/scikit-learn/issues/21350This issue is one of the easiest to get started, because it doesn‚Äôt require you to deep dive in the code. You will fix docstrings that don‚Äôt comply with the numpydoc format.Choose unformatted functions from the list (also double check comments on the conversation to make sure no-one is already tackling the function you have chosen) and comment on which one you have chosen.","2-estimator-_validate_params#2. Estimator _validate_params":"https://github.com/scikit-learn/scikit-learn/issues/23462I recommend it as a second issue, because it is a gentle start into coding. You will extend parameter validation for estimators.","3-stalled-pull-request#3. Stalled pull request":"Pull requests ¬∑ scikit-learn/scikit-learnOnce you have done some PR on the 2 issues above, you can try to tackle more involved pull request by looking for the ‚Äústalled‚Äù label on pull requests. It is basically PRs that require a bit more work to be merged, and often brings a lot of value.Other labels that you want to check are:\ngood first issue\nhelp wanted\neasy\nmoderate\nTo continue a stalled issue, you need to:\nCheck that it is still stalled:\nscikit-learn.org/stable/developers/contributing.html#stalled-pull-requests\nIf a contributor comments on an issue to say they are working on it, a pull request is expected within 2 weeks (new contributor) or 4 weeks (contributor or core dev), unless an larger time frame is explicitly given. Beyond that time, another contributor can take the issue and make a pull request for it. We encourage contributors to comment directly on the stalled or unclaimed issue to let community members know that they will be working on it.\nContinue from the contributor branch when possible\ngit pull upstream pull/<PULL_REQUEST_NUMBER>/head:<BRANCH_NAME>\ngit checkout <BRANCH_NAME>\nResolve eventual conflicts with the main branch\ngit fetch upstream\ngit merge upstream/main\nCreate a new PR indicating ‚Äúfollow-up‚Äù or ‚Äúsupersede‚Äù in the PR description. Also mention the original issue with #<ISSUE_NUMBER> in the description\nAdd original author into the change log entry, if any"}},"/contributing-scikit-learn/home":{"title":"Contributing to Scikit-Learn","data":{"":"Created: August 7, 2022 3:00 PMContributing for the first time to a rich package like scikit-learn can seem like an exciting but daunting task. In addition to a large codebase, you must get acquainted with the developer experience and review process to make a contribution.Finding a good first issue in itself is challenging! With many people all around the world wanting to be a part of it, finding a well defined issue to fix or a feature to develop is often not obvious.This tutorial is a collection of notes written while contributing to scikit-learn for the first time myself. These notes are a simple revamp of some existing documentation, plus some helpful material.I hope this minimalistic guide will help you in your journey :)","1-start-here-to-get-your-local-environment-running#1. Start here to get your local environment running":"Setup","2-how-to-contribute#2. How to contribute":"Find a good first issueContrib cheatsheet","3-develop-your-own-estimators#3. Develop your own estimators":"Dev estimators cheatsheetUtils cheatsheet","4-release#4. Release!":"Release"}},"/contributing-scikit-learn/contrib-checklist":{"title":"Contrib cheatsheet","data":{"":"Based on: https://scikit-learn.org/stable/developers/contributing.html#pull-request-checklist","1-dev-in-a-new-branch#1. Dev in a new branch":"When you have identify an issue to work on:\nPull the latest change from upstream\ngit pull upstream main\nCreate a new branch\ngit checkout -b your_branch\nCode!\nAdd tests","2-testing#2. Testing":"Add non-regressive tests\nRun pytest on changed files\nTest a single module: pytest sklearn/tests/test_common.py -v -k LogisticRegression\nDisplay variable on error: pytest -l\nRun pdb shell on error: pytest --pdb\nRun ipdb shell on error: pytest --pdbcls=IPython.terminal.debugger:TerminalPdb --capture no\nDoesn‚Äôt capture print: pytest -s\nCheck unittest coverage (at least 80%)\npip install pytest pytest-cov\npytest --cov sklearn path/to/tests_for_package\nRun static typing analysis with mypy sklearn\nUsing¬†#¬†type:¬†ignore¬†annotation can be a workaround for a few cases\nAuto-format code: black .\nMake sure the code is flake8 compatible: git diff upstream/main -u -- \"*.py\" | flake8 --diff\nProfilinghttps://scikit-learn.org/stable/developers/performance.html#performance-howto","3-make-sure-the-code-is-commented-and-well-documented#3. Make sure the code is commented and well documented.":"Add tutorial and relevant material\nBuild the doc locally and check its correctness\ncd doc\nmake html","4-once-your-tests-are-successful#4. Once your tests are successful":"Commit\ngit add my_file\ngit commit -m \"my new commit\"\nThis will run pre-commit on flake8, black and mypy.\nPush your new branch on your forked repo\ngit push -u origin my_branch\nFinally, create a pull request from your fork to the original repoChoose your branch and click on ContributeThis will prompt the followingClick on Create Pull Request","5-document-your-pull-request#5. Document your pull request":"","title#Title":"Choose a prefix for the title:\nMAINT¬†and¬†MNT: general project maintenance, e.g. private code edits that doesn‚Äôt impact users\nFEA: new feature\nEHN¬†and¬†ENH: improving an existing feature\nFIX: fixture\nDOC: documentation\nCI: continuous integration\nTST: tests\nPOC: proof of concept\nPERF:¬†EHN¬†of performance\nSometimes <FIX ‚ÄúPR_NAME‚Äù> is enough, but <FIX #PR_NUMBER> is never a good title","body#Body":"Add links to the related issue, e.g. closing #1234\nExplain what your PR introduces\nThe PR should substantiate the change: through benchmark of performance or example of usage\nTake part in maintenance and support"}},"/contributing-scikit-learn/dev-estimators":{"title":"Developing estimators","data":{"":"Based on: Developing scikit-learn estimatorsOr ‚ÄúHow to safely interact with Pipelines and model selection‚Äù","estimator-api#Estimator API":"","instantiation#Instantiation":"__init__ accepts model constraints, but must not accept training data (reserved for fitting)‚úÖ¬†Do:\nestimator = SVC(C=1)\n‚ùå¬†Don‚Äôt:\nestimator = SVC([[0, 1], [1, 2]])\nModel hyper parameters should have default values, so that the user can instantiate a model without passing any arguments\nEvery parameter should directly match an attribute, without additional logic to enable model_selection.‚úÖ¬†Do:\ndef __init__(self, p_1=1, p_2=2):\n\tself.p_1 = p_1\n\tself.p_2 = p_2\n‚ùå¬†Don‚Äôt\ndef __init__(self, p_1=1, p_2=2):\n\tif p_1 > 1:\n\t\tp_1 += 1\n   self.p_1 = p_1\n\t\t\n\tself.p_3 = p_2\nNo parameter validation in __init__, only in fit","fitting#Fitting":"For supervised\nestimator = estimator.fit(X, y)\nor for unsupervised\nestimator = estimator.fit(X)\nkwargs can be added, restricted to data dependent variableexemple: a precomputed matrix is data dependent, a tolerance criterion is not\nThe estimator holds no reference to X or y (exceptions for precomputed kernel where this data must be stored for use by the predict method)\nUse utils check_X_y to ensure that X and y length are consistent\nEven if the fit method doesn‚Äôt imply a target y, y=None must be set to enable pipelines\nThe method must return self for better usability with chained operations\ny_pred = SVC(C=1).fit(X_train, y_train).predict(X_test)\nFit must be idempotent, and any new call to fit overwrites the result of the previous call (exceptions when using warm_start=True strategy to speed-up next fit operations)\nNames of attributes created during fit must end with a trailing underscore: param_\nn_features_in_ keyword can be added to make input expectations explicit","predictor#Predictor":"For supervised or unsupervised:\nprediction = predictor.predict(X)\nClassification can also offer to quantify a prediction (without applying thresholding)\nprediction = predictor.predict_proba(X)","transformer#Transformer":"For transforming or filtering data, in a supervised or unsupervised way\nX_transformed = transformer.transform(X)\nWhen fitting and transforming can be implementing together more efficiently\nX_transformed = transformer.fit_transform(X)","rolling-your-own-estimator#Rolling your own estimator":"","back-bone#Back bone":"Test your estimator using check_estimator\nfrom sklearn.utils.estimator_checks import check_estimator\nfrom sklearn.svm import LinearSVC\ncheck_estimator(LinearSVC())  # passes\nYou can leverage the project template to get started with all the estimator required methods.\nYou can also use inheritance from BaseEstimator, ClassifierMixin or RegressorMixin ot significantly reduce the amount of boilerplate code, including:\nget_params: take no arguments, return fit parameters.use deep=True to return submodel parameters\nset_params: overwrite fit parameters\nbase.clone compatibility is enable with get_params\n_estimator_type must be \"classifier\" , \"regressor\" or \"clusterer\"This is automatic with ClassifierMixin,¬†RegressorMixin¬†or¬†ClusterMixin inheritance","specific-estimators#Specific estimators":"A classifier fit can accept y with integers or string values, with the following conversion:\nself.classes_, y = np.unique(y, return_inverse=True)\nA classifier predict method must return arrays containing class labels from classes_\ndef predict(self, X):\n    D = self.decision_function(X)\n    return self.classes_[np.argmax(D, axis=1)]\nIn linear models, coefficient are stored in coef_ and intercept in intercept_\nThe¬†sklearn.utils.multiclass¬†module contains useful functions for working with multiclass and multilabel problems.\nAlso, check tags to define your estimator capabilities","coding-guidelines#Coding guidelines":"How new code should be written for inclusion in scikit-learn and make review easier.","style#Style":"Format and indentation follows PEP8\nUse underscores to separate words in non class names:¬†n_samples¬†rather than¬†nsamples.\nAvoid multiple statements on one line. Prefer a line return after if/for\nUse relative imports for references inside scikit-learn.\nPlease don‚Äôt use¬†import¬†* in any case\ncode becomes hard to read\nno reference for static analysis tool to run\nUse the numpy docstring standard\nCheck the utils module for better integration and reusability"}},"/contributing-scikit-learn/release":{"title":"Release","data":{"":"Changing the version in VERSION.txt and CHANGES.rst\nUpdating tags\ngit fetch --prune --prune-tags --tags upstream\ngit tag x.y.z\ngit push upstream x.y.z\nCreate wheels and sdist\npython setup.py bdist_wheel sdist\nCreate a new env, install the wheel just created and run:\npytest --pyargs <package>\nto check that all tests still pass.\nUpload to Pypi\npip install twine -U\ntwine check dist/*\ntwine upload dist/* --verbose"}},"/euroscipy-2022":{"title":"Euroscipy 2022","data":{"euroscipy-2022---linear-algebra-and-optimization-demo#EuroScipy 2022 - Linear Algebra and Optimization Demo":"I made a tutorial about scipy linalg and optimization modules, which was first introduced during EuroScipy 2022 at Basel.vincent-maladiere.github.io/scipy-demoIt is a beginner-friendly tutorial, addressed to python programmers interested in getting to know more about some of the core operations in machine learning: matrix operations and optimizers.The tutorial is made interactive thanks to Jupyter Lite, no local setup is required. Because scripts run directly in the browser through WebAssembly and Piodyde, python packages are downloaded during the launch of the kernel and that might cause some latencies."}},"/contributing-scikit-learn/setup":{"title":"Setup","data":{"":"Based on: How to contributeThe major difference between tech companies and open source that in open source you don‚Äôt work on the main repository directly.\nwhy?Projects like scikit-learn have hundreds of pull request monthly and an equal number of branches from many different contributors. If people were directly developing on scikit-learn, it will end up with an intractable number of branches on the main repo, and we don‚Äôt want that.\nInstead, you must develop your branches on your local fork, before submitting your pull request (PR) from your fork to the main branch of the original repo:Let‚Äôs setup your local working environment.\nFork the original repository\nClone the project on your local machineOn your terminal:\ngit clone <Your Project URL>\nAdd the original scikit-learn upstream\ngit remote add upstream git@github.com:scikit-learn/scikit-learn.git\nCheck that git remote -v display the following:\norigin  git@github.com:YourLogin/scikit-learn.git (fetch)\norigin  git@github.com:YourLogin/scikit-learn.git (push)\nupstream        git@github.com:scikit-learn/scikit-learn.git (fetch)\nupstream        git@github.com:scikit-learn/scikit-learn.git (push)\nIt will allow you to pull the latest change from the original repo, while pushing your commits to your fork.\nInstall mamba from forgeMamba is a fast front-end from conda.Conda is useful because it links all dependencies to the same backend, whereas pip setup adhoc backends for each library.\nInstall mamba compilers\nmamba install compilers\nCreate and activate a mamba environment\nmamba create -n sk\nmamba activate sk\nInstall scikit dependencies\nmamba install cython scipy numpy joblib\nInstall test dependencies\nmamba install pytest pytest-cov flake8 mypy numpydoc black==22.3.0\nInstall pre-commit\npip install pre-commit\npre-commit install\nBuild and install sklearn locally\npip install --no-build-isolation -e . -v \nWhen developing Cython files, you need to recompile them by running this command before testing\nCheck your installation\npython -c \"import sklearn; sklearn.show_versions()\"\nIf the command is successful, go to the next tutorial to start contributing!"}},"/contributing-scikit-learn/utility-functions":{"title":"Utility Functions","data":{"":"Based on: Utilities for developer","assert#Assert":"validation.assert_all_finite: Throw an error if array contains NaNs or Infs.\n_testing.assert_allclose: quasi equality of arrays, using a tol parameter","formater--converter#Formater / converter":"validation.as_float_array: convert input to array of float\nvalidation.check_array:\ncheck that input is a 2D array. Sparse matrix and other dimensions can be optionally allowed\ncall assert_all_finite\nvalidation.check_X_y:\ncheck that X and y have consistent lengths\ncall check_array on X, column_or_1d on y (use multi_output=True for multilabel y)\nvalidation.indexable: check that input arrays have consistent length and can be sliced or indexed (useful for cross-validation)\ndo not use np.asanyarray¬†or¬†np.atleast_2d since it lets np.matrix through (different API from np.array)","random-state#Random state":"validation.check_random_state: randomness must be handle with np.random.RandomState only","estimators#Estimators":"validation.check_is_fitted: check that estimator has been fitted before calling predict or transform method\nvalidation.has_fit_parameter: check that the fit method has a given parameter\nall_estimators: return a list of all scikit-learn estimators\nmulticlass.type_of_target: return the type of the target y among continuous, continuous-multioutput, binary, multiclass, multiclass-multioutput, multilabel-indicator or unknown\nmulticlass.unique_labels: Extract an ordered array of unique labels, \"multiclass-multioutput\" input type not allowed.","warnings--exceptions#Warnings & Exceptions":"utils.deprecated: decorator to mark a function or class as deprecated\nexceptions.ConvergenceWarning: Custom warning to catch convergence problems (used in Lasso)"}},"/euroscipy-2023":{"title":"Euroscipy 2023","data":{"euroscipy-2023---introducing-hazardous#EuroScipy 2023 - Introducing hazardous":"Olivier Grisel and I introduced hazardous at EuroScipy 2023 Basel in our tutorial ‚ÄúPredictive survival analysis and competing risk modeling‚Äù.I also participated in the poster session to explain in more depth how our Gradient Boosting Incidence works and its benchmarks with SurvTRACE.TutorialPoster"}},"/euroscipy-2024":{"title":"Euroscipy 2024","data":{"euroscipy-2024---skrub-bringing-everything-into-the-model#EuroScipy 2024 - Skrub: Bringing everything into the model":"J√©rome Dock√®s and I presented the latest development and vision of\nskrub at EuroScipy 2024 Szczecin.When it comes to designing machine learning predictive models, it is reported that data scientists spend over 80% of their time      preparing data for machine learning algorithms.Currently, no automated solution exists to address this problem. The skrub Python library is here to alleviate some of the daily tasks of data scientists and offer an integration with the scikit-learn machine learning library.Our main message is that all the preprocessing and tedious dataframe code should be in the model, because:\nIt allows you fine tune hyper-parameters of your preprocessors\nIt significantly ease deploying and versioning ML models\nSee how: Slides"}},"/jupytercon-2023":{"title":"Jupytercon 2023","data":{"jupytercon-2023---predictive-survival-analysis-and-competing-risk-modeling#JupyterCon 2023 - Predictive Survival Analysis and Competing Risk Modeling":"See the poster of the talkThis tutorial is split into 2 notebooks:\nPart 1: we introduce the main metrics and estimators used in survival analysis, along with our new Gradient Boosted CIF model for competing events.\nPart 2: we detail the concept of sessionization: by transforming raw logs of activity, we can create labels for survival analysis in an implicit or heartbeat fashion. We showcase this feature engineering with both DuckDB and Polars.","repo-of-the-demo#Repo of the demo":"https://github.com/soda-inria/survival-analysis-benchmark"}},"/inria-2022":{"title":"Inria 2022","data":{"inria-2022---survival-analysis-lightening-talk#Inria 2022 - Survival Analysis Lightening Talk":"This is a lightning talk I gave at Inria‚Äôs Soda and Mind team in 2022 to introduce survival analysis in python and the results of the benchmarks that I realized with estimators based on scikit-learn.Survival analysis at scale"}},"/opencv-tutorial/2d-features-framework":{"title":"2D Features framework","data":{"":"https://docs.opencv.org/3.4/d9/d97/tutorial_table_of_content_features2d.htmlFeature detection and description are the basis for image recognition and detection. They are especially useful for simple tasks, when there is too few data to train a deep learning model or too little compute power.","features-detector#Features detector":"","harris-corner-detector#Harris Corner Detector":"https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html\nA feature should be uniquely recognizable, and can be edges, corners (aka interest points) blob (aka regions of interest)\nhigh variations in image gradient can be use to detect corners\nWe look for variation ofUsing Taylor decomposition, with  partial derivative\nWe look at the score with  eigenvalues of M\nA window with a high  is considered a corner\nIf¬†¬†and¬†¬†has some large positive value, then an edge is found.\nIf¬†¬†and¬†¬†have large positive values, then a corner is found.\nExact computation of the eigenvalues is computationally expensive, since it requires the computation of a¬†square root, hence the  equation\ndst = cv2.cornerHarris(gray, blockSize=2, apertureSize=3, k=0.04)\ndst_norm = np.empty(dst.shape, dtype=np.float32)\ncv.normalize(dst, dst_norm, alpha=0, beta=255, norm_type=cv.NORM_MINMAX)\ndst_norm_scaled = cv.convertScaleAbs(dst_norm)\n# Drawing a circle around corners\nfor i in range(dst_norm.shape[0]):\n    for j in range(dst_norm.shape[1]):\n        if int(dst_norm[i,j]) > thresh:\n            cv.circle(dst_norm_scaled, (j,i), 5, (0), 2)\n# or\n#result is dilated for marking the corners, not important\ndst = cv2.dilate(dst,None)\n# Threshold for an optimal value, it may vary depending on the image.\nimg[dst>0.01*dst.max()]=[0,0,255]","shi-tomasi#Shi-Tomasi":"Propose instead the score \ncorners = cv2.goodFeaturesToTrack(\n\tsrc_gray,\n\tmaxCorners,\n\tqualityLevel=0.01,\n\tminDistance=10,\n\tNone,\n  blockSize=3,\n\tgradientSize=3,\n\tuseHarrisDetector=False,\n\tk=0.04\n)\ncorners = np.int0(corners)\nfor i in corners:\n    x,y = i.ravel()\n    cv2.circle(img,(x,y),3,255,-1)\nThis function is more appropriate for tracking","corner-with-subpixel-accuracy#Corner with SubPixel Accuracy":"https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html#corner-with-subpixel-accuracy\nFind the corners with maximum accuracy\ndst = cv2.cornerHarris(gray,2,3,0.04)\ndst = cv2.dilate(dst,None)\nret, dst = cv2.threshold(dst,0.01*dst.max(),255,0)\ndst = np.uint8(dst)\n# find centroids\nret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)\n# define the criteria to stop and refine the corners\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\ncorners = cv2.cornerSubPix(gray,np.float32(centroids),(5,5),(-1,-1),criteria)\n# Now draw them\nres = np.hstack((centroids,corners))\nres = np.int0(res)\nimg[res[:,1],res[:,0]]=[0,0,255]\nimg[res[:,3],res[:,2]] = [0,255,0]\nFind the Harris Corners\nPass the centroids of these corners (There may be a bunch of pixels at a corner, we take their centroid) to refine them.\nHarris corners are marked in red pixels and refined corners are marked in green pixels. For this function, we have to define the criteria when to stop the iteration. We stop it after a specified number of iteration or a certain accuracy is achieved, whichever occurs first.\nWe also need to define the size of neighbourhood it would search for corners.","scale-invariance-feature-transform-sift#Scale Invariance Feature Transform: (SIFT)":"https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html#introduction-to-sift-scale-invariant-feature-transform\nHarris and Shi-Tomasi corner detectors are rotation invariant(an edge remains an edge) but not scale invariant (zooming on an edge so that you can't detect it)\nSIFT extract keypoints and compute its descriptor\nScale-space Extrema Detection\nTo detect larger corner we need larger windows\nLaplacian of Gaussian (LoG) acts as a blob detector which detects blobs in various sizes,   acts as a scaling parameter (large  to detect large corner)\nWe can find local maxima across the scale and space, to find potential keypoints at  at scale \nBut costly in compute, so use an approximation: Difference of Gaussians (DoG), consists in computing difference between two Gaussian blur with different \nWe compare at given pixel it to its 8 neighbours, along with previous and next scale. If its an local extrema,  is a potential keypoint\nKeypoint Localization\nNeed to remove both low contrast keypoints and edge keypoints so that only strong interests keypoints remain\nTaylor series expansion of scale space to get more accurate location of extrema\nIf the intensity of the extrema is below a contrastThreshold, the keypoint is discarded as low-contrast\nDoG has higher response for edges even if the candidate keypoint is not robust to small amounts of noise.\nSo similarly to Harris detection we compute the ratio between eigenvalues of hessian matrix.\nFor poorly defined peaks in the DoG function, the¬†principal curvature¬†across the edge would be much larger than the principal curvature along it\n and we reject \nOrientation Assignment\nAssign an orientation to each keypoint to achieve image rotation invariance, and improve stability of matching\nAn orientation histogram with 36 beans covering 360 degree is computed, weighted by gradient magnitude and gaussian circular windows with a scale of 1.5 the scale of keypoint\nWe consider all peak above 80% of the histogram to compute orientation\nKeypoint Descriptor\nOur goal is now to compute a descriptor for the local image region about each keypoint that is highly distinctive and invariant as possible to variations such as changes in viewpoint and illumination.\nWe split a 16x16 neighborhood into 16 sub-blocks of 4x4 size\nCreate 8 bin orientation histogram for each sub-block, that's a 128 bin descriptor vector in total\nThis vector is then normalized to unit length in order to enhance invariance to affine changes in illumination.\nImplementation\nsift = cv2.SIFT()\nkp = sift.detect(gray, None)\nimg = cv2.drawKeypoints(\n\t\tgray,\n\t\tkp,\n\t\tflags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n)\nor compute keypoints, descriptor  directly\nsift = cv2.SIFT()\nkp, des = sift.detectAndCompute(gray,None)\nKeypoints matching\nKeypoints between two images are matched by identifying their nearest neighbours\nIn some case, the second-closest match is very near to the first (due to noise)\nWe eliminate first/second ratio higher than 0.8, it eliminates 90% of false positive and only 5% of true positive","speed-up-robust-feature-surf#Speed-Up Robust Feature (SURF)":"https://docs.opencv.org/3.4/d7/d66/tutorial_feature_detection.htmlhttps://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html#surfhttps://medium.com/data-breach/introduction-to-surf-speeded-up-robust-features-c7396d6e7c4e\nComposed of feature extraction + feature description\nSIFT but faster\nFeature extraction\nInstead of computing Laplacian of Gaussian (LoG) using Difference of Gaussian (DoG) we approximate using box filter\nWe smooth the image and take gaussian second order derivative, and this is approximated via boxes below:\nInstead of computing the determinant of the Hessian Matrix\nWe multiply the image through the box approximation, yielding the following determinant, with \nScale-space are represented as pyramids, and instead of reducing the image size, we increase the filter size\nFor feature description, SURF uses Wavelet responses in horizontal and vertical direction (again, use of integral images makes things easier)\nOrientation assignment\nby computing Haar Wavelet on x and y direction in a scale dependant circular radius neighborhood of 6s of the detected keypoint\nIteratively compute the sum of vertical and horizontal wavelet response in the area, and change area orientation by \nThis orientation with the largest sum is our main one for the feature descriptor\nDescriptor components\nCreate window of size 20s centered around keypoint using its orientation\nSplit into 16 regions of size 5s each, and compute Haar wavelet in x (dx) and y (dy), filter size of 2s. Responses are weighted by Gaussian centered at keypoint.\nBuild our size 64 vector by computing the 4 components for each region (vs size 128 for SIFT, hence its speed)\nSURF is good at handling images with blurring and rotation, but not good at handling viewpoint change and illumination change.\n# Create SURF object. You can specify params here or later.\n# Here I set Hessian Threshold to 400\nsurf = cv2.SURF(400)\n# Find keypoints and descriptors directly\nkp, des = surf.detectAndCompute(img, None)\n# len(kp) gives 1199 points, too much to show in a picture.\n# We reduce it to some 50 to draw it on an image.\n# While matching, we may need all those features, but not now.\n# So we increase the Hessian Threshold.\n# We set it to some 50000. Remember, it is just for representing in picture.\n# In actual cases, it is better to have a value 300-500\nsurf.hessianThreshold = 50000\n# Again compute keypoints and check its number.\nkp, des = surf.detectAndCompute(img, None)\n# print(len(kp)) gives 47\nimg2 = cv2.drawKeypoints(img, kp, None, (255, 0, 0), 4)\nplt.imshow(img2)\nsurf.upright = True\nsetting upright True is even faster\n# Find size of descriptor\n>>> print(surf.descriptorSize())\n64\n# That means flag, \"extended\" is False.\n>>> surf.extended\n False\n# So we make it to True to get 128-dim descriptors.\n>>> surf.extended = True\n>>> kp, des = surf.detectAndCompute(img,None)\n>>> print(surf.descriptorSize())\n128\n>>> print(des.shape)\n(47, 128)","features-from-accelerated-segment-test-fast#Features from Accelerated Segment Test (FAST)":"https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_fast/py_fast.html\nPrevious feature detector aren't fast enough for real-time computation, like for SLAM (Simultaneous Localization and Mapping) mobile robot which have limited computational resources\nMethod\nChoose a pixel p, whose intensity is \nSelect a threshold value \nTake a circle of 16 pixels around p\np is a corner if there exists a set of  contiguous point whose intensity are below  or above . Let's choose \nHigh speed test by looking only at pixels  and , then  and , then full segment criterion (in order to eliminate candidate and process faster)\nShortfalls\nDoes reject as many candidates for \nEfficiency of the pixels choice depends on distribution of corner appearance and questions ordering\nResults of high speed test are thrown away\nMultiple features are detected adjacent to each other\nMachine learning (shortfall 1-3)\nSelect a set of images for training\nRun FAST algorithm on every images to find feature points\nFor every feature point, store the 16 pixels surrounding as a vector. It gives a vector  for all images\nEach pixel x can be in either state\nSelect x to split the vector into  and \nDefine a new boolean variable , True if  is a corner, False otherwise\nUse a ID3 algorithm (decision tree classifier) to find the next pixel number (among 16) until the entropy is 0\nNon maximal suppression (shortfall 4)\nCompute  for all detected feature points\nConsider 2 adjacent keypoints, keep the one with the highest \nStill shortfalls\nNot robust to high level of noise\nDependent on a threshold\nImplementation\n# Initiate FAST object with default values\nfast = cv2.FastFeatureDetector_create()\n# find and draw the keypoints\nkp = fast.detect(img, None)\nimg2 = cv2.drawKeypoints(img, kp, color=(255, 0, 0))\n# Print all default params\nprint \"Threshold: \", fast.getInt('threshold')\nprint \"nonmaxSuppression: \", fast.getBool('nonmaxSuppression')\nprint \"neighborhood: \", fast.getInt('type')\nprint \"Total Keypoints with nonmaxSuppression: \", len(kp)\ncv2.imwrite('fast_true.png', img2)\nWith and without non max suppression","features-descriptor#Features descriptor":"","binary-robust-independent-elementary-features-brief#Binary Robust Independent Elementary Features (BRIEF)":"https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_brief/py_brief.htmlhttps://medium.com/data-breach/introduction-to-brief-binary-robust-independent-elementary-features-436f4a31a0e6\nAdvantages\nSmall number of intensity difference tests to represent an image (low memory consumption)\nBulding and matching is faster\nHigher recognition rates (but not rotational invariant)\nFeature descriptors encode detected keypoints, by converting patch of the pixel neighborhood into binary feature vectors (128‚Äì512 bits)\nAs it focus on the pixel level, BRIEF is super noise sensitive, so Gaussian smoothing is needed\nWe need to define a set of  pairs within the patch of each keypoint, using of of the below sampling geometries\nOur BRIF descriptor is with  and  the pixel intensity\nImplementation\ntraining_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n# Convert the training image to gray scale\ntraining_gray = cv2.cvtColor(training_image, cv2.COLOR_RGB2GRAY)\n# Create test image by adding Scale Invariance and Rotational Invariance\ntest_image = cv2.pyrDown(training_image)\ntest_image = cv2.pyrDown(test_image)\nnum_rows, num_cols = test_image.shape[:2]\nrotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), 30, 1)\ntest_image = cv2.warpAffine(test_image, rotation_matrix, (num_cols, num_rows))\ntest_gray = cv2.cvtColor(test_image, cv2.COLOR_RGB2GRAY)\nfast = cv2.FastFeatureDetector_create() \nbrief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\ntrain_keypoints = fast.detect(training_gray, None)\ntest_keypoints = fast.detect(test_gray, None)\ntrain_keypoints, train_descriptor = brief.compute(training_gray, train_keypoints)\ntest_keypoints, test_descriptor = brief.compute(test_gray, test_keypoints)\nkeypoints_without_size = np.copy(training_image)\nkeypoints_with_size = np.copy(training_image)\ncv2.drawKeypoints(training_image, train_keypoints, keypoints_without_size, color = (0, 255, 0))\ncv2.drawKeypoints(training_image, train_keypoints, keypoints_with_size, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n# Create a Brute Force Matcher object.\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n# Perform the matching between the BRIEF descriptors of the training image and the test image\nmatches = bf.match(train_descriptor, test_descriptor)\n# The matches with shorter distance are the ones we want.\nmatches = sorted(matches, key = lambda x : x.distance)\nresult = cv2.drawMatches(training_image, train_keypoints, test_gray, test_keypoints, matches, test_gray, flags = 2)","oriented-fast-and-rotated-brief-orb#Oriented FAST and rotated BRIEF (ORB)":"https://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cfhttps://docs.opencv.org/3.4/dc/d16/tutorial_akaze_tracking.htmlhttps://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_orb/py_orb.html#orb\nBeyond FAST\nGiven a pixel p in an array fast compares the brightness of p to surrounding 16 pixels that are in a small circle around p.\nPixels in the circle is then sorted into three classes (lighter than p, darker than p or similar to p). If more than 8 pixels are darker or brighter than p than it is selected as a keypoint\nHowever, FAST features do not have an orientation component and multiscale features\nORB is partial scale invariant, by detecting keypoints at each level of the multiscale image pyramid\nWe oriente keypoints by considering O (center of the corner) and C (centroid of the corner)\nMoments of the patch: \nCentroid: , and orientation \nWe can rotate patch to a canonical orientation to get descriptor, achieving rotational invariance\nBeyond BRIEF\nBrief takes all keypoints found by the FAST algorithm and convert it into a binary feature vector so that together they can represent an object\nHere we choose 128 pairs from a keypoint by sampling a Gaussian distribution around it, and then sampling the second pixel around the first pixel with a Gaussian distribution of scale \nHowever BRIEF is not invariant to rotation, so we use Rotation-aware BRIEF (rBRIEF), so ORB propose a method to steer BRIEF according to keypoints orientation\nFor  binary test at  the set of points is \nUsing the patch orientation , the rotation matrix is \nORB discretize the angle to increments of¬†(12 degrees), and construct a lookup table of precomputed BRIEF patterns\nGreedy search among all possible binary tests to find the ones that have both high variance and means close to 0.5, as well as being uncorrelated. The result is called¬†rBRIEF.\n# Initiate STAR detector\norb = cv2.ORB()\n# find the keypoints with ORB\nkp = orb.detect(img,None)\n# compute the descriptors with ORB\nkp, des = orb.compute(img, kp)\n# draw only keypoints location,not size and orientation\nimg2 = cv2.drawKeypoints(img,kp,color=(0,255,0), flags=0)\nplt.imshow(img2),plt.show()","feature-matcher#Feature Matcher":"https://docs.opencv.org/3.4/d5/d6f/tutorial_feature_flann_matcher.htmlhttps://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html#matcher\nClassical feature descriptors (SIFT, SURF) are usually compared and matched using the Euclidean distance (or L2-norm).\nSince SIFT and SURF descriptors represent the histogram of oriented gradient (of the Haar wavelet response for SURF) in a neighborhood, alternatives of the Euclidean distance are histogram-based metrics (¬†œá2, Earth Mover‚Äôs Distance (EMD), ...).\nBinary descriptors (ORB, BRIEF, BRISK) are matched using the¬†Hamming distance. This distance is equivalent to count the number of different elements for binary strings (population count after applying a XOR operation):\nTo filter the matches, use a distance ratio test to try to eliminate false matches.\nThe distance ratio between the two nearest matches of a considered keypoint is computed and it is a good match when this value is below a threshold.\nThis ratio allows helping to discriminate between ambiguous matches (distance ratio between the two nearest neighbors is close to one) and well discriminated matches.","brute-force#Brute force":"cv2.BFMatcher() takes 2 args\nnormType: use cv2.NORM_L2 for SIFT or SURF and use cv2.NORM_HAMMING for binary string descriptor (BRIEF, BRISK, ORB)\ncrosscheck False by default. If True, matcher ensure that 2 features in 2 different sets match each other. Consistent results, and good alternative to ratio test.\nThen we can either\nmatcher.match() : best match\nmatcher.knnMatch(): k best matches\norb = cv2.ORB()\n# find the keypoints and descriptors with ORB\nkp1, des1 = orb.detectAndCompute(img1, None)\nkp2, des2 = orb.detectAndCompute(img2, None)\n # create BFMatcher object\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n# Match descriptors.\nmatches = bf.match(des1,des2)\n# Sort them in the order of their distance.\nmatches = sorted(matches, key=lambda x:x.distance)\n# Draw first 10 matches.\nimg3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:10], flags=2)\nplt.imshow(img3)\nplt.show()","fast-library-for-approximate-nearest-neighbors-flann#Fast Library for Approximate Nearest Neighbors (FLANN)":"2 parameters\nIndexParams\nFor SIFT, SURF:\n    index_params = dict(\n        algorithm=FLANN_INDEX_KDTREE,\n        trees=5,\n)\nFor ORB:\nindex_params= dict(\n        algorithm=FLANN_INDEX_LSH,\n    table_number=6, # 12\n    key_size=12,     # 20\n    multi_probe_level=1  # 2\n)\nSearchParams: specifies the number of times the trees in the index should be recursively traversed.\nsearch_params = dict(checks=100)\nImplementation\nsift = cv2.SIFT()\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1,None)\nkp2, des2 = sift.detectAndCompute(img2,None)\n# FLANN parameters\nFLANN_INDEX_KDTREE = 0\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict(checks=50)   # or pass empty dictionary\nflann = cv2.FlannBasedMatcher(index_params,search_params)\nmatches = flann.knnMatch(des1,des2,k=2)\n# Need to draw only good matches, so create a mask\nmatchesMask = [[0,0] for i in xrange(len(matches))]\n# ratio test as per Lowe's paper\nfor i,(m,n) in enumerate(matches):\n    if m.distance < 0.7*n.distance:\n        matchesMask[i]=[1,0]\ndraw_params = dict(matchColor = (0,255,0),\n                    singlePointColor = (255,0,0),\n                    matchesMask = matchesMask,\n                    flags = 0)\nimg3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)\nplt.imshow(img3,)\nplt.show()","homography#Homography":"https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html#feature-homographyhttps://docs.opencv.org/3.4/d9/dab/tutorial_homography.html\nIf enough matches are found, we extract the locations of matched keypoints in both the images.\nThey are passed to findHomography to find the perpective transformation.\nOnce we get this 3x3 transformation matrix M, we use perspectiveTransform to transform the corners of queryImage to corresponding points in trainImage.\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nMIN_MATCH_COUNT = 10\nimg1 = cv2.imread('box.png',0)          # queryImage\nimg2 = cv2.imread('box_in_scene.png',0) # trainImage\n# Initiate SIFT detector\nsift = cv2.SIFT()\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1,None)\nkp2, des2 = sift.detectAndCompute(img2,None)\nFLANN_INDEX_KDTREE = 0\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict(checks = 50)\nflann = cv2.FlannBasedMatcher(index_params, search_params)\nmatches = flann.knnMatch(des1,des2,k=2)\n# store all the good matches as per Lowe's ratio test.\ngood = []\nfor m,n in matches:\n    if m.distance < 0.7*n.distance:\n        good.append(m)\nif len(good)>MIN_MATCH_COUNT:\n    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n    matchesMask = mask.ravel().tolist()\n    h,w = img1.shape\n    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n    dst = cv2.perspectiveTransform(pts,M)\n    img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\nelse:\n    print \"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT)\n    matchesMask = None\ndraw_params = dict(matchColor = (0,255,0), # draw matches in green color\n                   singlePointColor = None,\n                   matchesMask = matchesMask, # draw only inliers\n                   flags = 2)\nimg3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\nplt.imshow(img3, 'gray'),plt.show()","perspective-correction#Perspective correction":"ret1, corners1 = cv.findChessboardCorners(img1, patternSize)\nret2, corners2 = cv.findChessboardCorners(img2, patternSize)\nH, _ = cv.findHomography(corners1, corners2)\nprint(H)\nimg1_warp = cv.warpPerspective(img1, H, (img1.shape[1], img1.shape[0]))","panorama-stitching#Panorama stitching":"The homography transformation applies only for planar structure.\nBut in the case of a rotating camera (pure rotation around the camera axis of projection, no translation), an arbitrary world can be considered\nThe homography can then be computed using the rotation transformation and the camera intrinsic parameters\nR1 = c1Mo[0:3, 0:3]\nR2 = c2Mo[0:3, 0:3]\nR2 = R2.transpose()\nR_2to1 = np.dot(R1,R2)\nH = cameraMatrix.dot(R_2to1).dot(np.linalg.inv(cameraMatrix))\nH = H / H[2][2]\nimg_stitch = cv.warpPerspective(img2, H, (img2.shape[1]*2, img2.shape[0]))\nimg_stitch[0:img1.shape[0], 0:img1.shape[1]] = img1"}},"/opencv-tutorial/camera-calibration-and-3d-reconstruction":{"title":"Camera calibration and 3D Reconstruction","data":{"":"https://docs.opencv.org/4.5.0/d6/d55/tutorial_table_of_content_calib3d.htmlCamera calibration allows you to use two cameras to perform depth estimation through epipolar geometry. Its implementation and practical usage is still quite hacky, so you might prefer using a builtin stereo camera directly instead of a DIY version.Also, OpenCV has its takes on Pose estimation, but its usage is also quite tedious.Understanding theory is the main goal of this section.","camera-calibration#Camera calibration":"","theory#Theory":"https://docs.opencv.org/3.4.15/dc/dbb/tutorial_py_calibration.htmlCalibrate both cameras once, and store calibration parameters for depth estimation. If the geometry between cameras change, you need to perform calibration again.2 kinds of parameters:\ninternal: focal length, optical center, radial distortion coefficients of the lens\nexternal: orientation (rotation + translation) of camera wrt some world coordinates system\nCoordinates linked bythis can also be expressed asThe 3√ó1 translation vector is appended as a column at the end of the 3√ó3 rotation matrix to obtain a 3√ó4 matrix called the¬†Extrinsic Matrix.Homogeneous coordinatesIn homogenous coordinate  is the same as the point¬† in cartesian coordinatesIt allow us to represent infinite quantities using finite numbers, when Projection of a P point:\nHowever, pixels in image sensor may not be square, so we distinguish  and  (usually the same though)\nOptical center may not match the center of the image coordinate system, \n is the skew between axis (usually 0)\nThe intrinsic matrix K isGoal of camera calibration: find R, t and K","calibration-process#Calibration process":"Step 1: define real world coordsThe world coordinates is attached to the checkerboard. Since all the corner lie on a plan, we can arbitrarily choose  for each pointSince corners are equally spaced, the  of each point can be found easily from the chosen origin.Step 2: capture multiple images of the checkerboard from different viewpointsStep 3: Find 2D coordinates of checkerboardWe now have multiple images of the checkerboard. We also know the 3D location of points on the checkerboard in the world coordinates.3.a. Find checkerboard corners\n#retval, corners = cv2.findChessboardCorners(image, patternSize, flags)\nret, corners = cv2.findChessboardCorners(\n    gray,\n    (6, 9),\n    cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE\n)\n3.b. Refine checkerboard cornersTakes in the original image, and the location of corners, and looks for the best corner location inside a small neighborhood of the original location.The algorithm is iterative in nature and therefore we need to specify the termination criteria\n#cv2.cornerSubPix(image, corners, winSize, zeroZone, criteria)\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\ncorners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\nStep 4: Calibrate camera\nobjp = np.zeros((1, CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32)\nobjp[0,:,:2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\nobjectPoints = [objp]\nimagePoints = [corners2]\nretval, cameraMatrix, distCoeffs, rvecs, tvecs = cv2.calibrateCamera(\n    objectPoints, imagePoints, gray.shape[::-1], None, None)\n)\nWhat calibrateCamera performs:\nComputes the linear intrinsic parameters and considers the non-linear ones to zero.\nEstimates the initial camera pose (extrinsics) in function of the approximated intrinsics. This is done using¬†cv::solvePnP(...).\nPerforms the Levenberg-Marquardt optimization algorithm to minimize the re-projection error between the detected 2D image points and 2D projections of the 3D object points. This is done using¬†cv::projectPoints(...).\nStep 5: Undistortion\nRadial factorFor an undistorted pixel point at¬†(x,y)¬†coordinates, its position on the distorted image will be¬†(x_distorted, y_distorted)The presence of the radial distortion manifests in form of the \"barrel\" or \"fish-eye\" effect.\nTangent factorTangential distortion occurs because the image taking lenses are not perfectly parallel to the imaging plane\nNow for the unit conversion we use the following formula: are camera focal lengths and  are the optical centers expressed in pixels coordinates\nUndistort\ngetOptimalNewCameraMatrix to refine the intrinsic matrix based on a scaling parameter 0 < alpha < 1\nIf alpha = 0, it effectively increases the focal length to have a rectangular undistorted image where all pixels correspond to a pixel in the original. You lose data from the periphery, but you don't have any padded pixels without valid data\nIf alpha = 1, all source pixels are undistorted, which can lead to a non-rectangular image region - visible in the black pincushioning. Because the image has to be rectangular in memory, the gaps are padded with black pixels\nalpha = 0 (left) and alpha = 1 (right)Two ways of undistort:\ncv.undistort\nnewcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\nimage = cv2.undistort(image, mtx, dist, None, newcameramtx)\nx, y, w, h = roi\nimage = image[y:y + h, x:x + w]\ncv.initUndistortRectifyMap and cv.remap\nnewcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\nmapx, mapy = cv2.initUndistortRectifyMap(mtx, dist, None, newcameramtx, dim, 5)\nimage = cv2.remap(image, mapx, mapy, cv2.INTER_LINEAR)\nx, y, w, h = roi\nimage = image[y:y + h, x:x + w]\nStep 6. Check: Re-projection Error\nmean_error = 0\nfor i in range(len(objpoints)):\n    imgpoints2, _ = cv.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)\n    error = cv.norm(imgpoints[i], imgpoints2, cv.NORM_L2)/len(imgpoints2)\n    mean_error += error\nprint( \"total error: {}\".format(mean_error/len(objpoints)) )\nimport cv2\nfrom tqdm import tqdm\nimport numpy as np\ndef main():\n    # Set the path to the images captured by the left and right cameras\n    pathL = \"./data/checkerStereoL/\"\n    pathR = \"./data/checkerStereoR/\"\n    # Termination criteria for refining the detected corners\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n    objp = np.zeros((9 * 6, 3), np.float32)\n    objp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)\n    img_ptsL = []\n    img_ptsR = []\n    obj_pts = []\n    for i in tqdm(range(1, 11)):\n        imgL = cv2.imread(pathL + \"img%d.png\" % i)\n        imgR = cv2.imread(pathR + \"img%d.png\" % i)\n        imgL_gray = cv2.imread(pathL + \"img%d.png\" % i, 0)\n        imgR_gray = cv2.imread(pathR + \"img%d.png\" % i, 0)\n        outputL = imgL.copy()\n        outputR = imgR.copy()\n        retR, cornersR = cv2.findChessboardCorners(outputR, (9, 6), None)\n        retL, cornersL = cv2.findChessboardCorners(outputL, (9, 6), None)\n        if retR and retL:\n            obj_pts.append(objp)\n            cv2.cornerSubPix(imgR_gray, cornersR, (11, 11), (-1, -1), criteria)\n            cv2.cornerSubPix(imgL_gray, cornersL, (11, 11), (-1, -1), criteria)\n            cv2.drawChessboardCorners(outputR, (9, 6), cornersR, retR)\n            cv2.drawChessboardCorners(outputL, (9, 6), cornersL, retL)\n            cv2.imshow(\"cornersR\", outputR)\n            cv2.imshow(\"cornersL\", outputL)\n            cv2.waitKey(0)\n            img_ptsL.append(cornersL)\n            img_ptsR.append(cornersR)\n    ### Calibrating left camera ###\n    retL, mtxL, distL, rvecsL, tvecsL = cv2.calibrateCamera(\n        obj_pts, img_ptsL, imgL_gray.shape[::-1], None, None\n    )\n    hL, wL = imgL_gray.shape[:2]\n    new_mtxL, roiL = cv2.getOptimalNewCameraMatrix(mtxL, distL, (wL, hL), 1, (wL, hL))\n    ### Calibrating right camera ###\n    retR, mtxR, distR, rvecsR, tvecsR = cv2.calibrateCamera(\n        obj_pts, img_ptsR, imgR_gray.shape[::-1], None, None\n    )\n    hR, wR = imgR_gray.shape[:2]\n    new_mtxR, roiR = cv2.getOptimalNewCameraMatrix(mtxR, distR, (wR, hR), 1, (wR, hR))\n    ### Stereo Calibrate ###\n    flags = 0\n    flags |= cv2.CALIB_FIX_INTRINSIC\n    criteria_stereo = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n    # This step is performed to transformation between the two cameras and calculate Essential and Fundamenatl matrix\n    retS, new_mtxL, distL, new_mtxR, distR, Rot, Trns, Emat, Fmat = cv2.stereoCalibrate(\n        obj_pts,\n        img_ptsL,\n        img_ptsR,\n        new_mtxL,\n        distL,\n        new_mtxR,\n        distR,\n        imgL_gray.shape[::-1],\n        criteria_stereo,\n        flags,\n    )\n    ### Stereo Rectify ###\n    rectify_scale = 1\n    rect_l, rect_r, proj_mat_l, proj_mat_r, Q, roiL, roiR = cv2.stereoRectify(\n        new_mtxL,\n        distL,\n        new_mtxR,\n        distR,\n        imgL_gray.shape[::-1],\n        Rot,\n        Trns,\n        rectify_scale,\n        (0, 0),\n    )\n    ### Undistord rectified stereo image ###\n    Left_Stereo_Map = cv2.initUndistortRectifyMap(\n        new_mtxL, distL, rect_l, proj_mat_l, imgL_gray.shape[::-1], cv2.CV_16SC2\n    )\n    Right_Stereo_Map = cv2.initUndistortRectifyMap(\n        new_mtxR, distR, rect_r, proj_mat_r, imgR_gray.shape[::-1], cv2.CV_16SC2\n    )\n    print(\"Saving parameters ......\")\n    cv_file = cv2.FileStorage(\"data/improved_params2.xml\", cv2.FILE_STORAGE_WRITE)\n    cv_file.write(\"Left_Stereo_Map_x\", Left_Stereo_Map[0])\n    cv_file.write(\"Left_Stereo_Map_y\", Left_Stereo_Map[1])\n    cv_file.write(\"Right_Stereo_Map_x\", Right_Stereo_Map[0])\n    cv_file.write(\"Right_Stereo_Map_y\", Right_Stereo_Map[1])\n    cv_file.write(\"Q\", Q)\n    cv_file.release()\nif __name__ == \"__main__\":\n    main()%","focus-on-perspective-n-points-used-by-calibratecamera#Focus on Perspective n Points (used by calibrateCamera)":"https://learnopencv.com/head-pose-estimation-using-opencv-and-dlib/\nThe goal of Perspective-n-Point is to find the pose of an object when\nwe have a calibrated camera\nwe know the locations of n 3D points on the object and the corresponding 2D projections in the image\nEstimating the pose is finding 6 degrees of freedom (3 for translation + 3 for rotation)\nWe need\n2D coordinates of a few pointsDlib‚Äôs¬†facial landmark detector¬†provides us with many points to choose from\n3D locations of the same points\nYou just need the 3D locations of a few points in some arbitrary reference frame (called World Coordinates)\nTip of the nose : ( 0.0, 0.0, 0.0)\nChin : ( 0.0, -330.0, -65.0)\nDirect Linear Transform is used to solve extrinsic matrix, and can be used anytime when the equation is almost linear but is off by an unknown scale.\nHowever, DLT solution does not minimize the correct objective function. Ideally, we want to minimize the¬†reprojection error.\nWhen the pose estimate is incorrect, we can calculate a¬†re-projection error¬†measure ‚Äî the sum of squared distances between the projected 3D points and 2D facial feature points.\nLevenberg-Marquardt¬†optimization allows to iteratively change the values of¬†¬†and¬†¬†so that the reprojection error decreases. LM is often used for non linear least square problems.\ncv::solvePnPRansac¬†instead of¬†cv::solvePnP¬†because after the matching not all the found correspondences are correct (outliers)\nRansac is an outlier detection method that fits linear models to several random samplings of the data and returning the model that has the best fit to a subset of the data.\nSelect a random subset of the data\nA model is fitted to the set of hypothetical inliers\nAll other data are then tested against the fitted model.\nThe estimated model is reasonably good if sufficiently many points have been classified as part of the consensus set\nRepeat\nfull script\n#!/usr/bin/env python\nimport cv2\nimport numpy as np\n# Read Image\nim = cv2.imread(\"headPose.jpg\");\nsize = im.shape\n#2D image points. If you change the image, you need to change vector\nimage_points = np.array([\n                            (359, 391),     # Nose tip\n                            (399, 561),     # Chin\n                            (337, 297),     # Left eye left corner\n                            (513, 301),     # Right eye right corne\n                            (345, 465),     # Left Mouth corner\n                            (453, 469)      # Right mouth corner\n                        ], dtype=\"double\")\n# 3D model points.\nmodel_points = np.array([\n                            (0.0, 0.0, 0.0),             # Nose tip\n                            (0.0, -330.0, -65.0),        # Chin\n                            (-225.0, 170.0, -135.0),     # Left eye left corner\n                            (225.0, 170.0, -135.0),      # Right eye right corne\n                            (-150.0, -150.0, -125.0),    # Left Mouth corner\n                            (150.0, -150.0, -125.0)      # Right mouth corner\n                        ])\n# Camera internals\nfocal_length = size[1]\ncenter = (size[1]/2, size[0]/2)\ncamera_matrix = np.array(\n                         [[focal_length, 0, center[0]],\n                         [0, focal_length, center[1]],\n                         [0, 0, 1]], dtype = \"double\"\n                         )\nprint \"Camera Matrix :\\n {0}\".format(camera_matrix)\ndist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n(success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.CV_ITERATIVE)\nprint \"Rotation Vector:\\n {0}\".format(rotation_vector)\nprint \"Translation Vector:\\n {0}\".format(translation_vector)\n# Project a 3D point (0, 0, 1000.0) onto the image plane.\n# We use this to draw a line sticking out of the nose\n(nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 1000.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)\nfor p in image_points:\n    cv2.circle(im, (int(p[0]), int(p[1])), 3, (0,0,255), -1)\np1 = ( int(image_points[0][0]), int(image_points[0][1]))\np2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\ncv2.line(im, p1, p2, (255,0,0), 2)\n# Display image\ncv2.imshow(\"Output\", im)\ncv2.waitKey(0)","epipolar-geometry#Epipolar Geometry":"https://docs.opencv.org/3.4.15/da/de9/tutorial_py_epipolar_geometry.htmlhttps://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/https://web.stanford.edu/class/cs231a/course_notes/03-epipolar-geometry.pdf\nThe projection of the different points on¬†OX¬†form a line on right plane (line¬†l‚Ä≤). We call it¬†epiline¬†corresponding to the point\nSimilarly all points will have its corresponding epilines in the other image. The plane¬†XOO‚Ä≤¬†is called¬†Epipolar Plane.\nO¬†and¬†O‚Ä≤¬†are the camera centers. Projection of right camera¬†O‚Ä≤¬†is seen on the left image at the point,¬†e. It is called the¬†epipole\nAll the epilines pass through its epipole. So to find the location of epipole, we can find many epilines and find their intersection point.\nSo in this session, we focus on finding epipolar lines and epipoles\nBut to find epipolar lines, we need two more ingredients,¬†Fundamental Matrix (F)¬†and¬†Essential Matrix (E).\nEssential Matrix contains the information about translation and rotation, which describe the location of the second camera relative to the first in global coordinates\nFundamental Matrix contains the same information as Essential Matrix in addition to the information about the intrinsics of both cameras so that we can relate the two cameras in pixel coordinates\nFundamental Matrix F, maps a point in one image to a line (epiline) in the other image\nIf we are using rectified images and normalize the point by dividing by the focal lengths,¬†F=E\nA minimum of 8 such points are required to find the fundamental matrix (while using 8-point algorithm). More points are preferred and use RANSAC to get a more robust result.\nfull script\n# find the keypoints and descriptors with SIFT\nsift = cv.SIFT_create()\nkp1, des1 = sift.detectAndCompute(img1, None)\nkp2, des2 = sift.detectAndCompute(img2, None)\n# FLANN parameters\nFLANN_INDEX_KDTREE = 1\nindex_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\nsearch_params = dict(checks=50)\nflann = cv.FlannBasedMatcher(index_params,search_params)\nmatches = flann.knnMatch(des1 , des2, k=2)\npts1 = []\npts2 = []\n# ratio test as per Lowe's paper\nfor i,(m,n) in enumerate(matches):\n    if m.distance < 0.8*n.distance:\n        pts2.append(kp2[m.trainIdx].pt)\n        pts1.append(kp1[m.queryIdx].pt)\n# Now we have the list of best matches from both the images.\n# Let's find the Fundamental Matrix.\npts1 = np.int32(pts1)\npts2 = np.int32(pts2)\nF, mask = cv.findFundamentalMat(pts1, pts2, cv.FM_LMEDS)\n# We select only inlier points\npts1 = pts1[mask.ravel()==1]\npts2 = pts2[mask.ravel()==1]\n# Find epilines corresponding to points in right image (second image) and\n# drawing its lines on left image\nlines1 = cv.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2, F)\nlines1 = lines1.reshape(-1,3)\nimg5, _ = drawlines(img1, img2, lines1, pts1, pts2)\n# Find epilines corresponding to points in left image (first image) and\n# drawing its lines on right image\nlines2 = cv.computeCorrespondEpilines(pts1.reshape(-1,1,2), 1, F)\nlines2 = lines2.reshape(-1,3)\nimg3, _ = drawlines(img2, img1, lines2, pts2, pts1)\nYou can see in the left image that all epilines are converging at a point outside the image at right side. That meeting point is the epipole.For better results, images with good resolution and many non-planar points should be used.https://answers.opencv.org/question/18125/epilines-not-correct/\nThe problem with this particular scene is that almost all matched points lie on the same plane in 3d space. This is a known degenerate case for fundamental matrix estimation. Take a look¬†[these slides](https://www.dropbox.com/s/gpx63bam1i5frdz/geometry-for-computer-vision-lecture-5.pdf?dl=0](https://www.dropbox.com/s/gpx63bam1i5frdz/geometry-for-computer-vision-lecture-5.pdf)¬†for explanation. I tried to increase 2nd nearest neighbor threshold in this code from 0.7 to 0.8:\nIn general fundamental matrix estimation is very sensitive to quality of matches and number of outliers, since opencv uses 8-point algorithm for model estimation. Try to work with higher resolution images with rich textures and non-planar scenes.\nhttps://answers.opencv.org/question/17912/location-of-epipole/https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/\nequivalent¬†vectors, which are related by just a scaling constant, form a class of¬†homogeneous vectors\nThe set of all equivalent classes, represented by (a,b,c) forms the¬†projective space.\nWe use the homogeneous representation of homogeneous coordinates to define elements like points, lines, planes, etc., in projective space.\nWe use the rules of¬†projective geometry¬†to perform any transformations on these elements in the projective space.\nCamera calibration gives us the projection matrix  for  and  for \nWe define the  ray as , where  is the pseudo inverse of \n is a scaling parameter as we do not know the actual distance C1 ‚Üí X\nNeed to find epipolar line  to reduce the search space for a pixel in  corresponding to pixel  in \nHence to calculate , we first find two points on ray , and project them in image  using \nLet's use  and : it yields  (the epipole ) and \nIn projective geometry, a line can be define with the cross product of two points with  fundamental matrix\nIn projective geometry, if a point x lies on a line L:Thus, as  lies on epiline\nThis is a necessary condition for the two points x1 and x2 to be corresponding points, and it is also a form of epipolar constraint\nLet's use the F matrix to find epilines:\nWe use feature matching methods like ORB or SIFT to find matching points to solve the above equation for F.\nfindFundamentalMat()¬†******provides implementations of 7-Point Algorithm, 8-Point Algorithm, RANSAC algorithm, and LMedS Algorithm, to calculate F using matched feature points.","depthmap#Depthmap":"https://docs.opencv.org/4.5.4/dd/d53/tutorial_py_depthmap.htmlhttps://learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/#performing-stereo-calibration-with-fixed-intrinsic-parameters\nWe simplify the dense matching problem even more with stereo disparity\nAll the epipolar lines have to be parallel and have the same vertical coordinate left and right\nWe still have to perform triangulation for each point. We use stereo disparity.\n, where  is the baseline,  the focal length\nStereoSGBM\n# Setting parameters for StereoSGBM algorithm\n# the offset from the x-position of the left pixel at which to begin searching.\nminDisparity = 0\n# How many pixels to slide the window over.\n# The larger it is, the larger the range of visible depths, \n# but more computation is required.\nnumDisparities = 64\nblockSize = 8\ndisp12MaxDiff = 1\n# speckle_size is the number of pixels below which a disparity blob\n# is dismissed as \"speckle.\" \n# speckle_range controls how close in value disparities must be \n# to be considered part of the same blob.\nspeckleWindowSize = 10\nspeckleRange = 8\n# filters out areas that don't have enough texture for reliable matching\n# texture_threshold = ?\n# Another post-filtering step. \n# If the best matching disparity is not sufficiently better than \n# every other disparity in the search range, the pixel is filtered out.\n# You can try tweaking this if texture_threshold and the speckle filtering\n# are still letting through spurious matches.\nuniquenessRatio = 10\n# Creating an object of StereoSGBM algorithm\nstereo = cv2.StereoSGBM_create(\n\t\tminDisparity=minDisparity,\n    numDisparities=numDisparities,\n    blockSize=blockSize,\n    disp12MaxDiff=disp12MaxDiff,\n    uniquenessRatio=uniquenessRatio,\n    speckleWindowSize=speckleWindowSize,\n    speckleRange=speckleRange,\n)\n# Calculating disparith using the StereoSGBM algorithm\ndisp = stereo.compute(imgL, imgR).astype(np.float32)\ndisp = cv2.normalize(disp, 0, 255, cv2.NORM_MINMAX)\nfull script, create mesh 3D\nimport numpy as np\nimport cv2 as cv\nply_header = '''ply\nformat ascii 1.0\nelement vertex %(vert_num)d\nproperty float x\nproperty float y\nproperty float z\nproperty uchar red\nproperty uchar green\nproperty uchar blue\nend_header\n'''\ndef write_ply(fn, verts, colors):\n    verts = verts.reshape(-1, 3)\n    colors = colors.reshape(-1, 3)\n    verts = np.hstack([verts, colors])\n    with open(fn, 'wb') as f:\n        f.write((ply_header % dict(vert_num=len(verts))).encode('utf-8'))\n        np.savetxt(f, verts, fmt='%f %f %f %d %d %d ')\ndef main():\n    print('loading images...')\n    imgL = cv.pyrDown(cv.imread('aloeL.jpeg'))  # downscale images for faster processing\n    imgR = cv.pyrDown(cv.imread('aloeR.jpeg'))\n    # disparity range is tuned for 'aloe' image pair\n    window_size = 3\n    min_disp = 16\n    num_disp = 112-min_disp\n    stereo = cv.StereoSGBM_create(minDisparity = min_disp,\n        numDisparities = num_disp,\n        blockSize = 16,\n        P1 = 8*3*window_size**2,\n        P2 = 32*3*window_size**2,\n        disp12MaxDiff = 1,\n        uniquenessRatio = 10,\n        speckleWindowSize = 100,\n        speckleRange = 32\n    )\n    print('computing disparity...')\n    disp = stereo.compute(imgL, imgR).astype(np.float32) / 16.0\n    print('generating 3d point cloud...',)\n    h, w = imgL.shape[:2]\n    f = 0.8*w                          # guess for focal length\n    Q = np.float32([[1, 0, 0, -0.5*w],\n                    [0,-1, 0,  0.5*h], # turn points 180 deg around x-axis,\n                    [0, 0, 0,     -f], # so that y-axis looks up\n                    [0, 0, 1,      0]])\n    points = cv.reprojectImageTo3D(disp, Q)\n    colors = cv.cvtColor(imgL, cv.COLOR_BGR2RGB)\n    mask = disp > disp.min()\n    out_points = points[mask]\n    out_colors = colors[mask]\n    out_fn = 'out.ply'\n    write_ply(out_fn, out_points, out_colors)\n    print('%s saved' % out_fn)\n    cv.imshow('left', imgL)\n    cv.imshow('disparity', (disp-min_disp)/num_disp)\n    cv.waitKey()\n    print('Done')\nif __name__ == '__main__':\n    print(__doc__)\n    main()\n    cv.destroyAllWindows()","qa#Q&A":"http://wiki.ros.org/stereo_image_proc/Tutorials/ChoosingGoodStereoParametershttps://stackoverflow.com/questions/42737732/how-to-change-the-attributes-of-cv2-stereobm-create-for-depth-map-in-opencv-pyth","full-api-documentation#Full API Documentation":"https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html","consistent-video-depth-estimation#Consistent video depth estimation":"https://www.youtube.com/watch?v=5Tia2oblJAg&ab_channel=JohannesKopf","pose-estimation-using-opencv#Pose Estimation using OpenCV":"https://docs.opencv.org/4.5.0/dc/d2c/tutorial_real_time_pose.html","1-model-registration#1. Model registration":"Output\nCreate your own textured 3D model\nInputs\nThe application needs an input image of the object to be registered and its 3D mesh.\nWe have also to provide the intrinsic parameters of the camera with which the input image was taken.\nAlgo\nStarts up extracting the ORB features and descriptors from the input image\nUses the mesh along with the¬†M√∂ller‚ÄìTrumbore intersection algorithm¬†to compute the 3D coordinates of the found features.\nFinally, the 3D points and the descriptors are stored in different lists in a file with YAML format which each row is a different point","2-model-detection#2. Model detection":"Inputs\n3D textured model in YAML file\nAlgo\nDetect and extract ORB features and descriptors from the scene\nMatching between the scene descriptors and the model descriptor using cv::FlannBasedMatcher¬†and¬†cv::flann::GenericIndex\nmatch using¬†two Nearest Neighbour¬†the extracted descriptors with the given model descriptors\nAfter the matches filtering we have to subtract the 2D and 3D correspondences from the found scene keypoints and our 3D model using the obtained¬†DMatches¬†vector\nCompute R and t using cv::solvePnPRansac\nPnP algorithm in order to estimate the camera pose\ncv::solvePnPRansac¬†instead of¬†cv::solvePnP¬†because after the matching not all the found correspondences are correct (outliers)\nRansac is an outlier detection method that fits linear models to several random samplings of the data and returning the model that has the best fit to a subset of the data.\nSelect a random subset of the data\nA model is fitted to the set of hypothetical inliers\nAll other data are then tested against the fitted model.\nThe estimated model is reasonably good if sufficiently many points have been classified as part of the consensus set\nRepeat\nhttps://docs.opencv.org/4.5.0/dc/d2c/tutorial_real_time_pose.html\nFinally, a KalmanFilter is applied in order to reject bad poses.","pose-estimation-using-caffe#Pose estimation using Caffe":"Load Caffe pose estimation module\nprotoFile = \"pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\"\nweightsFile = \"pose/mpi/pose_iter_160000.caffemodel\"\nnet = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\nLoad input img into the model by converting into blob\ninpBlob = cv2.dnn.blobFromImage(\n\tframe, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False\n)\nnet.setInput(inpBlob)\nMake prediction and parse keypoint\nmatrix_output = net.forward()\nThe first dimension is the image ID (in case forward on multiple images)\nThe second dimension indicates the index of a keypoint. 18 keypoint confidence Maps + 1 background + 19*2 Part Affinity Maps\nThe third dimension is the height of the output map.\nThe fourth dimension is the width of the output map.\nH = out.shape[2]\nW = out.shape[3]\n# Empty list to store the detected keypoints\npoints = []\nfor i in range(output.shape[1]):\n    # confidence map of corresponding body's part.\n    probMap = output[0, i, :, :]\n    # Find global maxima of the probMap.\n    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n    # Scale the point to fit on the original image\n    x = (frameWidth * point[0]) / W\n    y = (frameHeight * point[1]) / H\n    if prob > threshold:\n        cv2.circle(frame, (int(x), int(y)), 15, (0, 255, 255), thickness=-1, lineType=cv.FILLED)\n        cv2.putText(frame, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 255), 3, lineType=cv2.LINE_AA)\n        points.append((int(x), int(y)))\n    else:\n\t\t\t\tpoints.append(None)","pose-estimation-using-mediapipe#Pose estimation using MediaPipe":"import mediapipe as mp\nfrom mediapipe.solutions.pose import Pose\nfrom mediapipe.solutions.drawing_utils import draw_landmarks\npose = Pose()\ncap = cv2.VideoCapture('a.mp4')\npTime = 0\nwhile True:\n\tsuccess, img = cap.read()\n\th, w, c = img.shape\n\timgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\tresults = pose.process(imgRGB)\n\tprint(results.pose_landmarks)\n\tif results.pose_landmarks:\n\t\tdraw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS)\n\t\t\n\t\tfor idx, lm in enumerate(results.pose_landmarks.landmark):\n\t\t\tprint(idx, lm)\n\t\t\tcx, cy = int(lm.x*w), int(lm.y*h)\n\t\t\tcv2.circle(img, (cx, cy), 5, (255,0,0), cv2.FILLED)\n\t\t\tcTime = time.time()\n\t\t\tfps = 1/(cTime-pTime)\n\t\t\tpTime = cTime\n\t\t\tcv2.putText(img, str(int(fps)), (50,50), cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0), 3)\n\t\t\tcv2.imshow(\"Image\", img)"}},"/opencv-tutorial/machine-learning":{"title":"ML","data":{"svm#SVM":"https://docs.opencv.org/4.5.0/d1/d73/tutorial_introduction_to_svm.htmlhttps://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_ml/py_svm/py_svm_basics/py_svm_basics.html#svm-understandinghttps://learnopencv.com/support-vector-machines-svm/https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_ml/py_svm/py_svm_opencv/py_svm_opencv.html\nDefinition of a hyperplane: \nCanonical hyperplane: by convention, among all the possible, we choose  , where  represent the training image closest to the hyperplane\nDistance between a point and hyperplane: \nThe margin M is twice the distance to the closest example: \nFinally, maximizing M is minimizing  with some constraints to correctly classify all training examplessubject tothis is a Lagrangian optimization problem\nfull script\nlabels = np.array([1, -1, -1, -1])\ntrainingData = np.matrix([[501, 10], [255, 10], [501, 255], [10, 501]], dtype=np.float32)\n# train\nsvm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, 100, 1e-6))\nsvm.train(trainingData, cv.ml.ROW_SAMPLE, labels)\n# Data for visual representation\nwidth = 512\nheight = 512\nimage = np.zeros((height, width, 3), dtype=np.uint8)\ngreen = (0,255,0)\nblue = (255,0,0)\nfor i in range(height):\n    for j in range(width):\n        sampleMat = np.matrix([[j,i]], dtype=np.float32)\n        response = svm.predict(sampleMat)[1]\n        if response == 1:\n            image[i,j] = green\n        elif response == -1:\n            image[i,j] = blue\nthickness = 2\nsv = svm.getUncompressedSupportVectors()\nfor i in range(sv.shape[0]):\n    cv.circle(image, (sv[i,0], sv[i,1]), 6, (128, 128, 128), thickness)","non-linear-svm#Non Linear SVM":"The training data can be rarely separated using an hyperplane.\nThe training data can be rarely separated using an hyperplane\nChance is more for a non-linear separable data in lower-dimensional space to become linear separable in higher-dimensional space.\nIn general, it is possible to map points in a d-dimensional space to some D-dimensional space¬†to check the possibility of linear separability\nThe new model has to include both the old requirement of finding the hyperplane that gives the biggest margin and the new one of generalizing the training data correctly by not allowing too many classification errors.\nFor example, one could think of minimizing the same quantity plus a constant times the number of misclassification errors in the training data\nA better solution will take into account the¬†distance of the misclassified samples to their correct decision regions\nsubject to and \nChoose C\nLarge values give solutions with less classification errors but smaller margin\nSmall values of C give solutions focusing more on the hyperplane, without much importance for classification errors, so wider margin\nImplementation\nsvm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setC(0.1)  # new\nsvm.setKernel(cv.ml.SVM_LINEAR)\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, int(1e7), 1e-6))","pca#PCA":"https://docs.opencv.org/4.5.0/d1/dee/tutorial_introduction_to_pca.htmleigenvectors\nThe points vary the most along the blue line, more than they vary along the Feature 1 or Feature 2 axes, so we'll be better off with that dimension only\nWe want to turn dataset  into a small dimension one \nKarhunen-Lo√®ve transform: \nAlgorithm\nCompute empirical mean for every dimension, with  vector of size (p, 1)\nCompute deviation from mean with , of size (n, 1)\nFind the covariant matrix of size (p, p), where  is the conjugate transpose of If B consists only of real number, it is just the regular transpose.\nSVD decomposition of the covariant matrix to find eigenvalues and eigenvectorsWith , with  being the i-th eigenvalues of C are eigenvectors, come in pairs with eigenvalues\nfull script\nfrom __future__ import print_function\nfrom __future__ import division\nimport cv2 as cv\nimport numpy as np\nimport argparse\nfrom math import atan2, cos, sin, sqrt, pi\ndef drawAxis(img, p_, q_, colour, scale):\n    p = list(p_)\n    q = list(q_)\n    \n    angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians\n    hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))\n    # Here we lengthen the arrow by a factor of scale\n    q[0] = p[0] - scale * hypotenuse * cos(angle)\n    q[1] = p[1] - scale * hypotenuse * sin(angle)\n    cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv.LINE_AA)\n    # create the arrow hooks\n    p[0] = q[0] + 9 * cos(angle + pi / 4)\n    p[1] = q[1] + 9 * sin(angle + pi / 4)\n    cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv.LINE_AA)\n    p[0] = q[0] + 9 * cos(angle - pi / 4)\n    p[1] = q[1] + 9 * sin(angle - pi / 4)\n    cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv.LINE_AA)\n    \ndef getOrientation(pts, img):\n    \n    sz = len(pts)\n    data_pts = np.empty((sz, 2), dtype=np.float64)\n    for i in range(data_pts.shape[0]):\n        data_pts[i,0] = pts[i,0,0]\n        data_pts[i,1] = pts[i,0,1]\n    \n\t\t# Perform PCA analysis\n    mean = np.empty((0))\n    mean, eigenvectors, eigenvalues = cv.PCACompute2(data_pts, mean)\n    \n\t\t# Store the center of the object\n    cntr = (int(mean[0,0]), int(mean[0,1]))\n    cv.circle(img, cntr, 3, (255, 0, 255), 2)\n    p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])\n    p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])\n    \n\t\tdrawAxis(img, cntr, p1, (0, 255, 0), 1)\n    drawAxis(img, cntr, p2, (255, 255, 0), 5)\n    angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians\n    \n    return angle\nparser = argparse.ArgumentParser(description='Code for Introduction to Principal Component Analysis (PCA) tutorial.\\\n                                              This program demonstrates how to use OpenCV PCA to extract the orientation of an object.')\nparser.add_argument('--input', help='Path to input image.', default='pca_test1.jpg')\nargs = parser.parse_args()\nsrc = cv.imread(cv.samples.findFile(args.input))\n# Check if image is loaded successfully\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\ncv.imshow('src', src)\n# Convert image to grayscale\ngray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n# Convert image to binary\n_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)\ncontours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)\nfor i, c in enumerate(contours):\n    # Calculate the area of each contour\n    area = cv.contourArea(c)\n    # Ignore contours that are too small or too large\n    if area < 1e2 or 1e5 < area:\n        continue\n    # Draw each contour only for visualisation purposes\n    cv.drawContours(src, contours, i, (0, 0, 255), 2)\n    # Find the orientation of each shape\n    getOrientation(c, src)\ncv.imshow('output', src)\ncv.waitKey()","kmeans#Kmeans":"Basic intuition\nRandomly choose 2 centroids,  and \nCalculate distance for each from both centroid. Closer to  are labeled , o.w labeled \nNext, compute the average for blue and red points and update the centroids\nIterate over 2 and 3 until convergence (or stopping criteria on number of iteration, or precision) is reachThe distances between test data and their centroids are minimum\nfull script\nimport numpy as np\nimport cv2\nimg = cv2.imread('home.jpg')\nZ = img.reshape((-1,3))\n# convert to np.float32\nZ = np.float32(Z)\n# define criteria, number of clusters(K) and apply kmeans()\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\nK = 8\nret, label, center = cv2.kmeans(\n\tZ, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS\n)\n# Now convert back into uint8, and make original image\ncenter = np.uint8(center)\nres = center[label.flatten()]\nres2 = res.reshape((img.shape))\ncv2.imshow('res2',res2)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}},"/opencv-tutorial/image-processing":{"title":"Image processing","data":{"reading-viewing-and-saving#Reading, viewing and saving":"import cv2 as cv\n# read img\nimg = cv.imread(\"path_to_file\")\n# display img\ncv.imshow(\"windows_name\", img)\ncv.waitKey(0) # wait infinitely until any key is pressed\n# save img\ncv.imwrite(\"filename.jpg\", img)","eroding--dilating#Eroding & Dilating":"originaldilatederoded\ndilatation: \nerosion: \nelement = cv.getStructuringElement(erosion_type, (2*erosion_size+1, 2*erosion_size+1), (erosion_size, erosion_size))\nerosion_dst = cv.erode(src, element)\nparameters of getStructuringElement:\nshape: cv.MORPH_RECT , cv.MORPH_CROSS, cv.MORPH_ELLIPSE\nksize: size of the structuring element\nanchor position within the element (default is center), only used for the cross shape\nMorphology transformation\nOpening\nUseful for removing small objects (it is assumed that the objects are bright on a dark foreground)\nClosing\nUseful to remove small holes (dark regions).\nMorphological gradiant\nIt is useful for finding the outline of an object as can be seen below\nBlackhat\nTophat","hit-or-miss#Hit or miss":"The Hit-or-Miss transformation is useful to find patterns in binary images.\nIn particular, it finds those pixels whose neighbourhood matches the shape of a first structuring element B1 while not matching the shape of a second structuring element B2 at the same time\nErode image A with structuring element B1\nErode the complement of image A ( Ac) with structuring element B2\nAND results from step 1 and step 2.\nkernel = np.array((\n        [0, 1, 0],\n        [1, -1, 1],\n        [0, 1, 0]), dtype=\"int\")\noutput_image = cv.morphologyEx(input_image, cv.MORPH_HITMISS, kernel)","extracting-h-or-v-lines#Extracting h or v lines":"Opening images with h or v vectors\nhorizontalStructure = cv.getStructuringElement(cv.MORPH_RECT, (horizontal_size, 1))\nverticalStructure = cv.getStructuringElement(cv.MORPH_RECT, (1, verticalsize))","image-pyramids#Image pyramids":"Gaussian pyramid:¬†Used to downsample images\nLaplacian pyramid:¬†Used to reconstruct an upsampled image from an image lower in the pyramid (with less resolution)\nTo produce layer (i+1) in the Gaussian pyramid, we do the following:\nConvolve Gi with a Gaussian kernel\nRemove every even-numbered row and column.\nHow to upsample the image instead?\nFirst, upsize the image to twice the original in each dimension, with the new even rows and\nPerform a convolution with the same kernel shown above (multiplied by 4) to approximate the values of the \"missing pixels\"\nsrc = cv.pyrUp(src, dstsize=(2 * cols, 2 * rows))\nsrc = cv.pyrDown(src, dstsize=(cols // 2, rows // 2))","basic-thresholding-operations#Basic Thresholding Operations":"The simplest segmentation method\n_, dst = cv.threshold(src_gray, threshold_value, max_binary_value, threshold_type )","thresholding-operations-using-inrange#Thresholding operations using InRange":"Hue channel models the color type, it is very useful in image processing tasks that need to segment objects based on its color\nVariation of the saturation goes from unsaturated to represent shades of gray and fully saturated\nValue channel describes the brightness or the intensity of the color.\nHSV is useful since colors in the RGB colorspace are coded using the three channels, so it is more difficult to segment an object in the image based on its color.\nframe_HSV = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\nframe_threshold = cv.inRange(frame_HSV, (low_H, low_S, low_V), (high_H, high_S, high_V))\nScript to test in range with camera\nfrom __future__ import print_function\nimport cv2 as cv\nimport argparse\nmax_value = 255\nmax_value_H = 360//2\nlow_H = 0\nlow_S = 0\nlow_V = 0\nhigh_H = max_value_H\nhigh_S = max_value\nhigh_V = max_value\nwindow_capture_name = 'Video Capture'\nwindow_detection_name = 'Object Detection'\nlow_H_name = 'Low H'\nlow_S_name = 'Low S'\nlow_V_name = 'Low V'\nhigh_H_name = 'High H'\nhigh_S_name = 'High S'\nhigh_V_name = 'High V'\ndef on_low_H_thresh_trackbar(val):\n    global low_H\n    global high_H\n    low_H = val\n    print(\"low_h: \", val)\n    low_H = min(high_H-1, low_H)\n    cv.setTrackbarPos(low_H_name, window_detection_name, low_H)\ndef on_high_H_thresh_trackbar(val):\n    global low_H\n    global high_H\n    high_H = val\n    print(\"high_h: \", val)\n    high_H = max(high_H, low_H+1)\n    cv.setTrackbarPos(high_H_name, window_detection_name, high_H)\ndef on_low_S_thresh_trackbar(val):\n    global low_S\n    global high_S\n    low_S = val\n    print(\"low_S: \", val)\n    low_S = min(high_S-1, low_S)\n    cv.setTrackbarPos(low_S_name, window_detection_name, low_S)\ndef on_high_S_thresh_trackbar(val):\n    global low_S\n    global high_S\n    high_S = val\n    print(\"high_S: \", val)\n    high_S = max(high_S, low_S+1)\n    cv.setTrackbarPos(high_S_name, window_detection_name, high_S)\ndef on_low_V_thresh_trackbar(val):\n    global low_V\n    global high_V\n    low_V = val\n    print(\"low_V: \", val)\n    low_V = min(high_V-1, low_V)\n    cv.setTrackbarPos(low_V_name, window_detection_name, low_V)\ndef on_high_V_thresh_trackbar(val):\n    global low_V\n    global high_V\n    high_V = val\n    print(\"high_V: \", val)\n    high_V = max(high_V, low_V+1)\n    cv.setTrackbarPos(high_V_name, window_detection_name, high_V)\ndef main():\n\t\tparser = argparse.ArgumentParser(description='Code for Thresholding Operations using inRange tutorial.')\n\t\tparser.add_argument('--camera_id', help='Camera divide number.', default=0, type=int)\n\t\targs = parser.parse_args()\n\t\t\n\t\tcap = cv.VideoCapture(args.camera)\n\t\t\n\t\t#cv.namedWindow(window_capture_name)\n\t\tcv.namedWindow(window_detection_name)\n\t\tcv.createTrackbar(low_H_name, window_detection_name , low_H, max_value_H, on_low_H_thresh_trackbar)\n\t\tcv.createTrackbar(high_H_name, window_detection_name , high_H, max_value_H, on_high_H_thresh_trackbar)\n\t\tcv.createTrackbar(low_S_name, window_detection_name , low_S, max_value, on_low_S_thresh_trackbar)\n\t\tcv.createTrackbar(high_S_name, window_detection_name , high_S, max_value, on_high_S_thresh_trackbar)\n\t\tcv.createTrackbar(low_V_name, window_detection_name , low_V, max_value, on_low_V_thresh_trackbar)\n\t\tcv.createTrackbar(high_V_name, window_detection_name , high_V, max_value, on_high_V_thresh_trackbar)\n\t\t\n\t\tidx = 0\n\t\twhile True:\n\t\t\n\t\t    ret, frame = cap.read()\n\t\t    if not ret:\n\t\t\t\t\t\tbreak\n        frame_HSV = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n        frame_threshold = cv.inRange(frame_HSV, (low_H, low_S, low_V), (high_H, high_S, high_V))\n        cv.imshow(window_capture_name, frame)\n        cv.imshow(window_detection_name, frame_threshold)\n\t\n\t\t    key = cv.waitKey(30)\n\t\t    if key == ord('q') or key == 27:\n\t\t        break\n\t\t\nif __name__ == \"__main\"__:\n\t\tmain()","sobel-derivatives#Sobel derivatives":"One of the most important convolutions is the computation of derivatives in an image (or an approximation to them)\nA method to detect edges in an image can be performed by locating pixel locations where the gradient is higher than its neighbors (or to generalize, higher than a threshold).\nThe Sobel Operator combines Gaussian smoothing and differentiation.\nWe calcul 2 derivatives\na. Horizontal changesb. Vertical changes\nAt each point of the image we calculate an approximation of the gradient in that point by combining both results above:\nWhen the size of the kernel is 3, the Sobel kernel shown above may produce noticeable inaccuracies (after all, Sobel is only an approximation of the derivative). OpenCV addresses this inaccuracy for kernels of size 3 by using the Scharr() function.\ngrad_x = cv.Sobel(gray, ddepth, 1, 0, ksize=3, scale=scale, delta=delta, borderType=cv.BORDER_DEFAULT)\n# Gradient-Y\n# grad_y = cv.Scharr(gray,ddepth,0,1)\ngrad_y = cv.Sobel(gray, ddepth, 0, 1, ksize=3, scale=scale, delta=delta, borderType=cv.BORDER_DEFAULT)\n  \nabs_grad_x = cv.convertScaleAbs(grad_x)\nabs_grad_y = cv.convertScaleAbs(grad_y)\ngrad = cv.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)","laplace-operator#Laplace Operator":"Sobel\nGetting the first derivative of the intensity, we observed that an edge is characterized by a maximum\nYou can observe that the second derivative is zero!\nSo, we can also use this criterion to attempt to detect edges in an image. However, note that zeros will not only appear in edges (they can actually appear in other meaningless locations); this can be solved by applying filtering where needed.\ndst = cv.Laplacian(src_gray, ddepth, ksize=kernel_size)\nTrees and the silhouette of the cow are approximately well defined (except in areas in which the intensity are very similar, i.e. around the cow's head).\nThe roof of the house behind the trees (right side) is notoriously marked. This is due to the fact that the contrast is higher in that region.","canny-edge-detector#Canny Edge Detector":"Low error rate:¬†Meaning a good detection of only existent edges.\nGood localization:¬†The distance between edge pixels detected and real edge pixels have to be minimized.\nMinimal response:¬†Only one detector response per edge.\nSteps\nFilter out any noise. The Gaussian filter is used for this purpose\nFind the intensity gradient of the image. For this, we follow a procedure analogous to Sobel:\nApply a pair of convolution masks (in x and y directions)\nFind the gradient strength and directionThe direction is rounded to one of four possible angles (namely 0, 45, 90 or 135)\nNon-maximum suppression is applied. This removes pixels that are not considered to be part of an edge. Hence, only thin lines (candidate edges) will remain.\nHysteresis: The final step. Canny does use two thresholds (upper and lower):\nIf a pixel gradient is higher than the upper threshold, the pixel is accepted as an edge\nIf a pixel gradient value is below the lower threshold, then it is rejected.\nIf the pixel gradient is between the two thresholds, then it will be accepted only if it is connected to a pixel that is above the upper threshold.\nRecommended upper:lower ratio between 2:1 and 3:1\nimg_edges = cv.Canny(img, threshold1=100, threshold2=200)","hough-line-transform#Hough Line Transform":"The Hough Line Transform is a transform used to detect straight lines.\nTo apply the Transform, first an edge detection pre-processing is desirable.\nLine equation can be written as: \nArranging the terms: \nIf for a given (x0, y0) we plot the family of lines that goes through it, we get a sinusoid.\nIf the curves of two different points intersect in the polar plane, that means that both points belong to a same line.\nIt means that in general, a line can be¬†detected¬†by finding the number of intersections between curves.\nThis is what the Hough Line Transform does.\nIt keeps track of the intersection between curves of every point in the image.\nIf the number of intersections is above some¬†threshold, then it declares it as a line with the parameters¬†(Œ∏,rŒ∏)¬†of the intersection point.\nThe Standard Hough Transform output a vector of couples (Œ∏,rŒ∏)\nThe Probabilistic Hough Line Transform output the extremes of the detected lines (x0,y0,x1,y1)\ndst = cv.Canny(src, 50, 200)\nlines = cv.HoughLines(dst, 1, np.pi / 180, 150)\n# Draw the lines\nif lines is not None:\n    for i in range(0, len(lines)):\n        rho = lines[i][0][0]\n        theta = lines[i][0][1]\n        a = math.cos(theta)\n        b = math.sin(theta)\n        x0 = a * rho\n        y0 = b * rho\n        pt1 = (int(x0 + 1000*(-b)), int(y0 + 1000*(a)))\n        pt2 = (int(x0 - 1000*(-b)), int(y0 - 1000*(a)))\n        cv.line(cdst, pt1, pt2, (0,0,255), 3, cv.LINE_AA)\ncv.HoughLines parameters:\nsrc: output of edge detector\nrho: resolution of the parameter r, in pixels (default is 1)\ntheta: resolution of the parameter theta, in degrees (default is 1)\nthreshold: the minimum number of intersections to detect a line\nlinesP = cv.HoughLinesP(dst, 1, np.pi / 180, 50, None, 50, 10)\n# Draw the lines\nif linesP is not None:\n    for i in range(0, len(linesP)):\n        l = linesP[i][0]\n        cv.line(cdstP, (l[0], l[1]), (l[2], l[3]), (0,0,255), 3, cv.LINE_AA)\ncv.HoughLinesP parameters:\nsame parameters than houghlines, plus:\nminLineLength: The minimum number of points that can form a line. Lines with less than this number of points are disregarded.\nmaxLineGap: The maximum gap between two points to be considered in the same line.\nStandard Line TransformProbabilistic Line Transform\nThe threshold, minLineLength and maxLineGap often need to be tuned with a visual feedback to the image, this is exactly what the script below does. Try it!\nimport cv2\nimport argparse\nimport numpy as np\nNAME_WINDOW = \"hough_line\"\nglobal min_threshold\nmin_threshold = 0\nglobal min_line_length\nmin_line_length = 0\nglobal max_line_gap\nmax_line_gap = 20\ndef main():\n    parser = argparse.ArgumentParser(description=\"Code for Hough Line\")\n    parser.add_argument(\"--camera\", help=\"Camera ID\", default=0, type=int)\n    parser.add_argument(\"--input\", help=\"Input video\", default=\"\", type=str)\n    args = parser.parse_args()\n    if args.input:\n        cam = cv2.VideoCapture(args.input)\n    else:\n        cam = cv2.VideoCapture(args.camera)\n    cv2.namedWindow(NAME_WINDOW)\n    cv2.createTrackbar(\"min_threshold\", NAME_WINDOW, min_threshold, 500, on_threshold_change)\n    cv2.createTrackbar(\"min_line_length\", NAME_WINDOW, min_line_length, 500, on_min_line_length_change)\n    cv2.createTrackbar(\"max_line_gap\", NAME_WINDOW, max_line_gap, 500, on_max_line_gap_change)\n    while cam.isOpened():\n        ret, img = cam.read()\n        if ret:\n            edges = preprocess(img)\n            lines = cv2.HoughLinesP(\n                edges,\n                1,\n                np.pi/360,\n                min_threshold,\n                np.array([]),\n                min_line_length,\n                max_line_gap,\n            )\n            show_lines(img, lines)\n        else:\n            print('no video')\n            cam.set(cv2.CAP_PROP_POS_FRAMES, 0)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    cam.release()\n    cv2.destroyAllWindows()\ndef preprocess(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n    return edges\ndef show_lines(img, lines):\n    if lines is None:\n        cv2.imshow(NAME_WINDOW, img)\n    else:\n        for line in lines:\n            x1, y1, x2, y2 = line[0]\n            cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n        cv2.imshow(NAME_WINDOW, img)\ndef on_threshold_change(val):\n    global min_threshold\n    min_threshold = val\n    cv2.setTrackbarPos(\"min_threshold\", NAME_WINDOW, val)\ndef on_min_line_length_change(val):\n    global min_line_length\n    min_line_length = val\n    cv2.setTrackbarPos(\"min_line_length\", NAME_WINDOW, val)\ndef on_max_line_gap_change(val):\n    global max_line_gap\n    max_line_gap = val\n    cv2.setTrackbarPos(\"max_line_gap\", NAME_WINDOW, val)\nif __name__ == \"__main__\":\n    main()","hough-circle-transform#Hough Circle Transform":"In the circle case, we need three parameters to define a circle\ncircles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, rows / 8,\n                               param1=100, param2=30,\n                               minRadius=1, maxRadius=30)","remapping#Remapping":"To accomplish the mapping process, it might be necessary to do some interpolation for non-integer pixel locations, since there will not always be a one-to-one-pixel correspondence between source and destination images.\nmap_x = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\nmap_y = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\nfor i in range(map_x.shape[0]):\n    map_x[i,:] = [map_x.shape[1]-x for x in range(map_x.shape[1])]\nfor j in range(map_y.shape[1]):\n    map_y[:,j] = list(range(map_y.shape[0]))\ndst = cv.remap(src, map_x, map_y, cv.INTER_LINEAR)","affine-transformation#Affine transformation":"Used for:\nRotations (linear transformation)\nTranslations (vector addition)\nScale operations (linear transformation)\n2 ways to express an affine relation\nWe have X and T, so we need to find M\nWe have X and M, and it's easy to get T.\nWe find M with the Affine transformation of 3 points.\nThen we apply this relation to all the pixels in Image.\n# Warping\nsrcTri = np.array( [[0, 0], [src.shape[1] - 1, 0], [0, src.shape[0] - 1]] ).astype(np.float32)\ndstTri = np.array( [[0, src.shape[1]*0.33], [src.shape[1]*0.85, src.shape[0]*0.25], [src.shape[1]*0.15, src.shape[0]*0.7]] ).astype(np.float32)\nwarp_mat = cv.getAffineTransform(srcTri, dstTri)\nwarp_dst = cv.warpAffine(src, warp_mat, (src.shape[1], src.shape[0]))\n# Rotating the image after Warp\ncenter = (warp_dst.shape[1]//2, warp_dst.shape[0]//2)\nangle = -50\nscale = 0.6\nrot_mat = cv.getRotationMatrix2D( center, angle, scale )\nwarp_rotate_dst = cv.warpAffine(warp_dst, rot_mat, (warp_dst.shape[1], warp_dst.shape[0]))","histogram-equalization#Histogram Equalization":"Image Histogram = intensity distribution\nHistogram Equalization improves the contrast in an image, in order to stretch out the intensity range\nEqualization implies mapping one distribution (the given histogram) to its cumulative distribution (more details needed)\nsrc = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\ndst = cv.equalizeHist(src)","histogram-calculation#Histogram Calculation":"bgr_planes = cv.split(src)\nb_hist = cv.calcHist(bgr_planes, [0], None, [histSize], histRange, accumulate=accumulate)\ng_hist = cv.calcHist(bgr_planes, [1], None, [histSize], histRange, accumulate=accumulate)\nr_hist = cv.calcHist(bgr_planes, [2], None, [histSize], histRange, accumulate=accumulate)","histogram-comparison#Histogram Comparison":"use 4 different metrics to express how well two histograms match with each other.\nCorrelation\nChi Square\nIntersection\nBhattacharyya distance\nMethod\nConvert the images to HSV format\nCalculate the H-S histogram for all the images and normalize them in order to compare them.\nCompare the histogram of the base image with respect to the 2 test histograms\nhsv_base = cv.cvtColor(src_base, cv.COLOR_BGR2HSV)\nhsv_test = cv.cvtColor(src_test, cv.COLOR_BGR2HSV)\nh_ranges = [0, 180]\ns_ranges = [0, 256]\nranges = h_ranges + s_ranges # concat lists\nhist_base = cv.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_base, hist_base, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nbase_test = cv.compareHist(hist_base, hist_test, compare_method=1)","back-projection#Back projection":"Calculate the histogram model of a feature and then use it to find this feature in an image\nIf you have a histogram of flesh color (say, a Hue-Saturation histogram ), then you can use it to find flesh color areas in an image:\nAfter applying some mask to capture the skin area in T0 image, we computed the H-S histogram.\nWe also computed the entire H-S histogram for T2.\nMethod\nFor each pixel of the test image, find the correspondent bin location for that pixel (hi,j,si,j)\nLookup the model histogram in the correspondent bin and get the value\nStore this bin value in a new image (BackProjection)\nThe values stored in BackProjection represent the probability that a pixel in Test Image belongs to a skin area, based on the model histogram that we use\nhist = cv.calcHist([hue], [0], None, [histSize], ranges, accumulate=False)\ncv.normalize(hist, hist, alpha=0, beta=255, norm_type=cv.NORM_MINMAX)\nbackproj = cv.calcBackProject([hue], [0], hist, ranges, scale=1)","template-matching#Template matching":"match method0: SQDIFF1: SQDIFF NORMED2: TM CCORR3: TM CCORR NORMED4: TM COEFF5: TM COEFF NORMED\nresult = cv.matchTemplate(img, templ, match_method)\nPossible use case: fix templates, like stop signs ?","finding-contours-of-your-image#Finding contours of your image":"The most important practical difference is that findContours gives connected contours, while Canny just gives edges, which are lines that may or may not be connected to each other.\nTo choose, I suggest that you try both on your sample application and see which gives better results.\n# Detect edges using Canny\ncanny_output = cv.Canny(src_gray, threshold, threshold * 2)\n# Find contours\ncontours, hierarchy = cv.findContours(canny_output, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n# Draw contours\ndrawing = np.zeros((canny_output.shape[0], canny_output.shape[1], 3), dtype=np.uint8)\nfor i in range(len(contours)):\n    color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n    cv.drawContours(drawing, contours, i, color, 2, cv.LINE_8, hierarchy, 0)\nhttps://medium.com/analytics-vidhya/contours-and-convex-hull-in-opencv-python-d7503f6651bc\n2nd argument is contour retrieval mode: a shape(or a contour) may be present inside another shape(or contour).\nThe retrieval mode argument decides if want the output of the contours list in a hierarchical(parent-child) manner or we simply want them as a list(having no hierarchy, cv2.RETR_LIST).\n3rd argument is contour approximation, it specifies how many points should be stored so that the contour shape is preserved and can be redrawn (cv2.CHAIN_APPROX_NONE store all the points)\nOperations on contours include moments (extract important contour‚Äôs physical properties like the center of mass of the object, area of the object)\ncentroidXCoordinate = int(moment['m10'] / moment['m00'])\ncentroidYCoordinate = int(moment['m01'] / moment['m00'])","convex-hull#Convex Hull":"Minimum boundary that completely wrap a contour\ncanny_output = cv.Canny(src_gray, threshold, threshold * 2)\n# Find contours\ncontours, _ = cv.findContours(canny_output, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n# Find the convex hull object for each contour\nhull_list = []\nfor contour in contours:\n    hull = cv.convexHull(contour)\n    hull_list.append(hull)\nAny deviation of the contour from its convex hull is known as the convexity defect\nconvexityDefects = cv2.convexityDefects(contour, convexhull)","bounding-box-and-bounding-circle#Bounding Box and bounding circle":"def bounding_box(frame):\n\t  frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n    frame = cv.blur(frame, (3,3))\n    threshold = 250\n    canny_output = cv.Canny(frame, threshold, threshold * 2)\n    contours, _ = cv.findContours(canny_output, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n    frame_contour = np.zeros((canny_output.shape[0], canny_output.shape[1], 3), dtype=np.uint8)\n    n_contour = len(contours)\n    contours_poly = [None] * n_contour\n    bound_rect = [None] * n_contour\n    for idx, contour in enumerate(contours):\n        contours_poly[idx] = cv.approxPolyDP(contour, 3, True)\n        bound_rect[idx] = cv.boundingRect(contours_poly[idx])\n    for idx in range(n_contour):\n        color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n        #cv.drawContours(frame_contour, contours_poly, idx, color)\n        cv.rectangle(\n            frame_contour,\n            (\n                int(bound_rect[idx][0]),\n                int(bound_rect[idx][1]),\n            ),\n            (\n                int(bound_rect[idx][0]+bound_rect[idx][2]),\n                int(bound_rect[idx][1]+bound_rect[idx][3]),\n            ),\n            color,\n            2,\n        )\n    return frame_contour\nOriginalBack projection using another texture imagedraw contour with Canny threshold = 255bounding box with Canny threshold = 255","image-moments#Image Moments":"moment m00 is the area\ncanny_output = cv.Canny(src_gray, threshold, threshold * 2)\ncontours, _ = cv.findContours(canny_output, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n# Get the moments\nmu = [None]*len(contours)\nmc = [None]*len(contours)\nfor idx, contour in enumerate(contours):\n    mu[idx] = cv.moments(contour)\n    # Get the mass centers\n    # add 1e-5 to avoid division by zero\n    mc[idx] = (mu[idx]['m10'] / (mu[idx]['m00'] + 1e-5), mu[idx]['m01'] / (mu[idx]['m00'] + 1e-5))","point-polygon-test#Point Polygon Test":"PointPolygonTest computes distance between a point and a contour\ncontours, _ = cv.findContours(src, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n# Calculate the distances to the contour\nraw_dist = np.empty(src.shape, dtype=np.float32)\nfor i in range(src.shape[0]):\n    for j in range(src.shape[1]):\n        raw_dist[i,j] = cv.pointPolygonTest(contours[0], (j,i), True)\nminVal, maxVal, _, maxDistPt = cv.minMaxLoc(raw_dist)\nminVal = abs(minVal)\nmaxVal = abs(maxVal)\n# Depicting the  distances graphically\ndrawing = np.zeros((src.shape[0], src.shape[1], 3), dtype=np.uint8)\nfor i in range(src.shape[0]):\n    for j in range(src.shape[1]):\n        if raw_dist[i,j] < 0:\n            drawing[i,j,0] = 255 - abs(raw_dist[i,j]) * 255 / minVal\n        elif raw_dist[i,j] > 0:\n            drawing[i,j,2] = 255 - raw_dist[i,j] * 255 / maxVal\n        else:\n            drawing[i,j,0] = 255\n            drawing[i,j,1] = 255\n            drawing[i,j,2] = 255\ncv.circle(drawing,maxDistPt, int(maxVal),(255,255,255), 1, cv.LINE_8, 0)\ncv.imshow('Source', src)\ncv.imshow('Distance and inscribed circle', drawing)","distance-transform-and-watershed-algorithm#Distance Transform and Watershed Algorithm":"https://docs.opencv.org/4.5.0/d2/dbd/tutorial_distance_transform.html\nSpecially useful when extracting touching or overlapping objects in images\nUse marks of different intensities. Each mark intensity is seen as a negative height. The algorithm will then \"flow\" until connecting all valleys.\nChange background to black\nsrc[np.all(src == 255, axis=2)] = 0\nSharpen our image to acute the edges. Use a Laplacian filter.As the kernel has negative values, we need to convert our images to float32, before converting it back to uint8\nkernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)\nimgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)\nsharp = np.float32(src)\nimgResult = sharp - imgLaplacian\nimgResult = np.clip(imgResult, 0, 255)\nimgResult = imgResult.astype('uint8')\nimgLaplacian = np.clip(imgLaplacian, 0, 255)\nimgLaplacian = np.uint8(imgLaplacian)\nConvert to grayscale then binary: Otsu infers the threshold\nbw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)\n_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)\ncv.imshow('Binary Image', bw)\nApply Distance transform on the image. Normalize for vizualisation\ndist = cv.distanceTransform(bw, cv.DIST_L2, 3)\ncv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)\nWe threshold the dist image and then perform some morphology operation (i.e. dilation) in order to extract the peaks from the above image\n_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)\nkernel1 = np.ones((3,3), dtype=np.uint8)\ndist = cv.dilate(dist, kernel1)\nExtract contours and create marks at different background level\ndist_8u = dist.astype('uint8')\ncontours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\nmarkers = np.zeros(dist.shape, dtype=np.int32)\n# Draw the foreground markers\nfor i in range(len(contours)):\n    cv.drawContours(markers, contours, i, (i+1), -1)\n# Draw the background marker\ncv.circle(markers, (5,5), 3, (255,255,255), -1)\nFinally, performs Watershed algo\ncv.watershed(imgResult, markers)\nmark = markers.astype('uint8')\nmark = cv.bitwise_not(mark)\ncolors = []\nfor contour in contours:\n    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))\ndst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)\nfor i in range(markers.shape[0]):\n    for j in range(markers.shape[1]):\n        index = markers[i,j]\n        if index > 0 and index <= len(contours):\n            dst[i,j,:] = colors[index-1]","out-of-focus-deblur-filter#Out-of-focus Deblur Filter":"Degradation model:  (S is the blurred image, U the original image, H the frequency response of point spread function (PSF) and N random noise\nThe circular PSF is a good approximation (just a circle, only parameter is radius)\nObjective: find an estimate of the original image. .\nWiener filter is a way to restore a blurred image:  (Need to find the PSF radius to get H, and SNR is signal to noise ratio\ncv.calcPSF(h, roi.size(), R)\ncv.calcWnrFilter(h, Hw, 1.0/snr)\ncv.filter2DFreq(imgIn(roi), imgOut, Hw)","motion-deblur-filter#Motion Deblur Filter":"https://docs.opencv.org/4.5.0/d1/dfd/tutorial_motion_deblur_filter.htmlPSF is a line (2 parameters: length and angle)LEN = 125, THETA = 0, SNR = 700LEN = 78, THETA = 15, SNR = 300","anisotropic-image-segmentation-by-a-gradient-structure-tensor#Anisotropic image segmentation by a gradient structure tensor":"https://docs.opencv.org/4.5.0/d4/d70/tutorial_anisotropic_image_segmentation_by_a_gst.html\nThe gradient structure tensor of an image is a 2x2 symmetric matrix.\nEigenvectors of the gradient structure tensor indicate local orientation, whereas eigenvalues give coherency (a measure of anisotropism).\nStructure gradiant of an image Z:\nEigenvalues: \nCoherency:\nOrientation: \nimgCoherency, imgOrientation = calcGST(imgIn, W)\n_, imgCoherencyBin = cv.threshold(imgCoherency, C_Thr, 255, cv.THRESH_BINARY)\n_, imgOrientationBin = cv.threshold(imgOrientation, LowThr, HighThr, cv.THRESH_BINARY)\nimgBin = cv.bitwise_and(imgCoherencyBin, imgOrientationBin)\nW = 52          # window size is WxW\nC_Thr = 0.43    # threshold for coherency\nLowThr = 35     # threshold1 for orientation, it ranges from 0 to 180\nHighThr = 57    # threshold2 for orientation, it ranges from 0 to 180","periodic-noise-removing-filter#Periodic Noise Removing Filter":"https://docs.opencv.org/4.5.0/d2/d0b/tutorial_periodic_noise_removing_filter.html\nPeriodic noise produces spikes in the Fourier domain that can often be detected by visual analysis.\nPeriodic noise can be reduced significantly via frequency domain filtering\nWe use a notch reject filter with an appropriate radius to completely enclose the noise spikes in the Fourier domain\nThe number of notch filters is arbitrary. The shape of the notch areas can also be arbitrary (e.g. rectangular or circular). We use three circular shape notch reject filters.\nPower spectrum densify of an image is used for the noise spike‚Äôs visual detection.\nr = 21\nsynthesizeFilterH(H, Point(705, 458), r)\nsynthesizeFilterH(H, Point(850, 391), r)\nsynthesizeFilterH(H, Point(993, 325), r)\nH = fftshift(H)\nimgOut = filter2DFreq(imgIn, H)"}},"/opencv-tutorial/core-module":{"title":"Core module","data":{"":"This section details some of the key behind-the-scene concepts, that make OpenCV implementation possible.Mat object\nOutput image allocation for OpenCV functions is automatic (unless specified otherwise).\nYou do not need to think about memory management with OpenCV's C++ interface.\nThe assignment operator and the copy constructor only copies the header.\nThe underlying matrix of an image may be copied using the¬†cv::Mat::clone()¬†and¬†cv::Mat::copyTo()¬†functions.\nColor systems\nRGB is the most common as our eyes use something similar, however keep in mind that OpenCV standard display system composes colors using the BGR color space (red and blue channels are swapped places).\nThe HSV and HLS decompose colors into their hue, saturation and value/luminance components, which is a more natural way for us to describe colors. You might, for example, dismiss the last component, making your algorithm less sensible to the light conditions of the input image.\nYCrCb is used by the popular JPEG image format.\nCIE Lab* is a perceptually uniform color space, which comes in handy if you need to measure the¬†distance¬†of a given color to another color.\nMat mask operation\nFast method\ndst1 = cv.filter2D(src, -1, kernel)\nI/O\nChoose channel upon reading\nimg = cv.imread(filename, cv.IMREAD_GRAYSCALE)\nBlending 2 images\nlinear blend operator: \nBy varying Œ± from 0‚Üí1 this operator can be used to perform a temporal cross-dissolve between two images or videos\ndst = cv.addWeighted(src1, alpha, src2, beta, 0.0)\nBrightness and contrast adjustments\nIncreasing Œ≤ (bias) increases the brightness, increasing Œ± increases saturation (gain)\nGamma transformation: \nWhen Œ≥<1, the original dark regions will be brighter and the histogram will be shifted to the right whereas it will be the opposite with Œ≥>1.\nDiscrete Fourier Transform\nDisplaying this is possible either via a real image and a complex image or via a magnitude and a phase image.\nHowever, throughout the image processing algorithms only the magnitude image is interesting as this contains all the information we need about the images geometric structure"}},"/opencv-tutorial/object-detection":{"title":"Object Detection","data":{"haar-cascade-inference#Haar Cascade Inference":"https://docs.opencv.org/4.5.0/db/d28/tutorial_cascade_classifier.html\nmachine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.\nFeatures extraction using Haar features.\nThey are just like our convolutional kernel.\nEach feature is a single value obtained by subtracting sum of pixels under the white rectangle from sum of pixels under the black rectangle.\nHowever large your image, it reduces the calculations for a given pixel to an operation involving just four pixels. Nice, isn't it? It makes things super-fast.\nBut among all these features we calculated, most of them are irrelevant\nThe same windows applied to cheeks or any other place is irrelevant\nSo how do we select the best features out of 160000+ features? It is achieved by¬†Adaboost\nFor this, we apply each and every feature on all the training images. For each feature, it finds the best threshold which will classify the faces to positive and negative.\nObviously, there will be errors or misclassifications. We select the features with minimum error rate, which means they are the features that most accurately classify the face and non-face images\nThe final classifier is a weighted sum of these weak classifiers.\nIn an image, most of the image is non-face region.\nSo it is a better idea to have a simple method to check if a window is not a face region.\nFor this they introduced the concept of¬†Cascade of Classifiers. Instead of applying all 6000 features on a window, the features are grouped into different stages of classifiers and applied one-by-one.\nIf a window fails the first stage, discard it.\nThe authors' detector had 6000+ features with 38 stages with 1, 10, 25, 25 and 50 features in the first five stages\nfull script\nfrom __future__ import print_function\nimport cv2 as cv\nimport argparse\ndef detectAndDisplay(frame):\n    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n    frame_gray = cv.equalizeHist(frame_gray)\n    #-- Detect faces\n    faces = face_cascade.detectMultiScale(frame_gray)\n    for (x,y,w,h) in faces:\n        center = (x + w//2, y + h//2)\n        frame = cv.ellipse(frame, center, (w//2, h//2), 0, 0, 360, (255, 0, 255), 4)\n        faceROI = frame_gray[y:y+h,x:x+w]\n        #-- In each face, detect eyes\n        eyes = eyes_cascade.detectMultiScale(faceROI)\n        for (x2,y2,w2,h2) in eyes:\n            eye_center = (x + x2 + w2//2, y + y2 + h2//2)\n            radius = int(round((w2 + h2)*0.25))\n            frame = cv.circle(frame, eye_center, radius, (255, 0, 0 ), 4)\n    cv.imshow('Capture - Face detection', frame)\nparser = argparse.ArgumentParser(description='Code for Cascade Classifier tutorial.')\nparser.add_argument('--face_cascade', help='Path to face cascade.', default='data/haarcascades/haarcascade_frontalface_alt.xml')\nparser.add_argument('--eyes_cascade', help='Path to eyes cascade.', default='data/haarcascades/haarcascade_eye_tree_eyeglasses.xml')\nparser.add_argument('--camera', help='Camera divide number.', type=int, default=0)\nargs = parser.parse_args()\nface_cascade_name = args.face_cascade\neyes_cascade_name = args.eyes_cascade\nface_cascade = cv.CascadeClassifier()\neyes_cascade = cv.CascadeClassifier()\n#-- 1. Load the cascades\nif not face_cascade.load(cv.samples.findFile(face_cascade_name)):\n    print('--(!)Error loading face cascade')\n    exit(0)\nif not eyes_cascade.load(cv.samples.findFile(eyes_cascade_name)):\n    print('--(!)Error loading eyes cascade')\n    exit(0)\ncamera_device = args.camera\n#-- 2. Read the video stream\ncap = cv.VideoCapture(camera_device)\nif not cap.isOpened:\n    print('--(!)Error opening video capture')\n    exit(0)\nwhile True:\n    ret, frame = cap.read()\n    if frame is None:\n        print('--(!) No captured frame -- Break!')\n        break\n    detectAndDisplay(frame)\n    if cv.waitKey(10) == 27:\n        break","haar-cascade-training#Haar Cascade Training":"https://docs.opencv.org/4.5.0/dc/d88/tutorial_traincascade.html\nThe opencv_traincascade supports both HAAR like wavelet features¬†[248]¬†and LBP (Local Binary Patterns)¬†[140]¬†feature\nLBP features yield integer precision in contrast to HAAR features, yielding floating point precision, so both training and detection with LBP are several times faster then with HAAR features.\nWe need a set of positive samples (containing actual objects you want to detect) and a set of negative images (containing everything you do not want to detect).\nThe set of negative samples must be prepared manually, whereas set of positive samples is created using the opencv_createsamples application.\nNegative samples may be of different sizes.\nHowever, each image should be equal or larger than the desired training window size¬†(which corresponds to the model dimensions, most of the times being the average size of your object)\nThese images are used to subsample one negative image into several image samples having this training window size.\n2 ways of generating positive samples\nGenerate a bunch of positives from a single positive object image.\nSupply all the positives yourself and only use the tool to cut them out, resize them and put them in the opencv needed binary format (better)\n100 real object images, can lead to a better model than 1000 artificially generated positives, by using the opencv_createsamples application\nStructure\n/img\n  img1.jpg\n  img2.jpg\ninfo.dat\ninfo.dat\nimg/img1.jpg  1  140 100 45 45\nimg/img2.jpg  2  100 200 50 50   50 30 25 25\nThe manual process of creating the¬†info¬†file can also been done by using the opencv_annotation tool.\nThis is an open source tool for visually selecting the regions of interest of your object instances in any given images.\nThe project need to be build (git clone)\nopencv_annotation --annotations=/path/to/annotations/file.txt --images=/path/to/image/folder/\nNext step is to train the model: opencv_traincascade\nVisualize the classifier with opencv_visualisation --image=/data/object.png --model=/data/model.xml --data=/data/result/"}},"/opencv-tutorial/home":{"title":"OpenCV Advanced Tutorials","data":{"":"OpenCV is a rich and complete environment for image manipulation and preprocessing and has become the backbone for many computer vision projects. It can also act like a naive baseline to benchmark against more advanced models for deep learning purposes.While its core is in C++, its Java and Python implementations offer both benefits and drawbacks. The main benefits are easy embedding into robotics and hardware projects and high compatibility with the ROS environment.However, as the number of modules has kept increasing, the documentation is often inconsistent and the implementation is non-pythonic. It all makes OpenCV for python harder to learn.This page is an aggregation from different OpenCV tutorials, almost like a cheatsheet, for you to understand the main concepts faster. I found it pretty helpful while developing computer vision applications, and I hope it will be handy for you as well."}},"/opencv-tutorial/video-analysis":{"title":"Video Analysis","data":{"":"https://docs.opencv.org/4.5.0/da/dd0/tutorial_table_of_content_video.html","background-substraction-bs#Background Substraction (BS)":"Goal is generating a foreground mask (a binary image containing the pixels belonging to moving objects in the scene) by using static cameras.\nBS calculates the foreground mask performing a subtraction between the current frame and a background model, containing the static part of the scene\nBack ground modeling is made of 2 steps:\nBackground initialization\nBack ground update, to adapt to possible changes\nBut in most of the cases, you may not have a image without people or cars to init, so we need to extract the background from whatever images we have.\nIt become more complicated when there is shadow of the vehicles. Since shadow is also moving, simple subtraction will mark that also as foreground. It complicates things.\nBackGroundSubstractorMOG\nGaussian Mixture-based Background/Foreground Segmentation Algorithm.\nModel each background pixel by a mixture of K Gaussian distributions (K = 3 to 5).\nThe weights of the mixture represent the time proportions that those colours stay in the scene.\nThe probable background colours are the ones which stay longer and more static.\nBackGroundSubstractorMOG2\nBackgroundSubtractorGMG\nfull script\nfrom __future__ import print_function\nimport cv2 as cv\nimport argparse\nparser = argparse.ArgumentParser(description='This program shows how to use background subtraction methods provided by \\\n                                              OpenCV. You can process both videos and images.')\nparser.add_argument('--input', type=str, help='Path to a video or a sequence of image.', default='vtest.avi')\nparser.add_argument('--algo', type=str, help='Background subtraction method (KNN, MOG2).', default='MOG2')\nargs = parser.parse_args()\nif args.algo == 'MOG2':\n    backSub = cv.createBackgroundSubtractorMOG2()\nelse:\n    backSub = cv.createBackgroundSubtractorKNN()\n#capture = cv.VideoCapture(cv.samples.findFileOrKeep(args.input))\ncapture = cv.VideoCapture(0)\nif not capture.isOpened:\n    print('Unable to open: ' + args.input)\n    exit(0)\nwhile True:\n    ret, frame = capture.read()\n    if frame is None:\n        break\n    \n    fgMask = backSub.apply(frame)\n    \n    cv.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n    cv.putText(frame, str(capture.get(cv.CAP_PROP_POS_FRAMES)), (15, 15),\n               cv.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n    \n    cv.imshow('Frame', frame)\n    cv.imshow('FG Mask', fgMask)\n    \n    keyboard = cv.waitKey(30)\n    if keyboard == 'q' or keyboard == 27:\n        break","mean-shift-and-camshift#Mean Shift and CamShift":"https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_video/py_meanshift/py_meanshift.html#meanshift\nMeanshift\nStart by computing back projection using a ROI window\nComputing difference between circle windows center and points centroid, update the center with the centroid, until convergence\nSo finally what you obtain is a window with maximum pixel distribution.\nContinuously Adaptive Meanshift: Camshift\nThe issue with meanshift is that our window has always the same size\nApplies meanshift first.\nOnce meanshift converges, update the window size as \nAlso compute the orientation of the best fitting ellipse\nImplementation\nimport numpy as np\nimport cv2\ncap = cv2.VideoCapture('slow.flv')\n# take first frame of the video\nret,frame = cap.read()\n# setup initial location of window\nr,h,c,w = 250,90,400,125  # simply hardcoded the values\ntrack_window = (c,r,w,h)\n# set up the ROI for tracking\nroi = frame[r:r+h, c:c+w]\nhsv_roi =  cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\nmask = cv2.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\nroi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])\ncv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n# Setup the termination criteria, either 10 iteration or move by atleast 1 pt\nterm_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\nwhile(1):\n    ret ,frame = cap.read()\n    if ret == True:\n        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n        # apply meanshift to get the new location\n        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n        # Draw it on image\n        pts = cv2.boxPoints(ret)\n        pts = np.int0(pts)\n        img2 = cv2.polylines(frame,[pts],True, 255,2)\n        cv2.imshow('img2',img2)\n        k = cv2.waitKey(60) & 0xff\n        if k == 27:\n            break\n        else:\n            cv2.imwrite(chr(k)+\".jpg\",img2)\n    else:\n        break\ncv2.destroyAllWindows()\ncap.release()\nhardcode initial track_window\ninRange on initial ROI to filter low saturation and low visibility\ntrack_window computed iteratively and passed as a argument","optical-flow#Optical Flow":"https://docs.opencv.org/4.5.0/d4/dee/tutorial_optical_flow.htmlhttps://learnopencv.com/optical-flow-in-opencv/","sparse#Sparse":"Hypothesis\nThe pixel intensities of an object do not change between consecutive frames.\nNeighbouring pixels have similar motion.\nOptical flow equationusing Taylor,with , , ,  are unknown. We solve it using Lucas-Kanade\nKanade:\nTakes a 3x3 patch around the point. So all the 9 points have the same motion\nWe can find¬†¬†for these 9 points, ie solving 9 equations with two unknown variables, which is over-determined\nieLeast squaresthus\nFails when there is a large motion\nTo deal with this we use pyramids: multi-scaling trick.\nWhen we go up in the pyramid, small motions are removed and large motions become small motions.\nWe find corners in the image using Shi-Tomasi corner detector and then calculate the corners‚Äô motion vector between two consecutive frames.\nfull script\n# params for ShiTomasi corner detection\nfeature_params = dict( maxCorners = 100,\n                       qualityLevel = 0.3,\n                       minDistance = 7,\n                       blockSize = 7 )\n# Parameters for lucas kanade optical flow\nlk_params = dict( winSize  = (15,15),\n                  maxLevel = 2,\n                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n# Create some random colors\ncolor = np.random.randint(0,255,(100,3))\n# Take first frame and find corners in it\nret, old_frame = cap.read()\nold_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\np0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n# Create a mask image for drawing purposes\nmask = np.zeros_like(old_frame)\nwhile(1):\n    ret,frame = cap.read()\n    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n    # calculate optical flow\n    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n    # Select good points\n    good_new = p1[st==1]\n    good_old = p0[st==1]\n    # draw the tracks\n    for i,(new,old) in enumerate(zip(good_new, good_old)):\n        a,b = new.ravel()\n        c,d = old.ravel()\n        mask = cv.line(mask, (a,b),(c,d), color[i].tolist(), 2)\n        frame = cv.circle(frame,(a,b),5,color[i].tolist(),-1)\n    img = cv.add(frame ,mask)\n    cv.imshow('frame', img)\n    k = cv.waitKey(30) & 0xff\n    if k == 27:\n        break\n    # Now update the previous frame and previous points\n    old_gray = frame_gray.copy()\n    p0 = good_new.reshape(-1,1,2)","dense#Dense":"Compute motion for every pixel in the image\ndef dense_optical_flow(method, video_path, params=[], to_gray=False):\n    \n\t\t# Read the video and first frame\n    cap = cv2.VideoCapture(video_path)\n    ret, old_frame = cap.read()\n \n    # crate HSV & make Value a constant\n    hsv = np.zeros_like(old_frame)\n    hsv[..., 1] = 255\n \n    # Preprocessing for exact method\n    if to_gray:\n        old_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n\t\twhile True:\n\t\t\t  # Read the next frame\n\t\t\t  ret, new_frame = cap.read()\n\t\t\t  frame_copy = new_frame\n\t\t\t  if not ret:\n\t\t\t      break\n\t\t\t\n\t\t\t  # Preprocessing for exact method\n\t\t\t  if to_gray:\n\t\t\t      new_frame = cv2.cvtColor(new_frame, cv2.COLOR_BGR2GRAY)\n\t\t\t\n\t\t\t  # Calculate Optical Flow\n\t\t\t  flow = method(old_frame, new_frame, None, *params)\n\t\t\t\n\t\t\t  # Encoding: convert the algorithm's output into Polar coordinates\n\t\t\t  mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n\t\t\t  # Use Hue and Value to encode the Optical Flow\n\t\t\t  hsv[..., 0] = ang * 180 / np.pi / 2\n\t\t\t  hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n\t\t\t\n\t\t\t  # Convert HSV image into BGR for demo\n\t\t\t  bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n\t\t\t  cv2.imshow(\"frame\", frame_copy)\n\t\t\t  cv2.imshow(\"optical flow\", bgr)\n\t\t\t  k = cv2.waitKey(25) & 0xFF\n\t\t\t  if k == 27:\n\t\t\t      break\n\t\t\t\n\t\t\t  # Update the previous frame\n\t\t\t  old_frame = new_frame\nDense Pyramid Lucas-Kanade algorithm\nelif args.algorithm == 'lucaskanade_dense':\n    method = cv2.optflow.calcOpticalFlowSparseToDense\n    frames = dense_optical_flow(method, video_path, save, to_gray=True)\nFarneback algorithm\nApproximate some neighbors of each pixel with a polynomial, taking the second order of Taylor expansion\nWe observe the differences in the approximated polynomials caused by object displacements\nelif args.algorithm == 'farneback':\n    method = cv2.calcOpticalFlowFarneback\n    params = [0.5, 3, 15, 3, 5, 1.2, 0]  # default Farneback's algorithm parameters\n    frames = dense_optical_flow(method, video_path, save, params, to_gray=True)\nRLOF\nThe intensity constancy assumption doesn‚Äôt fully reflect how the real world behaves.\nThere are also shadows, reflections, weather conditions, moving light sources, and, in short, varying illuminations. with  illumination parameters\nelif args.algorithm == \"rlof\":\n    method = cv2.optflow.calcOpticalFlowDenseRLOF\n    frames = dense_optical_flow(method, video_path, save)"}},"/proba-ml/clustering/intro":{"title":"21.1 Introduction","data":{"":"Clustering is a very common form of unsupervised learning. There are two main kinds of method.In the first approach, the input is a set of data samples , where typically .In the second approach, the input is a pairwise dissimilarity matrix , where .In both cases, the goal is to assign similar points to the same cluster. As often with unsupervised learning, it is hard to evaluate the quality of a clustering algorithm.If we have labels for some points, we can use the similarity between the labels of two points as a metric for determining if the two inputs ‚Äúshould‚Äù be assigned to the same cluster or not.If we we don‚Äôt have labels, but the method is based on generative model of the data, we can use log likelihood as a metric.","2111-evaluating-the-output-of-clustering-methods#21.1.1 Evaluating the output of clustering methods":"If we use probabilistic models, we can always evaluate the likelihood of the data, but this has two drawbacks:\nIt does not assess the clustering discovered by the model\nIt does not apply to non-probabilistic methods\nHence, we review performance measures not based on likelihood.Intuitively, similar points should be assigned to the same cluster. We can rely on some external form of data, like labels or a reference clustering, by making the hypothesis that objects with the same label are similar.","21111-purity#21.1.1.1 Purity":"Let  be the number of objects in cluster  that belong to class , with  be the total number of objects in cluster .Define  the empirical probability of the class  in cluster .We define the purity of a cluster as:and the overall purity of a clustering as:On the figure above we have:However, this measure doesn‚Äôt penalize the number of cluster, since we can achieve the best purity of  by trivially putting each object in its own cluster.","21112-rand-index#21.1.1.2 Rand index":"Let  and  two different partition of the  data points. For example,  could be the estimated clustering and  some reference clustering derived from the class labels.A common statistics is the Rand index:which is analogous to the accuracy for classification.","21113-mutual-information#21.1.1.3 Mutual information":"where:\n is the probability that a randomly chosen object belong to the cluster  in  and  in \n be the probability that a randomly chosen object belong to the cluster .\nThis lies between 0 and .Unfortunately, the maximum can be achieved by using a lot of small clusters, which have low entropy.To compensate for this, we can use the normalized mutual information:This lies between  and"}},"/proba-ml/clustering/biclustering":{"title":"21.6 Biclustering","data":{"":"In some cases, we have a data matrix , and we want the cluster the rows and the columns. This is known as biclustering.This is widely used in bioinformatic, where rows represent genes and columns represent conditions. It can also be used in collaborative filtering, where the rows represent users and columns represent movies.","2161-spectral-co-clustering#21.6.1 Spectral Co-clustering":"The spectral co-clustering algorithm finds bi-cluster with values higher than those in the corresponding other rows and columns.Each row and each column belongs exactly to one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal.An approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of the graph Laplacian.If the original data  has shape , then the Laplacian matrix for the corresponding bipartite graph has shape .However, in this case it is possible to work directly with , which is smaller and make the compute more efficient. is preprocessed as follow:where  and .The SVD decomposition , provides the partition of rows and columns:\nA subset of  gives the rows partitions\nA subset of  gives the columns partitions.\nThe  singular vectors, starting from the second give the partition.We can then form a low dimensional matrix:where the columns of  are  and similarly for .Finally, we apply K-means on  to find our rows and columns partitioning.","2162-spectral-biclustering#21.6.2 Spectral Biclustering":"The spectral biclustering assumes that the input data matrix has a hidden checkerboard structure.The rows and columns matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row cluster and column cluster is approximately constant.For instance, if there are 2 row partitions and 3 column partitions, each row will belong to 3 bicluster and each column will belong to 2 bicluster.The input matrix  is first normalized to make the checkerboard pattern obvious. There are three possible methods:\nIndependent row and column normalization, as in spectral co-clustering.\nBistochastization: repeat row and column normalization until convergence. This methods make both rows and columns sum to the same constant.\nLog normalization: we compute  and we normalize by the following:\nAfter normalizing, the first few eigenvectors are computed, as in co-clustering.We rank the first singular vectors  and  according to which can be best approximated by a piecewise constant vector.The approximation for each vector are found by using a 1d k-means and score via Euclidean distance.Next, the data is projected to this best subset of singular vectors by computing  and , and clustered."}},"/proba-ml/clustering/hierarchical-agglomerative-clustering":{"title":"21.2 Hierarchical agglomerative clustering (HAC)","data":{"":"Hierarchical agglomerative clustering is a common form of clustering.The input to the algorithm is an  dissimilarity matrix  and the output is a tree structure in which groups  and  with small dissimilarity are grouped together in a hierarchical fashion.We will use city block distance to define the dissimilarity:","2121-the-algorithm#21.2.1 The algorithm":"Agglomerative clustering starts with \ngroups, each initially containing one object, and then at each step it merges the two most similar groups until there is a single group containing all the data.The result is a binary tree known as dendogram. By cutting the tree at different heights, we can induce a different number of (nested) clusters.Since picking the two most similar groups takes  and there are  steps in the algorithm, the total running time is .However, by using a priority queue, this can be reduced to .There are three variants of agglomerative clustering, depending on how we define the dissimilarity between groups of objects.","21211-single-link#21.2.1.1 Single link":"In single link clustering, also called nearest neighbor clustering, the distance between two groups  and  is defined as the distance between the closest members of each groups:The tree built using single link clustering is a minimum spanning tree of the data, connecting all objects in a way that minimize the sum of the edge weights (distance).As a consequence of this, we can implement single link clustering in  time, whereas the naive form of other variants take  time.","21212-complete-link#21.2.1.2 Complete link":"In complete link clustering, aka furthest neighbor clustering, the distance between two groups is defined as the distance between the two most distant pairs:Single linkage only requires that a single pairs of object to be close for the two groups to be considered a cluster, regardless of the similarity of other members of the group.Thus, clusters can be formed that violate the compactness property, which says that all observations within a group should be similar to each other.If we define the diameter of a group as the largest dissimilarity of its members, then we can see that the single link can produce clusters with large diameters.Complete linkage represents the opposite extreme: two groups are considered close if all of the observations in their union are relatively similar. This tends to produce compact clusterings, with small diameters.In scipy, this is implemented using nearest neighbor chain, and takes  time.","21213-average-link#21.2.1.3 Average link":"In practice, the preferred method is average link clustering, which measures the average distance between all pairs:where  and  are the number of elements in each groups.Average link clustering represent a compromise between single and complete link clustering. It tends to produce compact clusters that are relatively far apart.However, since it involves averaging of the distances, any change to the measurement scale can change the result. In contrast, single linkage and complete linkage are invariant to monotonic transformations of , since they leave the relative ordering the same.In scipy, this is also implemented using nearest neighbor chain, and takes  time.","2122-example#21.2.2 Example":"Suppose we have a set of time series measurements of the expression levels for  genes at  points, in response to a given stimulus. Each data sample is a vector .We see that there are several kind of genes, some goes up monotonically over time, some goes down, and some exhibit more complex response patterns.Suppose we use Euclidean distance to compute a pairwise distance matrix , and apply HAC using average linkage. We get the following dendogram:The time series assigned to each cluster of the cut do indeed ‚Äúlook like‚Äù each other."}},"/proba-ml/clustering/kmeans-clustering":{"title":"21.3 K means clustering","data":{"":"There are several issues with the hierarchical agglomerative clustering.\nIt takes  time to compute for the average link method, making it hard to apply to big datasets (note that scipy.linkage implements it in  time using nearest-neighbor chain).\nIt assumes that a dissimilarity matrix has already been computed, whereas the notion of similarity is often unclear and needs to be learned\nIt is an algorithm, not a model, so it is hard to evaluate how good it is. It doesn‚Äôt optimize an objective.\nThe K-means algorithm addresses these issues:\nIt runs in  where  is the number of iterations\nIt computes dissimilarity in terms of Euclidean distance to learned cluster centers , rather than requiring a dissimilarity matrix.\nIt optimizes a well-defined cost function.","2131-the-algorithm#21.3.1 The algorithm":"We assume there are  cluster centers , so we can cluster the data by assigning each point  to its closest center:We can then update the cluster centers by computing the average value of all points assigned to them:We can then iterate these steps to convergence.More formally we can view this as finding a local minimum of the following cost function, known as distortion:where ,  and  contains the cluster centers  in its columns.K-means optimizes this using alternative minimization (this is closely related to the EM algorithm for GMMs).","2132-examples#21.3.2 Examples":"","21321-clustering-points-in-the-2d-plane#21.3.2.1 Clustering points in the 2d plane":"K-means clustering applied to some points in the 2d space induces a Voronoi tessellation of the points. The resulting clustering is sensitive to the initialization.By default, sklearn uses 10 random restarts (combined with K-means++ initialization) and returns the clustering with lowest distortion (in sklearn, the distortion is called ‚Äúinertia‚Äù).","21322-clustering-gene-expression-time-series-data#21.3.2.2 Clustering gene expression time series data":"We show the result of applying K-means clustering with . We see that time series that ‚Äúlook similar‚Äù to each other are assigned to the same cluster.We also see that the centroid of each cluster is a reasonable summary of all the data points assigned to that cluster.","2133-vector-quantization#21.3.3 Vector quantization":"Suppose we want to perform loss compression of some real-valued vector . A very simple approach is to use vector quantization (VQ).The basic idea is to replace each real-valued vector  with a discrete symbol , which is an index into a codebook of  prototypes, Each data vector is encoded by using the index of the most similar prototype, where similarity is measured in terms of Euclidean distance:We can define the cost function that measure the quality of the codebook by computing the reconstruction error (aka distorsion) it induces:where . This is exactly the cost function minimized by the K-means algorithm.We can achieve zero distorsion by using  and assigning , but this not compress the data at all. It takes  bits, where  is the number of bits needed to represent a real-valued scalar.We can do better by detecting similar vectors in the data, creating prototypes or centroids for them, and then representing the data as deviation from these prototypes.This reduces the space requirement to  bits.\nThe first term arises because each of the  data vectors need to specify which of the  codeword it is using.\nThe second term arises because we have to store each codebook entry, each of which is a -dimensional vector.\nWhen  is large, the first term dominates the second, so we can approximate the rate of the encoding scheme (number of bits needed per object) as , which is typically much less than .We treat the  pixel image below as a set of  scalars (i.e., .If we use one byte to represent each pixel (gray-scale intensity of 0 to 255), then , so we need   bits to represent the image in uncompressed form.For the compressed image, we need  bits. For , this is about 128kb, a factor of 4 compression, yet resulting in negligible perceptual loss.Greater compression could be achieved if we modeled spatial correlation between the pixels, e.g. if we encoded  blocks (as used by JPEG). This is because the residual errors would be smaller, and would take fewer bits to encode.This shows the deep connection between data compression and density estimation.","2134-the-k-means-algorithm#21.3.4 The K-means++ algorithm":"K-means is optimizing a non-convex objective, and hence needs to be initialized carefully.A simple approach is to pick  data points at random, and use these as the initial values for . We can improve on this using multiple restarts, i.e., we run the algorithm multiple times. However, this can be slow.A better approach is to pick the centers sequentially, so as to try to ‚Äúcover‚Äù the data. We pick the initial point uniformly at random, and then each subsequent point is picked from the remaining points, with probability proportional to the square distance to the point‚Äôs closest cluster center.That is, at iteration , we pick the next cluster to be  with probability:where:is the squared distance of  to the closest existing centroid.Thus, points that are far away from a centroid are more likely to be picked, thus reducing the distorsion. This is known as farthest point clustering or K-means++.Surprisingly, this simple trick can be shown to guarantee that the reconstruction error is never more than  worse than optimal.","2135-the-k-medoids-algorithm#21.3.5 The K-medoids algorithm":"K-medoids is a variant to K-means in which we estimate each cluster center  by choosing the data example  whose average dissimilarity to all the other points in the cluster is minimal. This point is called a medoid.By contrast in K-means, we take average over points assigned to each cluster to compute the center (so the cluster is often not a data example).K-medoids can be more robust to outlier (although this can be mitigated by using a mixture of Student distributions instead of a mixture of Gaussian).More importantly, K-medoids can be applied to data that doesn‚Äôt belong to , where averaging may not be well defined. The input is a  pairwise distance matrix , not a  feature matrix.K-medoids can be solved using Voronoi iteration, where we first find the medoid in a cluster by computing the average distance for each point, and then update the clusters by assigning each point to its closest medoid.","2136-speedup-tricks#21.3.6 Speedup tricks":"K-means take  time, where  is the number of iterations, but we can reduce the constant factors by using various tricks.In sklearn, the classical EM-style algorithm is Lloyd (default) and the Elkan variation can be more efficient on some datasets with well-defined clusters, by using the triangle inequality to keep track of the lower and upper bounds for the distances between inputs and centroids. This can be used to eliminate some redundant computation.Another approach is to use a minibatch approximation, which can be significantly faster for slightly worse results.","2137-choosing-the-number-of-clusters-k#21.3.7 Choosing the number of clusters ":"","21371-minimizing-the-distorsion#21.3.7.1 Minimizing the distorsion":"Based on our experience with supervised learning, a natural choice for picking  is the value that minimizes the reconstruction error on a validation set:where  is the reconstruction of .Unfortunately, this technique will not work, and the distortion monotonically decreases with .To see why, note that the K-means is a degenerated density model which consists of  ‚Äúspikes‚Äù at the  centers. As we increase  we ‚Äúcover‚Äù more of the input space, hence any given point is more likely to find a close prototype, which decrease the reconstruction error.Thus, unlike supervised learning, we can‚Äôt use reconstruction error on a validation set to select the best unsupervised model (this also apply to PCA).In the figure above, we see an ‚Äúelbow‚Äù in the curve at , but this is hard to detect.","21372-maximizing-the-marginal-likelihood#21.3.7.2 Maximizing the marginal likelihood":"If we use a proper probabilistic model like a GMM, we can use the log marginal likelihood (LML) of the data to perform model selection.We can approximate the LML using the BIC score, as discussed in section 5.2.5.1:where  is the number of parameters in a model with  clusters, and  is the MLE.We see this exhibits a typical U-shaped curve.It works because its cluster is associated to a Gaussian distribution that fills a volume of the input space, rather than being a degenerate spike.Once we have enough clusters to cover the true modes of the distribution, the Bayesian Occam‚Äôs razor kicks in, and start penalizing the model for being overly complex.","21373-silhouette-coefficient#21.3.7.3 Silhouette coefficient":"The silhouette coefficient is common heuristic for picking the number of clusters in K-means clustering model. This is designed to work with spherical (not elongated) clusters.We define the silhouette coefficient as:where  is the mean distance to the other instances within the cluster , defined as:and  is the mean distance to the other instances in the next closest cluster , defined as:Thus  measure the compactness of ‚Äôs cluster, and  measure the distance between the clusters.The silhouette coefficient varies from  to , where:\n means that the instance is close to the members of the clusters and far from other clusters\n means that the instance is close to a cluster boundary\n means that the instance may be in the wrong cluster\nThe silhouette score of a clustering  is the mean silhouette coefficient over all instances.In the figure above, we see a prominent peak at , although  is almost as good.It can be informative to look at individual silhouette coefficients and just the mean score. We can plot these in a silhouette diagram:The dotted vertical line is the average coefficient. Clusters with many point the left are likely to be of low quality. We can also use this figure to look at the size of each cluster, even when the data is not 2d.","21374-incrementally-growing-the-number-of-mixture-components#21.3.7.4 Incrementally growing the number of mixture components":"An alternative to searching the best  is to incrementally ‚Äúgrow‚Äù GMMs.We can start with a small value of , and after each round of training we consider splitting the highest mixing weight into two, with the new centroids being random perturbations of the original centroid, and the new scores being half of old scores.If a new cluster has too small a score or too narrow a variance, it is removed. We continue in this way until the desired number of cluster is reached."}},"/opencv-tutorial/video-io":{"title":"Video Input & Output","data":{"":"https://docs.opencv.org/4.5.0/df/d2c/tutorial_table_of_content_videoio.html","video-input#Video-Input":"OpenCV processes both real-time image feed (in the case of a webcam) or prerecorded and hard disk drive stored files\ncaptRefrnc = cv.VideoCapture(sourceReference)\nrefS = (\n\tint(captRefrnc.get(cv.CAP_PROP_FRAME_WIDTH)),\n\tint(captRefrnc.get(cv.CAP_PROP_FRAME_HEIGHT))\n)\nwhile True: # Show the image captured in the window and repeat\n   _, frameReference = captRefrnc.read()\n\tif frameReference is None:\n\t\t\tprint(\" < < <  Game over!  > > > \")\n      break\n  \n  # ops ...\n  cv.imshow(WIN_RF, frameReference)","video-output#Video-Output":"string::size_type pAt = source.find_last_of('.');                  // Find extension point\nconst string NAME = source.substr(0, pAt) + argv[2][0] + \".avi\";   // Form the new name with container\nint ex = static_cast<int>(inputVideo.get(CAP_PROP_FOURCC));     // Get Codec Type- Int form\nSize S = Size(\n\t(int) inputVideo.get(CAP_PROP_FRAME_WIDTH),    // Acquire input size\n  (int) inputVideo.get(CAP_PROP_FRAME_HEIGHT)\n);\noutputVideo.open(NAME, ex, inputVideo.get(CAP_PROP_FPS), S, true)\nfor(;;){\n\t\tinputVideo >> src;\n\t\t\n\t\t# ops...\n\t\n\t\toutputVideo << res;\n}","using-kinect-and-other-openni-compatible-depth-sensors#Using Kinect and other OpenNI compatible depth sensors":"also Using Creative Senz3D and other Intel RealSense SDK compatible depth sensorshttps://docs.opencv.org/4.5.0/d7/d6f/tutorial_kinect_openni.htmlVideoCapture can retrieve the following data:\ndata given from depth generator:\nCAP_OPENNI_DEPTH_MAP - depth values in mm (CV_16UC1)\nCAP_OPENNI_POINT_CLOUD_MAP - XYZ in meters (CV_32FC3)\nCAP_OPENNI_DISPARITY_MAP - disparity in pixels (CV_8UC1)\nCAP_OPENNI_DISPARITY_MAP_32F - disparity in pixels (CV_32FC1)\nCAP_OPENNI_VALID_DEPTH_MASK - mask of valid pixels (not occluded, not shaded etc.) (CV_8UC1)\ndata given from BGR image generator:\nCAP_OPENNI_BGR_IMAGE - color image (CV_8UC3)\nCAP_OPENNI_GRAY_IMAGE - gray image (CV_8UC1)\nIn order to get depth map from depth sensor use VideoCapture::operator >>, e. g. :\nVideoCapture capture( CAP_OPENNI );\nfor(;;)\n{\n    Mat depthMap;\n    capture >> depthMap;\n    if( waitKey( 30 ) >= 0 )\n        break;\n}\nFor getting several data maps use VideoCapture::grab and VideoCapture::retrieve, e.g. :\nVideoCapture capture(0); // or CAP_OPENNI\nfor(;;)\n{\n    Mat depthMap;\n    Mat bgrImage;\n    capture.grab();\n    capture.retrieve( depthMap, CAP_OPENNI_DEPTH_MAP );\n    capture.retrieve( bgrImage, CAP_OPENNI_BGR_IMAGE );\n    if( waitKey( 30 ) >= 0 )\n        break;\n}\nFor setting and getting some property of sensor` data generators use VideoCapture::set and VideoCapture::get methods respectively, e.g. :\nVideoCapture capture( CAP_OPENNI );\ncapture.set( CAP_OPENNI_IMAGE_GENERATOR_OUTPUT_MODE, CAP_OPENNI_VGA_30HZ );\ncout << \"FPS    \" << capture.get( CAP_OPENNI_IMAGE_GENERATOR+CAP_PROP_FPS ) << endl;"}},"/proba-ml/clustering/mixture-models":{"title":"21.4 Clustering using mixture models","data":{"":"We have seen how the K-means algorithm can be used to cluster data vectors in . However, this method assumes that all clusters have the same spherical shape, which is a very restrictive assumption.In addition, K-means assumes that all clusters can be described by Gaussian in the input space, so it can‚Äôt be applied to categorical data.We can overcome both of these issues with mixture models.","2141-mixtures-of-gaussians#21.4.1 Mixtures of Gaussians":"Recall from section 3.5.1 that a Gaussian mixture model (GMM) has the form:If we know the model parameters, , we can use the Bayes rule to compute the responsibility (posterior membership probability) of a cluster  for the data point :We can then compute the most probable cluster assignment:This is known as hard clustering.","21411-k-means-is-a-special-case-of-em#21.4.1.1 K-means is a special case of EM":"We can estimate the parameter of a GMM using the EM algorithm (section 8.7.3).It turns out K-means is a special case of this algorithm, which makes two assumptions:\nWe fix  and  for all clusters, so we just have to estimate the means \nWe approximate the E step by replacing the soft responsibilities with hard assignment, i.e. we set  instead of .\nWith this approximation, the weighted MLE problem of the M step reduces to equation:So we recover K-means.However, the assumption that all cluster have the same spherical shape is very restrictive. The figure below shows the marginal density and clusters induced by different covariance matrix shape.We see that for this dataset, we need to capture off-diagonal covariance (top row).","2142-mixtures-of-bernoullis#21.4.2 Mixtures of Bernoullis":"We can use a mixtures of Bernoulli to cluster binary data (as discussed in section 3.5.2). The model has the form:Here  is the probability that bits  turns on in cluster .We can fit this model using EM, SGD or MCMC."}},"/proba-ml/clustering/spectral-clustering":{"title":"21.5 Spectral clustering","data":{"":"Spectral clustering is an approach based on eigenvalue analysis of a pairwise similarity matrix. It uses the eigenvectors to derive feature vectors for each data point, which are then clustered using feature-based clustering methods like K-means.","2151-normalized-cuts#21.5.1 Normalized cuts":"We start by creating an undirected weighted graph , where each data point is a node, and the strength of the edge - is a measure of similarity.We only connect a node to its nearest neighbors to ensure the graph is sparse, which speeds computation.Our goal is to find  clusters of similar point, i.e. a graph partition into  disjoints sets of nodes so as to minimize the normalized cut:where:and where   is the complement of , ,  is the total weight of set  (the sum of the degree of its edges).This splits the graph into  clusters such that the nodes of each cluster are similar to each other but different from the rest.We could formulate the Ncut problem by using , where  iff the point  belongs to the cluster .Unfortunately, this is NP-hard. Below, we discuss a continuous relaxation of the problem based on eigenvectors that is easier to solve.","2152-eigenvectors-of-the-graph-laplacian-encode-the-clustering#21.5.2 Eigenvectors of the graph Laplacian encode the clustering":"The graph Laplacian is defined by:where  is the symmetric weight matrix of the graph and  is the diagonal matrix containing the weight degree of each node, .In practice, it is important to account for the normalized graph Laplacian, to account for the fact that some nodes are more highly connected than others. One way to do it is to create the symmetry matrix:The algorithm is the following:\nFind the smallest  eigenvectors of \nStack them into a matrix \nNormalize each row to unit norm  to make the matrix \nCluster the matrix  using K-means, then infer the partition of the original point","2153-example#21.5.3 Example":"We see that for nested circles, K-means does a poor job, since it assumes each cluster corresponds to a spherical Gaussian.Next, we compute a dense similarity matrix  using a Gaussian kernel:We then compute the first two eigenvectors of the normalized Laplacian . From this we infer the clustering using K-means with .","2154-connection-with-other-methods#21.5.4 Connection with other methods":"","21541-spectral-clustering-and-kpca#21.5.4.1 Spectral clustering and kPCA":"Kernel PCA uses the largest eigenvectors of , which are equivalent to the smallest eigenvectors of .In practice, spectral clustering tends to give better results than kPCA."}},"/proba-ml/decision-theory/bayesian-decision-theory":{"title":"5.1 Bayesian decision theory","data":{"":"Bayesian inference allows us to update our belief about hidden quantities  based on observed data . To convert this inference into an action, we use Bayesian decision theory.","511-basics#5.1.1 Basics":"We assume an agent has a set of possible actions . Each of these actions have a cost and benefits, depending on some state .We can encode this information using a loss function , specifying the cost of taking action  with state .In medical circles, a common measure to quantify the benefits of a drug is the quality-adjusted life years (QALY)Once we have the loss function, we can compute the posterior expected loss or riskThe optimal policy specifies what action to take given observed data In the case of Covid, let say that  is the age of each patient. We can convert test results into a distribution over disease states, and by combining it with the loss matrix above we can compute the optimal policy for each observation.Let say we improve the precision of the test, the probability that a positive test is correct increases and the cost of no-op for elderly increases as well, thus changing the action to take into 1.","512-classification-problems#5.1.2 Classification problems":"Given an observed input , what is the optimal class label to predict?Zero-one lossSuppose the states are some classes , and the action correspond to class labels .Which corresponds toIn this case the posterior expected loss isHence the action that minimize the risk is the most probable label, that is the mode of the posterior distribution, aka the MAP estimateCost sensitive classificationLet‚Äôs now have the following loss function Let  and . We select  iffwhen  that simplifies toAnd we have So if a false negative cost 2 times as much as a false positive, our positive threshold is .The reject optionIn some cases we would like to be able ‚ÄúI don‚Äôt know‚Äù instead of answering: that‚Äôs the reject option. The actions are  where 0 is the reject option.The loss function isAnd we choose the reject action whenever the highest probability is below . Otherwise choose the most probable class.","513-roc-curve#5.1.3 ROC curve":"For any fixed threshold  we consider the followingWe can compute the number of false positivesBy normalizing by row  or by columns  we can derive various summary statistics.We can plot the  and  as a function of  (ROC curve) and the PR curve.We summarize the ROC curve using its area under the curve (AUC), the best being 1 and the worst being 0.5 (random prediction).PR curve can be summarized by taking the precision of the  first recalled elements: this is the precision at  score (P@k).","514-pr-curve#5.1.4 PR Curve":"In rank retrieval problems, a PR curve can be non-monotonic. Suppose a classifier has a precision of 90% with recall of 10%, and a precision of 96% with a recall of 20%. We can compute the maximum precision (96%) we can achieve with at least a recall of  (10%): this is the interpolated precision.The average of the interpolated precisions (AP) is the area under the interpolated PR curve.The mean average precision (mAP) is the mean of the AP over a set of different PR curves.The  score can be defined as follow for :OrIf we set  we got the harmonic mean of the precision and recall:The harmonic mean is more conservative than the arithmetic mean and it requires both precision and recall to be high.Suppose the recall is very high and the precision is very low, we would have whereas Using the  score weights precision and recall equally. If recall is more important we may use , and if precision is more important we may use .Class imbalance doesn‚Äôt affect ROC curve, since the TPR and FPR are a class ratio.However the RC curve is affected, since the precision can be written:with , so that   when  and  when The F1 score is also affected, since:","515-regression-problems#5.1.5 Regression problems":"In regression settings, the set of state and action space is L2 lossThe most common loss is the  loss:In this case, the risk is given by:The optimal action reduce the risk at that point to 0, in this case the posterior mean:L1 loss loss is less sensitive to outliers than  loss:The optimal action is the posterior median,  such that:Huber lossAnother robust loss function:where","516-probabilistic-prediction-problems#5.1.6 Probabilistic prediction problems":"We now assume the set of possible actions is to pick a probability distribution over some value. The state is a distribution  and the action is another .We want to pick  to minimize  for a given .Kullback Leibler divergence and  are the entropy and cross-entropy. is discrete, but this can be generalized to continuous values.To find the optimal distribution to use, we minimize  and as  is constant wrt , we only minimize the cross-entropy term:If the state is degenerate and puts all its mass on a single outcome i.e. , the cross entropy becomes:This is the log loss of the predictive distribution  when given target label .Proper scoring ruleThe key property we desire is that the loss function is minimized iff the chosen distribution  matches the true distribution , ie  $$,  is the proper scoring rule.The cross-entropy is a proper scoring rule since , but the  term is sensitive to errors for low probability eventsA common alternative is the Brier score:See this discussion on scikit-learn about the benefit and drawbacks of Log loss vs Brier score."}},"/proba-ml/decision-theory/choosing-model":{"title":"5.2 Choosing the right model","data":{"":"We can use Bayesian decision theory to choose the best model among candidates.","521-bayesian-hypothesis-testing#5.2.1 Bayesian hypothesis testing":"We have two hypothesis: the null  and the alternative  and we want to know which one is more likely to be true: this is hypothesis testing.If we use zero-one loss, the optimal decision is to pick  when . With uniform prior,  and hence we pick   when the Bayes factor is:This is close to a likelihood ratio except we integrate out the parameters, so that we can compare models of different complexity, due to the Bayesian Occam‚Äôs razor effect.The Bayes factor is the frequentist equivalent to p-values.Exemple: testing the fairness of a coinThe marginal likelihood under  is simply:And the marginal likelihood under  by taking a Beta prior is:We prefer  over  when observing 2 or 3 heads. The figure below list all the possible draw\n() for 5 tosses.","522-model-selection#5.2.2 Model selection":"We now have more than 2 models and need to pick the most likely. We can view model selection as decision theory problem, where the action space requires choosing a model .If we have a 0-1 loss, the optimal action is to pick the higher posterior over models:With a uniform prior, , then the MAP is given by:with the marginal likelihood:If all settings of  assign a high probability to the data,  is probably a good model.We use a uniform prior over models and use empirical Bayes to estimate the prior over the regression weights. We then compute the evidence for each model. With , there is not enough data to choose a more complex model than degree of 1.","523-occams-razor#5.2.3 Occam‚Äôs razor":"Consider two models: a simpler one  and a more complex one . If both can explain the data equally well, ie , then we should choose the simpler one.This is because the complex model will put less prior probability on the ‚Äúgood parameters‚Äù that explain the data, since the prior must integrate to 1 over the feature space.The figure present the predictions (marginal likelihood) of 3 models on increasingly more ‚Äúcomplex‚Äú datasets ( is the observed dataset, on which  is optimal).","524-connection-between-cross-validation-and-marginal-likelihood#5.2.4 Connection between cross validation and marginal likelihood":"Marginal likelihood helps us choose models of the right complexity. In non-Bayesian approaches, it is standard to select model via cross validation.The marginal likelihood can be written:and by using the plugin approximation:Finally:This is similar to the leave-one-out cross validation (LOO-CV) which has the form:except the marginal likelihood ignores the  part.Overly complex models will overfit the early data examples and will predict the remaining poorly, leading to a low CV score.","525-information-criteria#5.2.5 Information criteria":"The marginal likelihood  can be difficult to obtain since it requires integrating over the entire parameter space.We introduce alternative information criterias5.2.5.1 Bayesian information criterion (BIC)BIC can be considered as a simple approximation to the marginal likelihood. If we make a Gaussian approximation to the posterior and use the quadratic approximation from section 4.6.8.2:where  is the hessian of the negative log joint  evaluated in . The hessian is called the Occam factor since it measures the model complexity.So:Let‚Äôs approximate each  by a constant :Where . We can drop the last term, constant in .With a uniform prior, , we have the BIC score:Then the BIC loss is5.2.5.2 Akaike information criterion (AIC)It has the form:This penalizes less heavily than BIC since the penalty doesn‚Äôt depend on","526-posterior-inference-over-effect-sizes-and-bayesian-significance-testing#5.2.6 Posterior inference over effect sizes and Bayesian significance testing":"Bayesian hypothesis testing leverages the Bayes factor  but computing the marginal likelihood is computationally difficult and the results can be sensitive to the choice of the prior.We are often more interested in the effect size, e.g. when comparing the means of 2 models\n. The probability that the mean of  is higher than mean of  can be computed as   or , with   the minimum magnitude effect (one-sided or two-sided t-test).More generally,  represents the region of practical equivalence (ROPE). We can define 3 events of interest:\nThe null hypothesis , when  (which is more realistic than )\n: ,  is better than \n: , the opposite\nTo choose among these hypothesis, we have to estimate , which avoids computing the Bayes factor.Bayesian t-test for difference in meansLet  be the error of method  on sample . Since the samples are common across datasets, we use paired test, that better compare methods than average performances.Let , we assume  and our goal is to estimate  with\n.If we use a non-informative prior, one can show that the posterior marginal for the mean is given by the student distribution:with the sample mean   and an unbiased estimate of the variance\n.Therefore we can easily compute  with a ROPE of Bayesian -test for differences in ratesLet  be the number of correct sample for method  over a total of   trial, so the accuracy is .We assume , and we are interested in  with  and\n.If we use a uniform prior: , our posterior is:The posterior for  is given by:We can compute it for any :This can be computed using 1-dimensional numerical integration or analytically.Note that data is often summarized in a contingency table:"}},"/proba-ml/dimensionality-reduction/factor-analysis":{"title":"20.2 Factor analysis","data":{"":"PCA is a simple form of linear low-dimensional representation of the data.Factor analysis is a generalization of PCA based on probabilistic model, so that we can treat it as building block for more complex models such as mixture of FA models or nonlinear FA models.","2021-generative-model#20.2.1 Generative model":"Factor analysis is a linear-Gaussian latent variable generative model:where  is known as the factor loading matrix and  is the covariance matrix.FA can be thought as a low-rank version of a Gaussian distribution. To see this, note that the following induced marginal distribution is Gaussian:Without loss of generality, we set  and .We get:Suppose we use  and  and .We can see it as taking an isotropic Gaussian ‚Äúspray can‚Äù representing the likelihood , and sliding it along the 1d line defined by  as we vary the 1d latent prior .In general, FA approximates the low rank decomposition of the visible covariance matrix:This only uses  parameters, which allows a flexible compromise between a full covariance Gaussian with  parameters, and a diagonal covariance with  parameters.From this equation, we see that we should restrict  to be diagonal, otherwise we could set , ignoring latent factors while still being able to model any covariance.The marginal variance of each visible variable is:where the first term is the variance due to the common factors, and the uniqueness  is the variance specific to that dimension.We can estimate the parameters of a FA using EM. Once we have fit the model, we can compute probabilistic latent embeddings using .Using Bayes rule for Gaussians, we have:","2022-probabilistic-pca#20.2.2 Probabilistic PCA":"We consider a special case when of the factor analysis in which  has orthonormal columns,  . This is called probabilistic PCA or sensible PCA.The marginal distribution on the visible variables has the form:where:The log likelihood is:where we plugged , the MLE of , and used the trace trick on the empirical covariance matrix  It has been shown that the maximum of this objective must satisfy:where  are the eigenvectors associated to the  largest eigenvalues of , and  is the diagonal eigenvalues matrix.In the noise free limit, where , we get:which is proportional to the PCA solution.The MLE for the observation variance is:which is the average distortion associated with the discarded dimensions.","2024-unidentifiability-of-the-parameters#20.2.4 Unidentifiability of the parameters":"The parameters of a FA model are unidentifiable. To see this, consider a model with weight , where  is an arbitrary orthogonal rotation matrix, satisfying .This has the same likelihood as model with weight , since:Geometrically, multiplying  by an orthogonal matrix is like rotating  before generating ; but since  is drawn from an anisotropic Gaussian distribution, it makes no difference on the likelihood.Consequently, we can‚Äôt uniquely identify  and the latent factors either.To break this symmetry, several solutions have been proposed:\nForcing  to have orthonormal columns. This is the approach adopted by PCA.\nForcing  to be lower triangular.\nSparsity promoting priors on the weight, by using  regularization on .\nNon gaussian prior .","2025-nonlinear-factor-analysis#20.2.5 Nonlinear factor analysis":"The FA model assumes the observed data can be modeled with a linear mapping from a low-dimensional set of Gaussian factors.One way to relax such assumption is to let the mapping from  to  be a nonlinear model, such as a neural network. The model becomes:This is called nonlinear factor analysis.Unfortunately, we can‚Äôt compute the posterior or MLE exactly. Variational autoencoders is the most common way to approximate nonlinear FA model.","2026-mixture-of-factor-analysers#20.2.6 Mixture of factor analysers":"One other way to relax the assumption made by FA is assuming the model is only locally linear, so the overall model becomes a weighted combination of FA models. This is called a mixture of FA (MFA).The overall model for the data is a mixture of linear manifolds, which can be used to approximate the overall curved manifold.Let  be the latent indicator which specify which subspace we should use to generate the data.If , we sample  from a Gaussian prior and pass it through the  matrix and add noise.The distribution in the visible space is:In the special case of  we get a mixture of PPCA (although it is diffucilt to ensure orthogonality of the  in this case)."}},"/proba-ml/dimensionality-reduction/autoencoders":{"title":"20.3 Autoencoders","data":{"":"We can think of PCA and factor analysis (FA) as learning a mapping  called the encoder and another mapping  called the decoder.The model tries to minimize the following loss function:where the reconstruction function is .More generally, we can use:In this section, we consider the case where encoder and decoder are nonlinear mappings implemented by neural nets. This is called an autoencoder.If we use an MLP with one hidden layer, we get:We can think of the hidden units in the middle as a low-dimensional bottleneck between the input and its reconstruction.If the hidden layer is wide enough, we can‚Äôt prevent it from learning the identity function. To avoid this degenerate solution, we can set use a narrow bottleneck layer ; this is called undercomplete representation.The other approach is to use , known as an overcomplete representation, but to impose some kind of regularization, such as adding noise to the inputs, forcing the activation of the hidden units to be sparse, or imposing a penalty on the derivate of the hidden units.","2031-bottleneck-autoencoders#20.3.1 Bottleneck autoencoders":"We start by considering linear autoencoders, in which there is one hidden layer.The hidden units are computed using:and the output is reconstructed usingwhere ,  and .Hence the output of the model is:If we train this model to minimize the squared reconstruction error,one can show that  is an orthonal projection onto the first  eigenvectors of the covariance matrix of . This is therefore equivalent to PCA.If we introduce nonlinearity in the autoencoder, we get a model that is strictly more powerful than PCA.We consider both an MLP architecture (2 layers and a bottleneck of size 30), and a CNN based architecture (3 layers, and a 3d bottleneck with 64 channels).We use a Bernoulli likelihood model and binary cross entropy at the loss.We see that the CNN model reconstructs the images more accurately than the MLP model (although both models are small and only trained on 5 epochs).We visualize above the first 2 (of 30) latent dimensions produced by the MLP-AE, by plotting the tSNE embeddings, color coded by class label.We see that both this method and the CNN have done a good job of separating the classes in a fully unsupervised way. We also see that both latent spaces are similar when viewed through 2d projection.","2032-denoising-autoencoders#20.3.2 Denoising autoencoders":"One way to control the capacity of an autoencoder is to add noise to the input, and then train the model to reconstruct the uncorrupted version of the input. This is called a denoising autoencoder.We can implement this by adding Gaussian noise or Bernoulli dropout.We see that the model is able to hallucinate details that are missing in the input, because it has already see similar images before and can store this information in its parameters.Suppose we train a DAE using Gaussian corruption and squared error reconstruction, i.e. we use:and:where  is the residual error.Then one can show that as  (and with a sufficiently powerful model and enough data), the residuals approximate the score function, which is the log probability of the data:That is, the DAE learns a vector field, corresponding to the gradient of the log data density. Thus, points that are close to the manifold will be projected onto it via the sampling process.","2033-contractive-autoencoders#20.3.3 Contractive autoencoders":"A different way to regularize autoencoders is to adding a penalty term to the reconstruction loss:where  is the value of the th hidden embedding unit.We penalize the Frobenius norm of the encoder‚Äôs Jacobian. This is called a contractive AE (CAE). A linear operator with Jacobian  is called a contraction if  for all unit norm input .Intuitively, to minimize the penalty, the model would like to ensure the encoder is a constant function. However, if it was completely constant, it would ignore its input and hence incur high reconstruction error.Thus, the two terms together encourage to model to learn a representation where only a few units change in response to the most significant variation of the input.Unfortunately, CAEs are slow to train due to the expense of computing the Jacobian.","2034-sparse-autoencoders#20.3.4 Sparse autoencoders":"Another way to regularize AEs is to add a sparsity penalty to the latent activations of the form:This is called activity regularization.An alternative way to implement sparsity is to use logistic units, and then compute , the average activation on each unit  in a minibatch.We use:where  is the desired target distribution and  is the empirical distribution for unit , computed as:We show below the activation of the hidden layer for a AE-MLP (with 300 hidden units).","2035-variational-autoencoders-vae#20.3.5 Variational autoencoders (VAE)":"VAE can be thought of as a probabilistic version of the deterministic AE  we previously saw. The principle advantage is that VAE is a generative model that can create new samples, whereas an autoencoder generates embeddings of input vectors.VAEs combined two key ideas.First we create a non linear extension of the factor analysis generative model, i.e. we replace:with:where  is the decoder.For binary observations we should use a Bernoulli likelihood:Second, we create another model  called the recognition network or inference network, that is trained simultaneously with the generative model to approximate the posterior inference.If we assume the posterior is Gaussian with diagonal variance we get:Thus, we train an inference network to ‚Äúinvert‚Äù a generative network, rather than running the optimization algorithm to infer the latent code. This is called amortized inference.20.3.5.1 Trainings VAEsWe can‚Äôt compute the exact marginal likelihood  needed for MLE training, because posterior inference in a nonlinear FA model is intractable. However, we can use the inference network to compute an approximation of the posterior .We can then use this to compute the evidence lower bound (ELBO). For a single example , this is given by:This can be interpreted as the expected log likelihood, plus a regularizer that penalize the posterior from deviating too much from the prior. This is different from sparse AEs where we apply KL penalty to the aggregate posterior in each minibatch.The ELBO is a lower bound on the log marginal likelihood (aka evidence) as can be seen from Jensen‚Äôs inequality:Thus, for a fix inference network , increasing the ELBO should increase the log likelihood of the data.20.3.5.2 The reparameterization trickWe now discuss how to compute the ELBO and its gradient. Since   is Gaussian, we can write:where . Hence:Now the expectation is independent of the parameters of the model so we can push gradients inside and use backpropagation for training the usual way, by minimizing  wrt  and . This is known as the reparameterization trick.The first term of the ELBO can be approximated by sampling , scaling it by the output of the inference network to get , and then evaluating  using the decoder network.The second term of the ELBO is the KL of two Gaussian, which has a close form solution. In particular, by using  and , we get:(see section 6.3 for details).20.3.5.3 Comparison of VAEs and autoencodersVAEs are very similar to autoencoders, where the generative model  acts like the decoder and the inference network  acts like the encoder.Their reconstruction abilities are similar.The primary advantage of the VAE is that it can be used to generate new data from random noise. In particular, we sample , scale it to get  and then pass this through the decoder to get .The VAE‚Äôs decoder is trained to convert random points in the embedding space (generated by perturbing the input encodings) to sensible outputs.By contrast, the decoder for the deterministic AE only ever gets as input the exact encodings of the training set, so it is not equipped to handle random inputs outside of what it has been trained on. So, a standard AE can‚Äôt create new samples.The reason the VAE is better at sample is that it embeds images into Gaussians in latent space, whereas the AE embeds images into points, which are like delta functions.The advantage of using a latent distribution is that it encourages local smoothness, since a given image may map to multiple nearby places, depending on the stochastic sampling.By contrast, in a AE, the latent space is typically not smooth, so images from different classes often end-up next to each other.We can leverage smoothness of the latent space to perform linear interpolation.Let  and  be two images and let  and  be their embeddings. We can now generate new images that interpolate between these two anchors by computing:where , and then decoding by using .This is called latent space interpolation. We can use a linear interpolation since it has been shown that the learned manifold has approximately zero curvature."}},"/proba-ml/decision-theory/frequentist-hypothesis-testing":{"title":"5.5 Frequentist hypothesis testing","data":{"":"The Bayes factor  is expensive to compute since it requires integrating over all parametrization of model  and . It‚Äôs also sensitive to the choice of prior.","551-likelihood-ratio-test#5.5.1 Likelihood ratio test":"If we use 0-1 loss, and assume that , then the optimal decision rule is to accept  if:Gaussian meansIf we have two Gaussian distributions with  and  and known shared variance , the likelihood ratio test is:Thus the test only depends on the observed data on the sufficient statistic . From the figure below, we see that we accept  if :Simple vs compound parametersIn our simple hypothesis test above, parameters were either specified () or shared ().A compound hypothesis doesn‚Äôt specify all parameters, and we should integrate out these unknown parameters like in the Bayesian hypothesis testing:As an approximation, we can maximize them out, giving the maximum likelihood ratio.","552-null-hypothesis-significance-testing-nhst#5.5.2 Null hypothesis significance testing (NHST)":"Instead of assuming the 0-1 loss, we design a decision rule with a false positive (error type I) probability of , called the significance of the test.In our Gaussian example:Hence:with  the upper -quantile of the standard Normal.Let‚Äôs  be the false negative error (error type II) probability:The power of a test is , it is the probability of rejecting  when  is trueThe least power occurs when the two Gaussian overlap: When , for the same type I error,  dominates .","553-p-values#5.5.3 p-values":"Rather than arbitrarily declaring a result significant or not, we compute its p-value:If we accept hypothesis where , then 95% of the time we will correctly reject . However, it doesn‚Äôt mean that  is true with probability 0.95.That quantity is given by the Bayesian posterior","554-p-values-considered-harmful#5.5.4 p-values considered harmful":"The frequent and invalid reasoning about p-value is:‚ÄúIf  is true, then this test statistic would probably not occur. This statistic did occur, therefore  is false‚Äú.This gives us: ‚ÄúIf this person is American, he is probably not a member of congress. He is a member of Congress. Therefore he is probably not American‚ÄùThis is induction: reasoning backward from observed data to probable causes, using statistics regularity and not logical definitions. Logic usually works with deduction: .To perform induction, we need to compute the probability of :when the prior is uniform with  and the likelihood ratio .If ‚Äúbeing an American‚Äù is  and ‚Äúbeing a member of Congress‚Äù is , then  is low, and  is zero, thus the probability of  is 1, which follows intuition.The NHST ignores  and also , hence the wrong results. This is why p-values can be much different from .Which is far greater that the 5% probability people often associate with a p-values of","555-why-isnt-everyone-a-bayesian#5.5.5 Why isn‚Äôt everyone a Bayesian?":"The frequentist theory yields counter-intuitive results because it violates the likelihood principle, saying that inference should be made based on prior knowledge, not on some unseen future data.Bradley Efron wrote Why isn‚Äôt everyone a Bayesian, stating that if the 19th century was Bayesian and the 20th frequentist, the 21th could be Bayesian again.Some journals like The American Statistician have already banned or warn against p-values and NHST.Computation has traditionally been a major road block for Bayesian, which is less of an issue nowadays with fast algorithms and powerful computers.Also, the Bayesian modeling assumptions can be restraining, but this is true as well for the Frequentist since sampling distribution relies on some hypothesis about data generation.We can check models empirically using cross-validation, calibration and Bayesian model checking."}},"/proba-ml/dimensionality-reduction/intro":{"title":"20. Dimensionality reduction","data":{"":"A common form of unsupervised learning is dimensionality reduction, in which we learn a mapping from the high-dimensional visible space  to a low-dimensional latent space .This mapping can either be parametric , which can be applied to any input, or it can be a nonparametric mapping where we compute an embeddings  for each input  in the dataset, but not for any other point.The former is mostly used for data visualization whereas the latter can be used as a preprocessing step for other kind of learning algorithms. For example, we can produce an embedding by mapping  to , and then learn a simple linear classifier by mapping  to ."}},"/proba-ml/decision-theory/empirical-risk-minimization":{"title":"5.4 Empirical risk minimization","data":{"":"How to apply frequentist decision theory to supervised learning?","541-empirical-risk#5.4.1 Empirical risk":"In supervised learning, the true state of nature is the distribution  and the estimator  is a prediction function .We define the population risk as:We can approximate  using its empirical distribution:Plugging this gives the empirical risk: is a random variable since it depends on the training set. We chose an estimator by minimizing the empirical risk over a specific hypothesis space of functions :Approximation error vs generalization errorLet‚Äôs define:\n, the function that achieve the minimal possible population risk\n the best function of our hypothesis space \n the function that minimizes the empirical risk, since we can‚Äôt compute the population risk.\nOne can show that the risk of our chosen predictor can be compared to the best possible estimator with a two terms decomposition: the approximation error and the generalization error.We can approximate this by the difference between the training and testing set errors:We can reduce the approximation error with a more complex model, but it may result in overfitting and increasing the generalization error.Regularized riskWe add a complexity penalty  to the objective function. Since we usually work with parametric functions, we apply the regularizer to the parameters  themselves:with  a hyperparameter.If the risk is the log-loss and the penalty is the negative log prior, minimizing the empirical regularized risk is equivalent to estimating the MAP:","542-structural-risk#5.4.2 Structural risk":"We estimate the hyperparameters with bilevel optimization:However, this won‚Äôt work since this technique will always pick the least amount of regularization .If we knew the population risk, we could minimize the structural risk and find the right complexity (the value of ).We can estimate the population risk via cross-validation or statistical learning.","543-cross-validation#5.4.3 Cross validation":"Let  and  be two partitions of , and:For each model , we fit it to the training set to get .We then use the unregularized risk on the validation set as an estimate of the population risk:","544-statistical-learning-theory-slt#5.4.4 Statistical learning theory (SLT)":"Cross validation is slow, so in SLT we derive analytically upper bound of the population risk (or more precisely the generalization error). If the bound is satisfied, we can be confident that minimizing the empirical risk will have low population risk.For binary classifiers, we say the hypothesis is probably approximately correct (PAC), and the hypothesis class is PAC learnable.Bounding the generalization errorLet use a finite of hypothesis space and . For any dataset  of size  drawn from :where:\n is the population risk\n is the empirical risk\nVC dimensionIf the hypothesis space is infinite, we need to use an estimate of the degree of freedom of the hypothesis class. This is called VC dimension. Unfortunately this is hard to compute and the bounds are very loose.However various estimates of the generalization error like the PAC-Bayesian bounds have recently been designed, especially for DNNs."}},"/proba-ml/dimensionality-reduction/word-embeddings":{"title":"20.5 Word embeddings","data":{"":"Words are categorical random variable, so their one-hot representation is sparse.The issue with this representation is that semantically similar words can have very different vector representation. ‚Äúman‚Äù and ‚Äúwoman‚Äù will have the same Hamming distance of 1 than ‚Äúman‚Äù and ‚Äúbanana‚Äù.The standard way to solve this is to use word embeddings. Instead of using sparse vectors , we can use a denser low dimensional representation , for the th word in the th document. This can significantly help with sparsity.We define semantic similarity by saying that similar words occur in similar contexts. This is known as the distributional hypothesis, summarized by ‚Äúa word is characterized by the company it keeps‚Äù.","2051-latent-semantic-analysis--indexing#20.5.1 Latent semantic analysis / indexing":"We discuss a simply way to learn word embeddings based on a SVD of a term-frequency count-matrix.","20511-latent-semantic-indexing-lsi#20.5.1.1 Latent semantic indexing (LSI)":"Let  be the frequency of ‚Äúterm‚Äù  occurs in ‚Äúcontext‚Äù .The definition of ‚Äúterm‚Äù is application specific, often considered to be the words by simplicity. We may remove infrequent or very frequent words during preprocessing.The definition of ‚Äúcontext‚Äú is also application specific. We often consider a set of document or corpus.  is then called the term-document frequency matrix.Sometimes we apply TF-IDF transformation to the count, as discussed in section 1.5.4.2.Let  be the count matrix and  the rank  approximation the minimizes the following loss:One can show that the minimizer of this loss is given by the rank  truncated SVD approximation .This means we can represent each  as a bilinear product:We define  to be the embedding for word , and  the embedding for context .We can use these embeddings for document retrieval. The idea is to compute embeddings for the query words, and compare this to the document embeddings . This is known as latent semantic embedding (LSI).More precisely, suppose the query is a bag of words , represented by the vector:Then, we rank document by the cosine similarity between the query vector and the document:Note that if the vectors are unit norm, cosine similarity is the same as inner product. It is also equal to squared euclidean distance, up to a change of sign and an additive constant.","20512-latent-semantic-analysis-lsa#20.5.1.2 Latent semantic analysis (LSA)":"Now suppose we define context more generally to be some local neighborhood of words  where  is the window size.Thus  is how many times the word  occurs in a neighborhood of type . We have the same formula as previously, and this is known as latent semantic analysis (LSA).For example, we compute  on the British National Corpus, and retrieve the  nearest neighbors ranked by cosine similarity. If the query word is ‚Äúdog‚Äù, and we use  or , the nearest neighbors are:The 2-word context window is more sensitive to syntax, while the 30-word window is more sensitive to semantics. The ‚Äúoptimal‚Äù value of  depends on the application","20513-pointwise-mutual-information-pmi#20.5.1.3 Pointwise mutual information (PMI)":"In practice, LSA (and similar methods) give better results if we replace the raw counts  by the pointwise mutual information (PMI).If word  is strongly associated to document , we have .If the PMI is negative it means that  and   co-occurs less that if they were independent. However, such negative correlation can be unreliable, so it is common to use the positive PMI:It has been showed that the SVD applied to the PPMI matrix result in word embeddings that perform well on various tasks related to word meaning.","2052-word2vec#20.5.2 Word2vec":"The popular word2vec model is a shallow neural net for predicting a word given its context. In section 20.5.5 we will discuss it connection with the SVD of the PMI matrix.There are two versions of word2vec: continuous bag of words (CBOW) and skipgram.","20521-word2vec-cbow-model#20.5.2.1 Word2vec CBOW model":"In the CBOW model, the log likelihood of a sequence is:where  is the vector for word at location ,  is the context size,  the vocabulary andis the average of word vectors in the window around word .Thus, we will try to predict each word given its context. The model is called CBOW because it uses a bag of words assumption for the context, and represent each word by its embedding.","20522-word2vec-skip-gram-model#20.5.2.2 Word2vec Skip-gram model":"A variant of CBOW is to predict the context (surrounding words) given a word:We define the log probability of some other context word  given the central word :See the derivation of the loss to obtain the gradient.","20523-negative-sampling#20.5.2.3 Negative sampling":"Computing the conditional probability of each word in the previous equation is expensive, because it needs to normalize over all possible words in . This makes computing the gradient slow for the CBOW and skip-gram models.A fast approximation has been proposed, called skip-gram with negative sampling (SGNS).The basic idea is to create a set of  context words for each central word  and label the one that actually occurs as positive and the rest as negative.The negative words are called noise words and can be sampled from a reweighted unigram distribution , which has the effect of redistributing probability mass from common to rare words.The conditional probability is now approximated by:where  are noise words, and  is the event that the word pair actually occurs in the data.The binary probabilities are given by:To train the model, we can then compute the log probability of the data and optimize the embedding vectors  and  for each word using SGD.See this implementation.","2053-glove#20.5.3 GloVE":"A popular alternative to Skipgram is GloVe (global vectors for word representation). This methods use a simpler objective, faster to optimize.Recall that in the skipgram model, the predicted conditional probability of word  occuring in the context window of central word  is:Let  the number of time the word  occurs in any context of the word  (note that this is symmetric, ).Then we can rewrite the loss as:If we define  to be the empirical probability of word  occuring in the context window of central word , we can rewrite the skipgram loss as a cross entropy loss:The issue with this objective is that computing  is expensive due to the normalization term over all words.In GloVe, we work with unnormalized probabilities:where  are bias term to capture marginal probabilities.In addition, we minimize the square loss,  , which is more robust to errors in estimating small probabilities than log loss.Finally, we down-weight rare words, which carry noise, by:with .This gives the final GloVe objective:We can precompute  offline, and then optimize the above objective using SGD.After training, we define the embedding of word  to be the average of  and Empirically, GloVe gives similar results to skipgram, but is faster to train.","2054-word-analogies#20.5.4 Word analogies":"One of the remarkable properties of word embeddings produced by word2vec or GloVe is that the learned vector space capture relational semantics in terms of vector addition.Suppose we take the words . How do we find ?Let  be the vector representing the concept of ‚Äúconverting the gender from male to female‚Äù. Intuitively, we can find the word  by computing , and then finding the closest word in the vocabulary to .It has been conjectured that  holds iff for every word  in the vocabulary, we have:This follows the Rand-Walk modeling assumption.","2055-rand-walk-model-of-word-embeddings#20.5.5 Rand-Walk model of word embeddings":"Word embeddings significantly improve the performance of various kinds of NLP models compared to using one-hot encodings. We explain why in this section.Consider a sequence of words , we assume each word is generated by a latent context  using the following bilinear language model:where  is the mebedding for word , and  is the partition function.Let us further assume the prior for the word embedding  is an isotropic Gaussian, and that the latent topic  undergoes a slow Gaussian random walk.Under this model, one can show that  is approximately equal to a fixed constant, , independent of the context. This is known as the self-normalizing property of log-linear model.Furthermore, one can show that the pointwise mutual information of predictions from the model is given by:We can therefore fit the Rand-Walk model by matching the model‚Äôs predicted values for PMI with the empirical values, i.e. we minimize:where  is the number of times  and  occur next to each other.This objective can be seen as frequency-weighted version of the SVD loss in LSI (section 2.5.1.1 above).Some additional approximations can be used to show that the NLL for the Rand-Walk model is equivalent to the CBOW and SGNS word2vec objectives. We can also derive the objective for GloVe from this approach.","2056-contextual-word-embeddings#20.5.6 Contextual word embeddings":"In the sentences ‚ÄúI ate an apple‚Äù and ‚ÄúI bought a new phone from Apple‚Äù, the meaning of the word ‚Äúapple‚Äù is different in both cases, but a fixed word embedding, of the type we saw in this section, wouldn‚Äôt be able to capture this.In section 15.7 we discuss contextual word embeddings, where the embedding of a word is a function of all the words in its context (usually a sentence). This can give much improved results, and is currently the standard approach to representing natural language data, as a pre-processing step before doing transfer learning."}},"/proba-ml/decision-theory/frequentist-decision-theory":{"title":"5.3 Frequentist decision theory","data":{"":"In frequentist decision theory, we don‚Äôt use prior and thus no posterior, so we can‚Äôt define the risk as the posterior expected loss anymore.","531-computing-the-risk-of-an-estimator#5.3.1 Computing the risk of an estimator":"The frequentist risk of an estimator , applied to data  sampled from the likelihood :ExempleWe estimate the true mean of a Gaussian. Let . We use a quadratic loss, so the risk is the MSE.We compute the risk for different estimators, the MSE can be decomposed:with .\n is the sample mean. This is unbiased, so its risk is:\n. This is also unbiased. One can show its variance is approximately:\n returns the constant  so its biased is , and its variance is zero. Hence:\n is the posterior mean under a  prior:\nWe can derive its MSE as follow:The best estimator depends on , which is unknown. If  is far from , the MLE is best.Bayes riskIn general the true distribution of   is unknown, so we can‚Äôt compute . One solution is to average out all values of the prior  for . This is the Bayes risk or integrated risk.The Bayes estimator minimizes the Bayes risk:which corresponds to optimal policy recommended by Bayesian decision theory here.Maximum riskTo avoid using a prior in frequentist, we can define the maximum risk:To minimize the maximum risk, we use minimax . Computing them can be hard though.","532-consistent-estimators#5.3.2 Consistent estimators":"An estimator  is consistent when  as , where the arrow is the convergence in probability.This is equivalent to minimizing the 0-1 loss .An example of consistent estimator is the MLE.Note that an estimator can be unbiased but not consistent, like . Since  this is unbiased, but the sampling distribution of  doesn‚Äôt converge to a fix value so it is not consistent.In practice, it is more useful to find some estimators that minimize the discrepancy between our empirical distribution  and the estimated distribution . If this discrepancy is the KL divergence, our estimator is the MLE.","533-admissible-estimators#5.3.3 Admissible estimators":"dominates  if:An estimator is admissible when it is not dominated by any others.In figure 5.8 above, we see that the risk of the sample median  is dominated by the sample mean .However this concept of admissibility is of limited value, since  is admissible even though this doesn‚Äôt even look at the data."}},"/proba-ml/dimensionality-reduction/manifold-learning":{"title":"20.4 Manifold learning","data":{"":"We discuss the problem of recovering the underlying low-dimensional structure of a high-dimensional dataset. This structure is often assumed to be a curved manifold so this problem is called manifold learning or nonlinear dimensionality reduction.The key difference from methods like AE is that we will focus on non-parametric methods, in which we compute an embedding for each point in the training set, as opposed to learning a generic model that can embed any input vector.This is, the methods we discuss don‚Äôt (easily) support out-of-sample generalization. However, they can be easier to fit and are quite flexible.Such methods can be useful for unsupervised learning, data visualization, and as a preprocessing step for supervised learning.","2041-what-are-manifolds#20.4.1 What are manifolds?":"Roughly speaking, a manifold is a topological space that is locally Euclidean. One of the simplest examples is the surface of the earth, which is a curved 2d surface embedded in a 3d space. At each local point on the surface, the earth seems flat.More formally, a -dimensional manifold  is a space in which each point  has a neighborhood which is topologically equivalent to a -dimensional Euclidean space, called the tangent space, denoted .A Riemannian manifold is a differentiable manifold that associates an inner product operator at each  in tangent space. This is assumed to depend smoothly on the position .The collection of these inner product is called a Riemannian metric. It can be shown that any sufficiently smooth Riemannian manifold can be embedded in any Euclidean space of potentially higher dimension. The Riemannian inner product at a point becomes Euclidean inner product in the tangent space.","2043-approaches-to-manifold-learning#20.4.3 Approaches to manifold learning":"The manifold methods can be categorized as:The term ‚Äúnonparametric‚Äù refers to methods that learn a low dimensional embedding  for each datapoint , but do not learn a mapping function that can be applied to out-of-sample example.To compare methods, we use 2 different datasets: a set of 1000 3d-points sampled from the 2d ‚ÄúSwiss roll‚Äù manifold, and a set of 1797 points from the UCI digit datasets.We will learn a 2d manifold to visualize the data.","2044-multi-dimensional-scaling-mds#20.4.4 Multi-dimensional scaling (MDS)":"The simplest approach to manifold learning is multi-dimensional scaling (MDS). This tries to find a set of low dimensional vectors .20.4.4.1 Classical MDSWe start with . The centered Gram (similarity) matrix is:In matrix notation, we have  with  and Now define the strain of a set of embeddings:where  is the centered vector.Intuitively, this measures how well similarities in the high-dimensional space are preserved in the low-dimensional space. Minimizing this loss is called classical MDS.We know that the best rank  approximation to a matrix is its truncated SVD representation . Since  is positive semi-definite, we have .Hence the optimal embeddings satisfies:Thus we can set the embedding vectors to be the rows of .We can still apply the MDS when we just have the Euclidean distance  instead of the raw data .First, we compute the squared Euclidean matrix:So, we see that  only differs from  a  factor and some rows and columns constants. Thus, we can apply the double centering trick on  to obtain :or equivalently:We can then compute the embeddings as before.It turns out the classical MDS is equivalent to PCA.To see this, let  be the rank  truncated SVD of the centered kernel matrix.The MDS embeddings are given by .Now consider the rank  SVD of the centered data matrix  The PCA embedding is .Now:Therefore, ,  and .20.4.4.2 Metric MDSClassical MDS assumes Euclidean distances. We can generalize it using any dissimilarity measure by defining the stress function:where .Note that this is a different objective than the one used with classical MDS, so even using Euclidean distances the results will be different.We can use gradient descent to solve the optimization problem, however it‚Äôs better to use a bound algorithm problem called Scaling by Majorizing a Complication Function (SMACOF).This is the method implemented in scikit-learn.20.4.4.3 Non-metric MDSInstead of trying to match the distance between points, we can instead try to match the ranking of how similar points are. Let  be a monotonic transformation from distances to ranks.The loss is:where . Minimizing this is known as non-metric MDS.This objective can be optimized iteratively.First, the function  is approximated using isotonic regression; this finds the optimal monotonic transformation of the input distance to match the current embedding distances.Then, the embeddings  are optimized, using a given , using gradient descent, and the process repeats.","2045-isomap#20.4.5 Isomap":"If the high-dimensional data lies on or near a curved manifold, such as the Swiss roll example, then MDS might consider the two points to be close even if their distance along the manifold is large.One way to capture this is to create the -nearest neighbor graph (by using scikit-learn) between datapoints and then approximate the manifold distance between a pair of points by the shortest distance on the graph.This can be computed efficiently using Dijkstra‚Äôs shortest path algorithm. Once we have computed this new distance metric, we use classical MDS (i.e. PCA).This is a way to capture local structure while avoiding local optima. The overall method is called Isometric mapping (Isomap).The results are quite reasonable.However, if the data is noisy, there can be ‚Äúfalse‚Äú edges in the nearest neighbor graph, which can result in ‚Äúshort circuits‚Äù, which significantly distort the embedding.This problem is known as topological instability. Choosing a very small neighborhood doesn‚Äôt solve the problem since this can fragment the manifold into a large number of disconnected regions.See more details on scikit-learn documentation.","2046-kernel-pca#20.4.6 Kernel PCA":"We now consider nonlinear projections of the data. The key idea is to solve PCA by finding the eigenvectors of the inner product (Gram) matrix  and then to use the kernel trick, which lets us replace inner product  with kernel function . This is known as kernel PCA.Recall from Mercer‚Äôs theorem that the use of kernel implies some underlying feature space, we are implicitly replacing  by .Let  be the corresponding (notional) design matrix, and  be the covariance matrix in feature space (we assume   are centered)From section 20.1.3.2, we know:where  and  contains the eigenvectors and eigenvalues of .We can‚Äôt compute  directly since the  might be infinite dimensional. However, we can compute the projection of the test vector  on the feature space:where If we apply kPCA with a linear kernel we recover the regular PCA. This is limited to using  embedding dimensions.\nIf we use a non-degenerate kernel, we can use up to  components, since  where  is the dimensionality of embedded feature vectors (potentially infinite).The figure below gives an example of the method with  using a RBF kernel.We obtain the blue levels using the following method: for each plot, we project points in the unit grid onto each component by computing  and its product with , obtained from the data points. We obtain a ‚Äúdensity map‚Äù for the points in the grid, and then draw a contour plot.We see that the first two components separate the three clusters, while the other split the clusters.The figure below show that the kPCA is not very useful for our dimension reduction problem. It can be shown that kPCA expand features instead of reducing it.","2047-maximum-variance-unfolding-mvu#20.4.7 Maximum variance unfolding (MVU)":"kPCA with RBF kernel might not result in low dimensional embedding. This observation led to the development of the semidefinite embedding algorithm, aka maximum variance unfolding, which tries to learn an embedding  such that:when  is the nearest neighbor graph (as in Isomap).This approach tries to ‚Äúunfold‚Äù the manifold while respecting its nearest neighbor constraints.This can be reformulated as a semidefinite programming (SDP) problem by defining the kernel matrix  and maximizing:The resulting kernel is then passed to kPCA, and the resulting eigenvectors give the low dimensional embedding.","2048--local-linear-embedding-lle#20.4.8  Local linear embedding (LLE)":"The techniques discussed so far relied on the eigenvalue decomposition of a full matrix of pairwise similarities, either in the input space (PCA), in the feature space (kPCA) or along the KNN graph (Isomap).In this section we discuss local linear embedding (LLE), a technique that solves a sparse eigenproblem, by focusing on local structure in the data.LLM assumes the data manifold around each point  is locally linear. The best linear approximation can be found by predicting   as a linear combination of its  nearest neighbors using reconstruction weights  (aka the barycentric coordinate of :Any linear mapping of this hyperplane to a lower dimensional space preserves the reconstruction weights, and thus the local geometry.Thus we can solve for the low dimensional embeddings for each point by solving:with the same constraints.We can rewrite this loss as:As shown in section 7.4.8, the solution is given by the eigenvectors of  corresponding to the smallest non-zero eigenvalues.The results do not seem as good as the one obtained with the Isomap, however this method tend to be more robust to short-circuiting (noise).","2049-laplacian-eigenmaps-spectral-embeddings#20.4.9 Laplacian eigenmaps (Spectral embeddings)":"The key idea of Laplacian eigenmaps is to find a low-dimensional representation of data in which the weighted distances between a datapoint and its  nearest neighbors are minimized.We put more weight on the first nearest neighbor than the second, etc.20.4.9.1 Using eigenvectors of the graph Laplacian to compute embeddingsWe want to find the embeddings that minimizes:where  if  are neighbors on the KNN graph, 0 otherwise.We add the constraint  to avoid the degenerate solution where , where  is the diagonal weight matrix storing the degree of each node We can rewrite the objective as follows:where  is the graph Laplacian.One can show that minimizing this graph is equivalent to solving the (generalized) eigenvalue problem:for the  smallest nonzero eigenvalues.20.4.9.2 What is the graph Laplacian?We saw above that we need to compute the eigenvectors of the graph Laplacian to compute a good embedding of the high dimensional points. We now give some intuitions on why this works.We saw that the elements of  can be computed as:Suppose we associate a value  to each node of the graph:Then we can use the graph Laplacian as a difference operator, to compute a discrete derivative of a function at a point:where  refers to the set of neighbors of node .We can also compute an overall measure of smoothness of the function by computing its Dirichlet energy:Applying linear algebra to study the adjacency matrix of a graph is called spectral graph theory.For example, we see that  is semi-positive definite, since   for all .Consequently,  has  nonzero, real-valued eigenvalues .The corresponding eigenvectors form an orthogonal basis for the function  defined on the graph, in order of decreasing smoothness.There are many applications of the graph Laplacian in ML, like normalized cuts (which is way of learning a clustering based on pairwise similarity) and RL where the eigenvectors of the state transition matrix are used.","20410-t-sne#20.4.10 t-SNE":"We describe a very popular nonconvex technique for learning low dimensional embeddings, called t-SNE. This is an extension of the earlier stochastic neighbor embedding (SNE).20.4.10.1 Stochastic neighbor embedding (SNE)The basic idea is to convert a high-dimensional Euclidean distance into a conditional probability. More precisely, we define  to be the probability that point  would pick point  as its neighbor:Here  is the variance of data point , which can be used to magnify the scale of points in dense regions of input space, and diminish the scale in sparser regions.Let  be the low-dimensional embedding representing . We define similarities in the low-embedding space in a similar way:If the embedding is good, then  should match . Therefore, SNE defines the objective:where  is the conditional distribution over all other data points given .Note that this is an asymmetric objective: the cost is high if a small  is used to model a large . This objective will prefer pulling distance point together rather than pushing nearby point  apart.We can get a better sense of the geometry by computing the gradient of the objective at :Thus points are pulled together if the ‚Äôs are bigger than the ‚Äôs, and repelled otherwise.This objective is not convex, but it can be minimized using SGD nevertheless. In practice, it is helpful to add Gaussian noise to the embeddings and anneal the amount of noise.20.4.10.2 Symmetric t-SNEWe can simplify SNE by minimizing a single KL between the joint distribution  and :This is called symmetric SNE.We can define  as:This corresponds to the gradient:Which gives similar results than regular SNE when .20.4.10.3 t-SNEA fundamental problem with SNE and many other embedding techniques is that they tend to squeeze points that are far in the high dimensional space close together in the low dimensional embedding space, this is called the crowding problem. This is due to the use of squared errors (or Gaussian probabilities).One solution to this is to use a probability distribution in latent space that has heavier tails, which eliminates the unwanted force of attraction between distant points in the high dimensional space.We can choose the Student-t distribution. In t-SNE, they set the degree , so the distribution is equivalent to a Cauchy:This uses the same objective as the symmetric SNE, and the gradient turns out to be:The new term acts like an inverse square law. This means data points is the embedding space act like stars and galaxies, forming well separated clusters each of which has many stars tightly packed inside.This can be useful to separate different classes of data in a unsupervised way.20.4.10.4 Choosing the length scaleThe local bandwidth  is an important parameter in t-SNE, and is usually chosen so that  has  a perplexity chosen by the user, defined as  where:This can be interpreted as smooth measure of the effective number of neighbors.Unfortunately, the results of t-SNE can be quite sensitive to the perplexity parameter, so it is often advised to run the algorithm with different values.If the perplexity is too small, the algorithm will find structure within the cluster which is not present. At perplexity 30 (the default for scikit-learn), the clusters seem equidistant in the embedding space.20.4.10.5 Computational issuesThe Naive implementation of t-SNE takes  time. A faster version can be created by leveraging an analogy to the -body simulation in physics.In particular, the gradient requires computing the forces of  points on each of   points. However, points that are far away can be grouped into clusters (computationally speaking), and their effective force can be approximated by a few representatitve points per cluster.We can then approximate the forces using the Barnes-Hut algorithm, which takes .Unfortunately, this only works well for low dimensional embeddings, such as  (which is adapted to data visualization).","20411-umap#20.4.11 UMAP":"Various extension of t-SNE have been proposed to improve its speed, the quality of its embedding space or the ability to embed into more than 2 dimensions.Uniform Manifold Approximation and Projection (UMAP) is a popular recent extension. At a high level, it is similar to t-SNE, but it tends to preserve global structure better, and it is much faster. This makes it easier to try different hyperparameters."}},"/proba-ml/examplar-based-models/intro":{"title":"16. Exemplar-based Models","data":{"":"So far, we have dealt with parametric models, either unconditional  or conditional . is a vector of parameters estimated from a training dataset , which is thrown away after training.In this section, we consider various kinds of nonparametric model that keep the training data at test time ‚Äîwe call them examplar-based models.Therefore, the number of parameters can grow with , and we focus on the similarity (or distance) between a test input  and training inputs ."}},"/proba-ml/dimensionality-reduction/principal-component-analysis":{"title":"20.1 Principal component analysis (PCA)","data":{"":"The simplest and most widely used form of dimensionality reduction is PCA. The basic idea is to find a linear and orthogonal projection of the high dimensional data  to a low dimensional subspace , such that the low dimensional representation is a ‚Äúgood approximation‚Äù to the original data.More specifically, if we project or encode  to get  and then unproject or decode  to get , then we want  and  to be close in the  distance.We define the following reconstruction error or distortion:where the encoder and decoder stages are both linear maps.We can minimize this objective by setting , where  contains the  eigenvectors with the largest eigenvalues of the empirical covariance matrix:where  is the centered version of the  design matrix.This is equivalent to maximizing the likelihood of a latent linear Gaussian model known as probabilistic PCA.","2011-examples#20.1.1 Examples":"We project below 2d data to a 1d line. This direction captures the most variation in the data.We show PCA applied to the Olivetti face image dataset, which is a set of 64 x 64 grayscale images.We project these to a 3d subspace and display the basic vectors (the columns of the projection matrix ) known as eigenfaces. We see that the main mode of variation of the data are related to overall lighting and then differences around the eyebrowsIf we use enough dimensions (but less than the original 4096) we can use the representation   as input to a nearest-neighbor classifier to perform face recognition. This is faster and more reliable than working in the pixel space.","2012-derivation-of-the-algorithm#20.1.2 Derivation of the algorithm":"Suppose we have an unlabeled dataset  where . We can represent this as a  data matrix.We assume , which can be assured by centering the data.We would like to approximate each  by a latent vector . The collection of these latent variable are called the latent factors.We assume each  can be ‚Äúexplained‚Äù in terms of a weighed combination of basis functions  where each  and the weigths are , i.e we assume:We can measure the reconstruction error produced by this approximation as:We want to minimize this subject to the constraint that  is an orthogonal matrix.We show below that the optimal solution is obtained by setting  where  contains the  eigenvectors with largest eigenvalues of the empirical covariance matrix.20.1.2.1 Base caseWe start by estimating the best 1d solution . We will find the remaining basis vectors later.Let the latent coefficient for each data points associated with the first basis vector be The reconstruction error is given by:where  by the orthonormality assumption.Taking derivative wrt  gives:So the optimal embedding is obtained by orthogonally projecting the data onto .Plugging it back to loss gives us:Where  is the empirical covariance matrix (since we assumed the data is centered) and  a constant.We can trivially optimize this by letting , so we impose the constraint  and instead optimize:where  is a Lagrange multiplier.Taking derivatives and equating to zero we have:Hence, the optimal direction onto which should project the data is an eigenvector of the covariance matrix.And since we have:We can minimize the loss in (13) by picking the largest eigenvector which corresponds to the largest eigenvalue.20.1.2.2 Optimal weight vector maximizes the variance of the projected dataBefore continuing, we observe that, since the data has been centered:Hence the variance of the projected data is given by:From this we see that minimizing the reconstruction error is equivalent to maximizing the variance of the projected data.This is why it is often said that PCA finds the direction of maximal variance.20.1.2.3 Induction stepNow let us find another direction  to further minimize the reconstruction error, subject to  and :Optimizing wrt  and  gives the same solution as before.We can show that  yields .Substituting this solution yields:Dropping the constant term, plugging the optimal  and adding the constraint yields:The solution is given by the eigenvector with the second largest eigenvalue:The proof continue that way to show that .","2013-computational-issues#20.1.3 Computational issues":"20.1.3.1 Covariance matrix vs Correlation matrixWe have been working with the covariance matrix, but it‚Äôs better to work with the correlation matrix because PCA might be ‚Äúmislead‚Äù by the directions in which the variance is higher because of the measurement scale.20.1.3.2 Dealing with high-dimensional dataWe have introduced PCA as finding the eigenvectors of the covariance matrix .When , it is faster to work with the Gram matrix  instead. We show how to use it.Let  be the eigenvectors of  with corresponding eigenvalues . By definition, we have:so by left-multiplying by we get:from which we see that the eigenvectors of  are , with the same eigenvalues .However, these eigenvectors are not normalized, since:The normalized eigenvector are given by:This provides an alternative way to compute the PCA basis, it also allows us to use to kernel trick.20.1.3.3 Computing PCA using SVDWe now show the equivalence between the PCA computed using eigenvectors and the truncated SVD.Let    be the top  engendecomposition of the covariance matrix  (assuming  is centered).We know that the optimal estimate of the projection is .Now let  be the -truncated SVD approximation of the matrix .We know that the right singular vectors of  are the eigenvectors of , so .In addition, the singular values of  are related to the eigenvalues of  with:Now suppose we are interested in the principal components, we have:Finally, if we want to reconstruct the data:This is precisely the same as truncated SVD approximation.Thus, we see that we can perform PCA either using a eigendecomposition of  or a SVD decomposition of . The latter is often preferable for computational reason.For very high dimensional problem, we can use a randomized SVD algorithm.","2014-choosing-the-number-of-latent-dimensions#20.1.4 Choosing the number of latent dimensions":"20.1.4.1 Reconstruction errorLet us define the reconstruction error on some dataset  when using  dimensions:where the reconstruction is given by  and .We see that it drops quite quickly after, indicating that we capture most of the empirical correlation with a small number of factors.Of course, if we choose , we get zero reconstruction error on the training set. To avoid overfitting, we compute the reconstruction error on the test set.Here we see that the error continues to decrease even when the model gets more complex. Thus, we don‚Äôt see the typical U-shaped curve that we see in supervised learning.The problem is that PCA is not a proper generative model of the data: if you give it more latent dimensions, it will approximate the test data more accurately. A similar problem arises if we plot reconstruction error on the test set with K-means clustering.20.1.4.2 Scree plotA common alternative to plotting reconstruction error vs  is to plot the eigenvalues in decreasing order of magnitude.We can show that:Thus as the number of dimensions increases, the eigenvalues get smaller and so do the reconstruction error.An related quantity is the fraction of variance explained:20.1.3.4 Profile likelihoodAlthough there is no U-shape in the reconstruction error plot, there is sometimes an ‚Äúelbow‚Äù where the error suddenly changes from relatively large to relatively small.The idea is that, for  where  is the ‚Äútrue‚Äù latent dimensionality, the rate of decrease of errors will be high, whereas for  the gains will be smaller since the model is already complex enough to capture the true distribution.One way to automate the detection of the change of gradient in the curve is to profile likelihood.Let  be some measure of error incurred by the model of size , such that . In PCA these are the eigenvalues but the method can be applied to reconstruction error from K-means clustering.We partition these values into two group depending on whether  or , where  is some threshold.To measure the quality of , we use a changepoint model such that:It‚Äôs important that  be the same to avoid overfitting in the case on regime has more data than the other. Within each of the two regimes, we assume  are iid, which is obviously incorrect but adequate for our present purposes.We can fit this model for each  by partitioning the data and computing the MLEs, using a pool estimate of the variance:We can then evaluate the profile log likelihood:We that the peak  is well determined."}},"/proba-ml/examplar-based-models/kernel-density-estimation":{"title":"16.3 Kernel density estimation (KDE)","data":{"":"KDE is a form of non-parametric density estimation. This also a form of generative model, since it defines a probability distribution  that can be evaluated pointwise, and which can be sampled to generate new data","1631-density-kernels#16.3.1 Density kernels":"Density kernels are function , such that  and .This latter symmetry property implies  and hence:A simple example of such a kernel is the boxcar kernel, which is the uniform distribution around within the unit interval around the origin:Another example is the Gaussian kernel:We can control the width of the kernel by introducing a bandwidth parameter :We can generalize to vector valued inputs by defining a radial basis function (RBF) kernel:In the case of the Gaussian kernel this becomes:Although the Gaussian kernel is popular, it has unbounded support. Compact kernels can be faster to compute.","1632-parzen-window-density-estimator#16.3.2 Parzen window density estimator":"We now explain how to use kernels to define a nonparametric density estimate.Recall the form of Gaussian mixture, with a fixed spherical Gaussian covariance and uniform mixture weights:One problem with this model is that it requires specifying the number  of clusters and their positions .An alternative is to allocate one cluster center per data point:This can be generalized to:This is called a Parzen window density estimator or kernel density estimator (KDE).Its advantage over a parametric model is that no fitting is required (except for choosing ) and there is no need to select the clusters.The drawback is that it takes a lot of memory and a lot of time to evaluate.The resulting model using the boxcar kernel simply count the number of points within a window of size . The Gaussian kernel gives a smoother density.","1633-how-to-choose-the-bandwith-parameter#16.3.3 How to choose the bandwith parameter":"The bandwidth parameter  controls the complexity of the model.In 1d data, if we assume the data has been generated from a Gaussian distribution, one can show the bandwidth minimizing the frequentist risk is given by:We can compute a robust approximation to the standard deviation by first computing the median absolute deviation (MAD):and then using .If we have  dimension, we can estimate each  separately for each dimension and then set:","1634-from-kde-to-knn-classification#16.3.4 From KDE to KNN classification":"We previously discussed the  neighbor classifier as a heuristic approach to classification. Interestingly, we can derive it as a generative classifier in which the class conditional densities  are modeled using a KDE.Rather than a fixed bandwidth a counting the points within a hypercube centered on a datapoint, we allow the bandwidth to be different for each point.We ‚Äúgrow‚Äù a volume around  until we captured  points, regardless of their class label. This is called a balloon density kernel estimator.Let the resulting volume have size  (this was previously  and let there  example of the class  in this volume. We can then estimate the class conditional density:where  is the total number of point with class labels  in the dataset.If we take the class prior to be , we have the posterior:","1635-kernel-regression#16.3.5 Kernel regression":"Just as KDE can be use for generative classifiers, it can also be used as generative models for regression.16.3.5.1 Nadaraya-Watson estimator for the meanIn regression, our goal is to compute:If we use a MVN for , we derive a result which is equivalent to linear regression.However, the Gaussian assumption on  is rather limiting. We can use KDE to more accurately approximate this joint density:Hence, using the previous kernel properties:where:We see that the prediction is just a weighted sum of the training labels, where the weights depend on the similarity between  and the stored training points.This method is called kernel regression, kernel smoothing, or the Nadaraya-Watson (N-W) model.16.3.5.2 Estimator for the varianceSometimes it can be useful to compute the predictive variance, as well as the predictive mean. We can do this by noting that:where  is the Nadara-Watson estimate.If we use a Gaussian kernel with variance , we can compute:where we used the fact that:Finally:16.3.5.3 Locally weighted regressionWe can drop the normalization term to get:Rather than interpolating the stored labels , we can fit a locally linear model around each training point:This is called locally linear regression (LLR), or locally-weighted scatterplot smoothing (LOWESS or LOESS).This is often used when annotating scatter plots with local trend lines."}},"/proba-ml/generalized-linear-models/insurance-claims":{"title":"12.5 Worked example: predicting insurance claims","data":{"":"In this section, we predict the frequency of insurance claims using linear and Poisson regression. The frequency is the number of claims divided by the duration of the contract (in years).The following is borrowed from a scikit-learn tutorial.We plot the test set and see that for 94% of policies, no claims are made, so the target has a lot of zeros, which is typical for rates and counts. The average frequency is 10%, which can be converted into a dummy regressor, which always predicts the average. Our goal is to do better than this.A simple approach is to use a ridge regression, with minimal feature engineering on the categorical columns. It is better than the baseline, but not great. We see that it fails to capture the long tail and makes some negative predictions.We can do better with a Poisson regression, using the same features.Metrics like MSE and MAE fail to identify Poisson as a better solution than Ridge.Instead, we can use the deviance, defined as:where  is the predicted mean parameter and  is the optimal parameter.In the case of Poisson regression we have , hence:We can also compute a calibration, which plots the actual frequency against the predicted frequency. This is accomplished by binning the predictions and counting the empirical frequencies that fall into each bin.We see that the dummy regressor is well-calibrated, but not very accurate. The ridge is miscalibrated for the lower and higher frequency regimes, unlike the Poisson which is well-calibrated."}},"/proba-ml/examplar-based-models/learning-distance-metrics":{"title":"16.2 Learning distance metrics","data":{"":"Being able to compute the semantic distance between a pair of points, , for  or equivalently their similarity , is of crucial importance to tasks such as nearest neighbors classification, self-supervised learning, similarity-based clustering, content-based retrieval etc.When the input space is , the most common metric is the Mahalanobis distance:We discuss some way to learn the matrix  below. For high dimensional or structured inputs, it is better to first learn an embedding  and then to compute distances in this embedding space.When  is a DNN, this is called deep learning metric.","1621-linear-and-convex-methods#16.2.1 Linear and convex methods":"In this section, we discuss approaches to learn the Mahalanobis matrix distance , either directly as a convex problem, or indirectly via a linear projection.16.2.1.1 Large margin nearest neighbors (LMNN)Large margin nearest neighbors learns  so that the resulting distance metric works well when used by a nearest neighbor classifier.For each example point , let  be the set of target neighbors, usually chosen as the set of  points sharing the same label and that are closest in Euclidean distance.We optimize  so that the distance between each point  and its target points is minimized:We also ensure that examples with incorrect labels are far away.To do so, we ensure that  is closer (by a margin ) to its target neighbors  than some other points  with different labels, called impostors:where  is the hinge loss function.The overall objective is:where . This is a convex function, defined over a convex set, which can be minimized using semidefinite programming.Alternatively, we can parametrize the problem using , and then minimize w.r.t using unconstrained gradient methods. This is no longer convex, but this allows to use a low-dimensional mapping .For large datasets, we need to tackle the  cost of computing .16.2.1.2 Neighborhood component analysis (NCA)NCA is another way to learn a mapping  such that .This defines a probability that sample  has  as it nearest neighbor, using the linear softmax function:This is a supervised version of the stochastic neighborhood embeddings.The expected number of correctly classified examples for a 1NN classifier using distance  is given by:Let  be the leave-one-out error.We can minimize  w.r.t  using gradient methods.16.2.1.3 Latent coincidence analysis (LCA)LCA is another way to learn  such that  by defining a conditional latent variable model for mapping a pair of inputs  to a label , which specifies if the inputs are similar (have the same class label) or dissimilar.Each input  is mapped to a low dimensional latent point  using:We then define the probability that the two inputs are similar with:We can maximize the log marginal likelihood using the EM algorithm:In the E step, we compute the posterior , which can be done in close form.In the M step, we solve a weighted least square problem.EM will monotonically increase the objective, and does not need step size adjustment, unlike gradient methods used in NCA.It is also possible to fit this model using variational Bayes, as well as various sparse and nonlinear extensions.","1622-deep-metric-learning-dml#16.2.2 Deep metric learning (DML)":"When measuring the distances of high-dimensional inputs, it is very useful to first learn an embedding to a lower dimensional ‚Äúsemantic‚Äù space, where the distances are more meaningful, and less subject to the curse of dimensionality.Let  be an embedding of the input, preserving its relevant semantic aspect.The -normalized version, , ensures that all points lie on the hyper-sphere.We can then measure the distance between two points using the normalized Euclidean distance (where smaller values mean more similar):or the cosine similarity (where larger values mean more similar):Both quantities are related by:The overall approach is called deep metric learning.The basic idea is to learn embedding function such that similar examples are closer than dissimilar examples.For example, if we have a labeled dataset, we can create a set of similar examples , and enforce  be more similar than .Note that this method also work in non supervised settings, providing we have other way to define similar pairs.Before discussing DML, it‚Äôs worth mentioning that some recent approaches made invalid claims due to improper experimental comparisons, a common flaw in contemporary ML research. We will therefore focus on (slightly) older and simpler methods, that tend to be more robust.","1623-classification-losses#16.2.3 Classification losses":"Suppose we have labeled data with  classes. We can fit a classification model in   time, and then reuse the hidden features as an embedding function ‚Äîit is common to use the second-to-last layer since it generalizes better to new classes than the last layer.This approach is simple and scalable, but it only learns to embed examples on the correct side of the decision boundary. This doesn‚Äôt necessarily result in similar examples being placed closed together and dissimilar examples placed far apart.In addition, this method can only be used with labeled training data.","1624-ranking-losses#16.2.4 Ranking losses":"In this section, we minimize ranking loss, to ensure that similar examples are closer than dissimilar examples. Most of these methods don‚Äôt require labeled data.16.2.4.1 Pairwise (contrastive) loss and Siamese networksOne of the earliest approach to representation learning from similar/dissimilar pairs was based on minimizing the contrastive loss:where  is a margin parameter.Intuitively, we want positive pairs to be close, and negative pairs to be further apart than some safety margin.We minimize this loss over all pairs of data. Naively, this takes  time.Note that we use the same feature extractor  for both inputs  and  to compute the distance, hence the name Siamese network:16.2.4.2 Triplet lossOne drawback of pairwise losses is that the optimization of positive pairs is independent of the negative pairs, which can make their magnitudes incomparable.The triplet loss suggest for each example  (known as anchor) to find a similar example  and a dissimilar example , so that the loss is:Intuitively, we want positive example to be close to the anchor, and negative example to be further apart from the anchor by a safety margin .This loss can be computed using a triplet network:Naively minimizing this loss take  time. In practice, we can use a minibatch where the anchor point is the first entry, and there is at least one positive and one negative example.However, this can still be slow.16.2.4.3 N-pairs lossThe drawback of triplet loss is that each anchor is only compared to one negative example, therefore the learning signal is not very strong.A solution is to create a multi-classification problem where we create a set of  negatives and one positive for every anchor. This is called the N-pairs loss:Where .This is the same as the InfoNCE loss used in the CPC paper.When , this reduces to the logistic loss:Compare this to the triplet loss when :","1625-speeding-up-ranking-loss-optimization#16.2.5 Speeding up ranking loss optimization":"The major issue of ranking loss is the  or  cost of computing the loss function, due to the need to compare pairs or triplet of examples.We now review speedup tricks.16.2.5.1 Mining techniquesA key insight is that most negative examples will result in zero loss, so we don‚Äôt need to take them all into account.Instead, we can focus on negative examples that are closer to the anchor than positive examples. These examples are called hard negative.If  is an anchor and  its nearest positive example,  is a hard negative if:When the anchor doesn‚Äôt have hard negative, we can include semi-hard negatives for which:This is the technique used by Google FaceNET model, which learns an embedding function for faces, so it can cluster similar looking faces together, to which the user can attach a name.In practice, the hard negative are chosen from the minibatch, which requires a large batch for diversity.16.2.5.2 Proxy methodsEven with hard negative mining, triplet loss is expensive.Instead, it has been suggested to define a set of  proxies representing each class and compute the distances between each anchor and proxies, instead of using all examples.These proxies need to be updated online as the distance metric evolve during training. The overall procedure takes  time, where .More recently, it has been proposed to take multiple prototypes for each class, while still achieving linear time complexity, using a soft triple loss.16.2.5.3 Optimizing an upper boundTo optimize the triplet loss, it has been proposed to define a fixed proxy or centroid per class, and then use the distance to the proxy as an upper bound on the triplet loss.Consider a triplet loss without the margin term:using the triangle inequality, we have:Therefore:We can use this to derive a tractable upper bound on the triplet loss:where It is clear that  can be computed in  time.It has been shown thatwhere  is some constant which depends on the number of centroids.To ensure the bound is tight, the inter-cluster distances should be large and similar.This can be enforced by defining the  vectors to be one-hot, one per class. These vectors are orthogonal between each other and have unit norm, so that the distance between each other is .The downside of this approach is that it assumes the embedding layer is  dimensional. Two solutions:\nAfter training, add a linear projection layer mapping  to , or use the second-to-last layer of the embedding network\nSample a large number of points on the -dimensional unit sphere (by sampling from the normal distribution, then normalizing) and then running K-means clustering with .\nInterestingly, this paper has been shown that increasing  results in higher downstream performance on various retrieval task, where:is the average intra-class distance andis the average inter-class distance, whereis the mean embedding for examples of class .","1626-other-training-tricks-for-dml#16.2.6 Other training tricks for DML":"We present other important details for good DML performance.i) One important factor is how the minibatch is created.In classification tasks (at least with balanced classes), selecting examples at random from the training set is usually sufficient.For DML, we need to ensure that each example has some other examples in the minibatch that are similar and dissimilar to it.\nOne approach is to use hard-mining like we previously saw.\nOne other is coreset methods applied to previously learned embeddings to select a diverse minibatch at each step.\nThe above cited-paper also shows that picking  classes and sampling  samples per class is a simple sampling method that works well for creating our batches.\nii) Another issue is avoiding overfitting. Since most datasets used in the DML literature are small, it is standard to use image classifiers like GoogLeNet or ResNet pre-trained on ImageNet, and then to fine-tune the model using DML loss.In addition, it is common to use data augmentation techniques (with self-supervised learning, it is the only way of creating similar pairs)iii) It has also been proposed to add a spherical embedding constraint (SEC), which is a batchwise regularization term, which encourages all the examples to have the same norm.The regularizer is the empirical variance of the norms of the unnormalized embeddings in that batch.This regularizer can be added to any DML loss to modestly improve the training speed and stability, as well as final performances, analogously to how batchnorm is used."}},"/proba-ml/generalized-linear-models/examples":{"title":"12.2 Examples","data":{"1221-linear-regression#12.2.1 Linear regression":"The linear regression has the form:where , and we see that Hence:","1222-binomial-regression#12.2.2 Binomial regression":"If the response variable is the number of success in  trials,  we can use the binomial regression:We see that binary logistic regression is the special case when .The log pdf is:where , and We rewrite this in GLM form:with Hence:","1223-poisson-regression#12.2.3 Poisson regression":"If the response variable is an integer count, , we can use the Poisson regression:where:The Poisson distribution is highly used in bio-stats application, where  might represent the number of diseases at a given place.The log pdf is:where , and Hence:"}},"/proba-ml/examplar-based-models/k-nearest-neighbor-classification":{"title":"16.1 K Nearest Neighbor (KNN) Classification","data":{"":"KNN is one of the simplest kind of classifier: to classify a new input , we find the  closest examples in the training dataset, denoted , and look at their labels, to derive a distribution over the outputs for a local region around .We can return this distribution or the majority label.The two main parameters are the size of the neighborhood  and the distance metric . It is common to use the Mahalanobis distance:where  is a positive definite matrix. When , this reduces to the Euclidean distance.Despite its simplicity, when  the KNN model can becomes within a factor of 2 of the Bayes error (measuring the performance of the best possible classifier).","1611-example#16.1.1 Example":"Below is an example in the case of , where the test point is marked as an ‚Äúx‚Äù.We can predict .If we use  we can return the label of the nearest point, and the classifier induces a Voronoi tessellation of the points. This is a partition of space which associates a region  to each  such that all point in  are closer to  that any other points in .Since the predictive label of each region is the label of , the training error is 0. However, such a model is overfitting the training set.The example below applies a KNN over 3 classes, and we see the decision boundaries becoming smoother as  increases. This results in the training error increasing, and the start of underfitting.The test error shows the characteristic U-shaped curve.","1612-the-curse-of-dimensionality#16.1.2 The curse of dimensionality":"The main statistical issue with KNN classifiers is that they don‚Äôt work well with high-dimensional inputs due to the curse of dimensionality.The basic problem is that the volume of space grows exponentially fast with dimension, so you might have to look far away in space to find your nearest neighbor.Suppose we apply KNN classifier to data uniformly distributed within a unit cube of dimension . We estimate the class labels density around a point  by growing a hypercube around  until it contains a fraction  of the data points.The expected length edge of this cube is:Therefore, if  and we want  of the data points, we need to extend the cube  along each dimension around .Since the range of the data is 0 to 1, we see this method is not very ‚Äúlocal‚Äù, so the neighbors might not be good predictors about the behavior of .There are two main solutions: make some assumption about the form of the function (i.e. use a parametric model) or use a metric that only care about a subset of the dimensions.","1613-reducing-the-speed-and-memory-requirements#16.1.3 Reducing the speed and memory requirements":"KNN classifiers store of the training data, which is very wasteful of space. Various pruning techniques have been proposed to remove points that don‚Äôt affect decision boundaries.In terms of running time, the challenge is to find the  nearest neighbors in less than  time, where  is the size of the training set.Finding exact nearest neighbors is intractable when  so most methods focus on finding good approximates.There are two main classes of techniques:\nPartitioning methods use some kind of k-d tree which divide space into axis-parallel region, or some kind of clustering methods which use anchor points\nHashing methods, like local sensitive hashing (LSH) is widely used, although more recent methods learn the hashing method from the data\nFAISS, an open source library for efficient exact or approximate nearest neighbors search (and K-means clustering) of dense vector is available at https://github.com/facebookresearch/faiss","1614-open-set-recognition#16.1.4 Open set recognition":"In all of the classification tasks so far we assumed the number of classes  was fixed (this is called closed world assumption).However, many real-world problem involve test samples that come from new categories. This is called open set recognition.16.1.4.1 Online learning, OOD recognition and open set recognitionSuppose we train a face recognition system to predict the identity of a person from a fixed set of images.Let  be the labeled dataset at time , with  the set of images and  the set of people known to the system.At test time, the system may encounter a new person, let  the new image and  the new label. The system need to recognize this is a new category, and not accidentally classify it with a label from .This is called novelty detection, where in this case the image is being generated from  with . This can be challenging if the new image appears close to an existing image in .If the system successfully detects that  is novel, then it may ask for the identity of this new person, and add both image and label to the dataset . This is called incremental or online learning.If the system encounters a photo sampled from an entirely different distribution (e.g. a dog) this is called out of distribution (OOD) detection.In this online setting, we often get a few (sometimes just one) example of each class. Prediction in this setting is called few-shot classification.KNN classifiers are well suited to this task, by storing all instances of each classes. At time , rather than labeling  using some parametric model, we just find the nearest example in the training set, called . We then need to determine if  and  are similar enough to constitute a match (this is called face verification in our example).If there is no match, we can declare the image to be novel or OOD.The key ingredient for all the above problems is the similarity metric between inputs.16.1.4.2 Other open world problemsThe open set recognition is an example of problem requiring the open world assumption.Entity resolution or entity linking is another example of this range of problem, where we have to determine whether different strings refer to the same entity or not (e.g. ‚ÄúJohn Smith‚Äù and ‚ÄúJon Smith).Multi-object tracking is another important application, also belonging to random finite sets problems."}},"/proba-ml/generalized-linear-models/intro":{"title":"12.1 Introduction","data":{"":"We previously discussed:\nthe logistic regression model \nthe linear regression model .\nFor both of the models, the mean of the output  is a linear function of the input .Both models belong to the broader family of generalized linear models (GLM).A GLM is a conditional version of an exponential family distribution, in which the natural parameters are a linear function of the input:where:\n is the (input dependent) natural parameter\n is the log normalizer\n is the sufficient statistic\n is the dispersion term\nWe denote the mapping from the linear inputs to the mean of the output using , known as the mean function, where  is the link function."}},"/proba-ml/generalized-linear-models/maximum-likelihood-estimation":{"title":"12.4 Maximum likelihood estimation","data":{"":"GLMs can be fit similarly to logistic regression. In particular, the NLL is:where:and . We assume .We can compute the gradient as follow:where  and  is the inverse link function mapping the canonical parameters to the mean parameters.In the case of logistic regression, we have .This gradient expression can be used in SGD or other gradient methods.The Hessian is given by:where:hence:For example, in the case of logistic regression, In general, we see that the Hessian is positive definite since , hence the NLL is convex, so the MLE for the GLM is unique (assuming  for all )."}},"/proba-ml/graph-embeddings/applications":{"title":"23.6 Applications","data":{"":"We give some examples of the applications of supervised and unsupervised graph embeddings.","2361-unsupervised-applications#23.6.1 Unsupervised applications":"","23611-graph-reconstruction#23.6.1.1 Graph reconstruction":"Graph reconstruction is a popular unsupervised application, where the goal is to learn a mapping (parametric or not) from nodes onto a manifold which can reconstruct the graph.This is regarded as unsupervised because there is no supervision beyond the graph structure. Models can be trained to minimize the reconstruction error, which is obtained by reconstructing the graph via learned embeddings.At a high-level, graph reconstruction is similar to dimension reduction like PCA, but instead of compressing high dimensional vectors into low dimensional ones, it compresses data defined on graph into low dimensional vectors.","23612-link-prediction#23.6.1.2 Link prediction":"The goal of link prediction is to infer missing or unobserved links (e.g. links that may appear in the future for dynamic and temporal network). It also can identify spurious links and remove them.It is a major application of graph learning models, used for predicting\nfriendship in social networks\nuser-product interactions in recommender systems\nsuspicious links in a fraud detection system\npredicting missing relationships in a knowledge graph\nA common approach to train link prediction models is to mask some edges (positive and negative ones) and use the rest of the graph to predict them.Note that link prediction is different from graph reconstruction, because the former aims at predicting links that are not observed in the original graph, while the latter learns embeddings that preserve the original graph structure through reconstruction error minimization.We put link predictions in the unsupervised category. Even if we can consider edges as labels, they are not used during training, but only used to evaluate the predictive quality of the embeddings.","23613-clustering#23.6.1.3 Clustering":"Clustering is useful to discover communities, with applications in social networks (groups with similar interests) or biological networks (groups of protein with similar properties).Unsupervised methods uses clustering algorithms like K-means on the embeddings that are output by an encoder.Further, clustering can be joined with the learning algorithm while learning a shallow or graph convolution embedding model.","23614-visualization#23.6.1.4 Visualization":"There are many off-the-shelf tools for mapping graph nodes onto 2d manifold for the purpose of visualization. Visualization allow to qualitatively understand graph properties, understand relationships between nodes or visualize clusters.Unsupervised graph embedding methods can be used: by first training an encoder-decoder model, and then mapping node embedding onto a 2d space using t-SNE or PCA. If nodes have attributes, they can be used to color the nodes on 2d visualization plots.Finally, beyond mapping every node to a 2d coordinate, methods mapping every graph to an embedding can similarly be projected in 2d to visualize and analyse graph-level properties.","2362-supervised-applications#23.6.2 Supervised applications":"","23621-node-classification#23.6.2.1 Node classification":"The goal of node classification is to learn representation that can accurately predict node labels, like topics in citation networks or gender in social networks.Since labelling large graph is often expensive, semi-supervised techniques are commonly used, where only a fraction of nodes are labeled, and the goal is to leverage links between nodes to predict attributes of unlabeled nodes.This setting is transductive since there is only one partially labeled fixed graph. It is also possible to do inductive node classification, which correspond to the task of classifying nodes in multiple graphs.Node features can significantly boost performances on node classification tasks if there are descriptive for the target label. Indeed, methods like GCN or GraphSAGE have achieved SoTA performances on multiple node classification benchmarks due to their ability to combine structural information and semantics coming from features.Other methods like random walks on graph fail to leverage feature information and therefore achieve lower performance on these tasks.","23622-graph-classification#23.6.2.2 Graph classification":"Graph classification aims at predicting graph labels. They are inductive, and a common example is classifying chemical compounds (e.g. predicting toxicity or odor from a molecule).Graph classification requires some notion of pooling, in order to aggregate node-level information into graph-level information.As discussed earlier, generalizing this notion of pooling to arbitrary graphs is non trivial because of the lack of regularity in the graph structure, making graph pooling an active research area.Some unsupervised methods for learning graph-level representation have also been proposed."}},"/proba-ml/graph-embeddings/encoder-decoder-problem":{"title":"23.2 Graph Embedding as an Encoder / Decoder Problem","data":{"":"Many approaches to GRL follow a similar pattern:\nThe network input (node features  and graph edges ) is encoded from the discrete domain of the graph to embeddings \nThe learned representation is used to optimize an objective (e.g., reconstructing the edges of the graph)\nIn this section, we will use the graph encoder-decoder model (GraphEDM) to analyze popular families of GRL methods, supervised and unsupervised. Some utilizes the graph as a regularizer, positional embeddings, or graph convolutions.GraphEDM takes as input a weighted graph  and optionally .In (semi-)supervised settings, we assume that we have training target labels for nodes , edges  or the entire graph . We denote the supervision signal .We can decompose GraphEDM into multiple components:GraphEncoder networkThis might capture different graph properties depending on the supervision task.GraphDecoder networkThis compute similarity scores for all nodes pairs in matrix .Classification networkwhere  and  is the label space.This network is used in (semi-)supervised settings output a distribution over the labels.Specific choices of the encoder and decoder networks allow GraphEDM to perform specific graph embedding methods, as we will explain.The output of GraphEDM is either a reconstructed graph similarity matrix  (often used to train unsupervised embedding algorithms) and/or labels  for supervised applications.The label output is task dependent, and can be node-level like , with  representing the node label space.Alternatively, for edge-level labeling,  , with  representing the edge-level label space.Finally, we note other kinds of labeling are possible, like graph labeling , with  representing the graph label space.A loss must be specified to optimize . GraphEDM models can be optimized using a combination of three different terms:where  are hyper-parameters than can be tuned or set to zero, and:\n is a supervised loss term\n is a graph reconstruction loss term, leveraging the graph structure to impose regularization constraints on the model parameters\n is a weight regularization loss, allowing to represent priors on trainable model parameters to limit overfitting."}},"/proba-ml/graph-embeddings/deep-graph-embeddings":{"title":"23.5 Deep Graph Embeddings","data":{"":"We now focus on graph in unsupervised and semi-supervised settings.","2351-unsupervised-graph-embeddings#23.5.1 Unsupervised graph embeddings":"","23511-structural-deep-network-embedding-sdne#23.5.1.1 Structural deep network embedding (SDNE)":"SDNE method uses auto-encoders preserving first and second order node proximity.It takes a row of the adjacency matrix  as input and produces node embeddings . Note that this ignores node features .The SDNE decoder returns , a reconstruction trained to recover the original graph adjacency matrix.SDNE preserves second order proximity by minimizing the loss:The first term is similar to matrix factorization techniques, except  is not computed using outer product. The second term is used by distance based shallow embedding methods.","23512-variational-graph-auto-encoders-gae#23.5.1.2 (Variational) graph auto-encoders (GAE)":"GAE use graph convolutions to learn node embeddings .The decoder is an outer product The graph reconstruction term is the sigmoid cross entropy between the true adjacency and the predicted edges similarity scores:To avoid computing the regularization term over all possible nodes pairs, GAE uses negative sampling.Whereas GAE is a deterministic model, the author also introduced variational graph auto-encoders (VGAE).The embedding  is modeled as a latent variable with a standard multivariate prior  and a graph convolution is used as the amortized inference network .The model is training by minimizing the corresponding negative ELBO:","23513-iterative-generative-modeling-of-graphs-graphite#23.5.1.3 Iterative generative modeling of graphs (Graphite)":"The graphite model extends GAE and VGAE by introducing a more complex decoder, which iterate between pairwise decoding functions and graph convolutions:where  is the input of the encoder.This process allows Graphite to learn more expressive decoders. Finally, similar to GAE, graphite can be deterministic or variational.","23514-methods-based-on-contrastive-loss#23.5.1.4 Methods based on contrastive loss":"The deep graph infomax (DGI) method is a GAN-like method for creating graph-level embeddings.Given one or more real (positive) graphs, each with its adjacency matrix  and nodes features , this method creates fake (negative) adjacency matrices  and their features .DGI trains:i) an encoder that processes both real and fake samples, giving:ii) A (readout) graph pooling function iii) A discriminator function  which is trained to outputfor nodes corresponding to given graph  and fake graph .The loss is:where  contains  and the parameter of  and .In the first expectation, DGI samples from the real graphs. If only one graph is given, it could sample some subgraphs from it (connected components).The second expectation sample fake graphs. In DGI, fake graphs use the real adjacency matrix  but fakes features  are row-wise random permutation of the real .The encoder used in DGI is a GCN, though any GNN can be used.The readout  summarizes an entire (variable size) graph to a single (fixed-dimension) vector. DGI uses a row-wise mean, though other graph pooling might be used, e.g. ones aware of the adjacency.The optimization of the loss maximize a lower-bound on the mutual information between the outputs of the encoder and the graph pooling function, i.e. between individual node representation and the graph representation.Graphical Mutual Information (GMI) is a variant that maximizes the MI between the representation of a node and its neighbors, rather than the maximizing the MO of node information and the entire graph.","2352-semi-supervised-graph-embeddings#23.5.2 Semi-supervised graph embeddings":"We now discuss semi-supervised losses for GNNs. We consider the simple special case in which we use a nonlinear encoder of the node features , but ignores the graph structure :","23521-semiemb#23.5.2.1 SemiEmb":"Semi-supervised embeddings (SemiEmb) use an MLP for the encoder of . For the decoder, we can use a distance-based graph decoder:where we can use the L1 or L2 norm.SemiEmb regularizes intermediate layers in the network using the same regularizer as the label propagation loss:","23522-planetoid#23.5.2.2 Planetoid":"Unsupervised skip-gram methods like DeepWalk and node2vec learn embeddings in a multi-step pipeline, where random walks are first generated from the graph and then used to learn embeddings.These embeddings are likely not optimal for downstream classification tasks. The Planetoid method extends such random walk methods to leverage node label information during the embedding.Planetoid first maps nodes to embeddings using a neural net (again ignoring graph structure):The node embeddings  capture structural information while the node embeddings  capture feature information.There are two variant of encoding for :\nA transductive version that directly learn  as an embedding lookup\nAn inductive model where  is computed with parametric mappings that act on input features \nThe planetoid objective contains both a supervised loss and a graph regularization loss. This latter term measures the ability to predict context using nodes embeddings:with  and , with  if  is a positive pair and  if  is a negative pair.The distribution under the expectation is defined through a sampling process.The supervised loss in Planetoid is the NLL of predicting the correct labels:where  is a node‚Äôs index, and   are computed using a neural net followed by a softmax activation, mapping  to predicted labels."}},"/proba-ml/graph-embeddings/graph-neural-nets":{"title":"23.4 Graph Neural Networks","data":{"2341-message-passing-gnns#23.4.1 Message passing GNNs":"The original graph neural network (GNN) was the first application of deep learning to graph-structured data. It views the supervised node embedding problem as an information diffusion mechanism, where nodes send information to their neighbors, until some state of equilibrium is reached.Given randomly initialized nodes embedding , It applies the following recursion:where parameters  are reused at each iteration.After convergence at , the embeddings  are used to predict the final output such as node or graph labels:This process is repeated several time to learn  and  are learned via backpropagation using the Almeda-Pineda algorithm.By Banach‚Äôs fixed point theorem, this is guaranteed to converge to a unique solution when the recursion provides a contraction mapping. We can express maps by using message passing networks.where  is a MLP constrained to be a contraction mapping. The decoder, however, has no specific constraints and can be any MLP.Gated Graph Sequence Neural Network (GGSNN) removes the contraction mapping requirement for GNNs. It relaxes the  update equation by applying mapping functions for a fixed number of steps, where each mapping function is a gated recurrent unit (GRU) with shared parameters for each iteration.This model outputs predictions at every steps, making it useful for sequential structures like temporal graphs.Unlike GNNs that run for an indefinite number of iterations, Message Passing Neural Network (MPNN) have a fixed number of layers.At every layer , message functions  receive messages (based on hidden states) from neighbors, which are then passed to an aggregation functions :where .After   layers of message passing, nodes‚Äô hidden representations encode information within hop neighborhoods.GraphNet extends the MPNN framework to learn representations of the edges and graph as well using message passing function.","2342-spectral-graph-convolutions#23.4.2 Spectral Graph Convolutions":"Spectral methods define graph convolution using the spectral domain of the graph Laplacian matrix.We distinguish two categories: spectrum-based methods (e.g. Spectral CNNs) and spectrum-free methods, which are motivated by spectral theory but do not actually perform spectral decomposition (e.g., Graph convolutional network (GCN)).A major disadvantage of spectrum-based methods is that they rely on the spectrum of the graph Laplacian and can‚Äôt generalize to new graphs. Also, computing the Laplacian‚Äôs spectral decomposition is expensive.Spectrum-free methods overcome these limitations by using approximation of these spectral filters, but they require using the whole graph , which doesn‚Äôt scale well.","2343-spatial-graph-convolutions#23.4.3 Spatial Graph Convolutions":"Spatial graph convolutions takes inspiration from image convolutions, applying rectangular patches around pixels. The core idea is to use neighborhood sampling and attention mechanisms to create fixed-size graph patches.","23431-sampling-based-spatial-methods#23.4.3.1 Sampling-based spatial methods":"GraphSAGE has been proposed to overcome the dependency and storage limitations of GCNs and learn inductive node embeddings.Instead of averaging signals from one-hop neighbors (via multiplications with the Laplacian matrix), SAGE samples neighborhoods (by fixing a sampling size ) for each node.This removes the strong dependency on fixed graph structure and allows generalization to new graphs. At each layer, nodes aggregate information from nodes sampled from their neighborhood.The propagation rule can be written:where the aggregation function  can be any permutation invariant function such as mean-averaging (SAGE-mean) or max-pooling (SAGE-pool).Using fix sampling neighborhood size (and not the full adjacency matrix ) also help reduce the computational complexity of training GCNs.","23432-attention-based-spatial-methods#23.4.3.2 Attention-based spatial methods":"Attention methods have been successfully used for language models, where they allow to identify relevant part of long sequence inputs.Graph-based attention model learns to focus on important neighbor during the message passing step, via parametric patches learned on top of node features. This provides more flexibility for inductive settings, compared to methods relying only on mixed weights like GCNs.Graph Attention Network (GAT) is an attention-based version of GCN, attending over the all the neighborhood of each node (rather than a fixed-size sample like SAGE uses) and defining the soft-weights:where  are node embeddings,  is a share weight matrix, the attention mechanism is a single feedforward network parametrized by , and  is some neighborhoods of .We use the attention coefficients to compute a linear combination of the features corresponding to them, and use multi-head attention to stabilize the learning process of self-attention:","2344-non-euclidean-graph-convolutions#23.4.4 Non-Euclidean Graph Convolutions":"Hyperbolic geometry enable us to learn shallow embeddings of hierarchical graphs, which have smaller distortion than Euclidean embeddings.Shallow embeddings don‚Äôt generalize well on new graphs, so Graph Neural Networks has been extended to non-Euclidean embeddings.One challenge in doing so revolve around the nature of convolution itself. How should we perform convolutions in a non-Euclidean space, when inner products and matrix multiplication are not defined?Hyperbolic Graph Convolution Network (HGCN) applys graph convolutions in hyperbolic space by leveraging the Euclidean tangent space, which gives first order approximation of the hyperbolic manifold at a point.For every graph convolution step, the embedding are mapping to the Euclidean space to perform convolution, then are mapped back to the hyperbolic space, yielding significant improvements for hierarchical graphs."}},"/proba-ml/graph-embeddings/shallow-graph-embeddings":{"title":"23.3 Shallow graph embeddings","data":{"":"Shallow embedding methods are transductive graph embedding methods, where the encoder maps categorical nodes IDs onto a Euclidean space through an embedding matrix.Each node  has a corresponding embedding  and the shallow encoder function is:The embedding dictionary  is directly learned as model parameters.In the unsupervised case, the embeddings are optimized to recover information about the input graph (e.g. the adjacency matrix ). This is similar to dimension reduction methods like PCA, but for graphs.In the supervised case, the embedding are optimized to predict some labels, for nodes, edges and/or the whole graph.","2331-unsupervised-embeddings#23.3.1 Unsupervised embeddings":"In the unsupervised case, we consider two main types of shallow embedding methods: distance-based and outer product-based.Distance-based methods optimize embeddings such that nodes  and  which are close on the graph (as measured by some graph distance function) are embedded in  such that the pairwise distance function  is small. can be customized, which lead to Euclidean or non-Euclidean embeddings. The decoder outputs:with .Pairwise dot-products compute node similarities. The decoder network can be written as:In both cases, embeddings are learned by minimizing the graph regularization loss:where  is an optional transformation and  is a pairwise distance function between matrix, which don‚Äôt need to be the same form as .","2332-distance-based-euclidean-methods#23.3.2 Distance-based: Euclidean methods":"Distance-based methods minimize Euclidean distance between similar (connected) nodes.Multi-dimensional scaling (MDS) is equivalent to setting   to some distance matrix measuring the dissimilarity between nodes (e.g. proportional to pairwise shortest distance) and then defining:where Laplacian eigenmaps learn embeddings by solving the generalized eigenvector problem:where  is the graph Laplacian, and  is the diagonal matrix of the row-wise sum of .The first constraint removes an arbitrary scaling factor in the embedding and the second remove trivial solutions corresponding to the constant eigenvector (with eigenvalue zero for connected graphs).Further, note that:where  is the ‚Äôth row of . Therefore the minimization of the objective can be written as a graph reconstruction term:where","2333-distance-based-non-euclidean-methods#23.3.3 Distance-based: non-Euclidean methods":"So far, we have assumed method creating embeddings in the Euclidean space.However, hyperbolic geometry is ideal for embedding trees and graph exhibiting a hierarchical structure.Embedding of hierarchical graphs can be learn using the Pointcar√© model of hyperbolic space. We only need to change :The optimization then learns embedding that minimize the distance between connected node, and maximize the distance between disconnected nodes:where the denominator is approximated by negative sampling.Note that since the hyperbolic space has a manifold structure, we need to make sure that the embedding stay remain on the manifold (by using Riemannian optimization techniques).It has been shown that variant using Lorentz model of hyperbolic space provides better numerical stability.","2335-outer-product-based-skip-gram-methods#23.3.5 Outer product-based: Skip-gram methods":"Skip-gram word embeddings are optimized to predict context words given a center word. Given a sequence of words , skip-gram will optimize:for each target word .This idea has been leveraged for graph embedding in the DeepWalk framework, since the author have proven that the frequency statistics induced by random walk in the graph is similar to that of words in natural language.Deepwalk train node embeddings to maximize the probability of context nodes for each center node. The context are the nodes reached during a single random walk.Each random walk starts with a nodes  and repeatedly samples the next node uniformly at random: . The walk length is a hyperparameter.All generated random-walks (sentences) can then be encoded by a sequence model. This has been implemented by node2vec.It is common to underlying representation to use two distinct representation for each node: one for when the node is a center and one for when it is in the context.To present DeepWalk on the GraphEDM framework, we can set:Training DeeWalk is equivalent to minimizing:where   and the left term can be approximated in  time by hierarchical softmax.Skip-gram methods can be viewed as implicit matrix factorization, which can inherit benefits of efficient sparse matrix operatioons.","2336-supervised-embeddings#23.3.6 Supervised embeddings":"In many applications, we have labeled data in addition to node features and graph structure.While we can tackle supervised problem by first learning unsupervised embeddings and apply them to a supervised task, this is not the recommended workflow. Unsupervised node embeddings might not preserve important graph properties (e.g., node neighborhoods) that are most useful for a downstream task.A number of methods combine these two steps, like label propagation (LP), a very popular algorithm for semi-supervised node classification. The encoder is a shallow model represented by a lookup table .LP use the label space to represent the node embedding directly (the decoder is the identity function):Laplacian eigenmaps are used in the regularization to enforce a smoothness on labels, which represents the assumption that neighbor nodes should have the same labels:LP minimizes this loss on the space of functions that take fixed values on label nodes (i.e.  using an iterative algorithm that updates unlabeled node‚Äôs label distribution via the weighted average of its neighbors‚Äô labels.Label spreading (LS) is a variant of label propagation which minimizes the following function:where  is the degree of node .In both methods, the supervised loss is the distance between the predicted labels and the ground truth (one-hot vectors)These methods are expected to work well with consistent graphs, that is graph where node proximity is positively correlated with label similarity."}},"/proba-ml/home":{"title":"Probabilistic Machine Learning: An Introduction, Kevin Murphy","data":{"":"Created: June 6, 2022 8:52 AMPDF, Exercices solutions\nIn this book, we will cover the most common types of ML, but from a probabilistic perspective. Roughly speaking, this means that we treat all unknown quantities (tomorrow‚Äôs temperature, or the parameters of some model) as random variables, that are endowed with probability distributions that describe a weighted set of possible values the variable may have.\nThere are two main reasons we adopt a probabilistic approach. First, it is the optimal approach to decision-making under uncertainty. Second, probabilistic modeling is the language used by most other areas of science and engineering and thus provides a unifying framework between these fields."}},"/proba-ml/kernel-methods/intro":{"title":"17. Kernel Methods","data":{"":"In this chapter, we consider nonparametric methods for classification and regression. These methods don‚Äôt assume fixed parametric form for the prediction function, but try to estimate the function itself (rather than its parameters) directly from the data.They key idea is to memorize a training dataset  and compare a new input  to each of the training points . We can then predict that  is a weighted combinations of the  values.In section 17.1, we explain that the similarity between  and each  is computed using a kernel function, . This approach is similar to RBF network, except we use the datapoints  themselves as anchors rather than learning centroids .In section 17.2, we discuss an approach called Gaussian processes, in which the kernel define a prior over functions, which we can update given data to get a posterior over functions.Alternatively, we can use the kernel with a Support Vector Machines to compute the MAP estimate of the function, as explained in section 17.3."}},"/proba-ml/kernel-methods/gaussian-processes":{"title":"17.2 Gaussian processes","data":{"":"Gaussian processes is a way to define distributions over functions of the form , where  is any domain.The key assumption is that the function values at a set of  inputs, , is jointly Gaussian, with mean  and covariance , where  is a mean function and  is a Mercer kernel.This assumption holds when , containing  training points  and  test point .Thus, we can infer  from knowledge of  by manipulating the joint Gaussian distribution .We can also extend this to work with noisy observations, such as in regression or classification problems.","1721-noise-free-observations#17.2.1 Noise-free observations":"Suppose we observe a training set , where  is the noise-free function at .We want the GP to act as an interpolator of the training data, i.e. returning the answer  with no uncertainty when it has already seen .Let‚Äôs now consider a test set  of size  (not in ). We want to predict the function outputs .By definition of the GP, the joint distribution  has the form:where , , and  is .By the standard rule for conditioning Gaussians, the posterior distribution is:We see the uncertainty increase as we move further away from the training points.","1722-noisy-observations#17.2.2 Noisy observations":"Let‚Äôs now consider the case where we observe a noisy version of the underlying function:where .In this case, the model is not required to interpolate the training data, but it must come ‚Äúclose‚Äù.The covariance of the noisy response is:In other words:The density of the observed data and the latent, noise-free function on the test points is:Hence the posterior predictive density at a set of points  is:In the case of a single test point , this simplifies as follow:where  and .If the mean function is zero, we can write the posterior mean as:This is identical to the predictions from kernel ridge regression.","1723-comparison-to-kernel-regression#17.2.3 Comparison to kernel regression":"In section 16.3.5 we discussed kernel regression, which is a generative approach to regression in which we approximate  using a density kernel estimation.This is very similar to our previous posterior mean , we few important differences:\nIn a GP, we use Mercer kernel instead of density kernel. Mercer kernels can be defined on objects such as string or graphs, which is harder to do for density kernels.\nGP is an interpolator (when ) , so . By contrast, kernel regression is not an interpolator..\nGP is a Bayesian model, meaning we can estimate hyperparameters of the kernel by maximizing the marginal likelihood. By contrast, in kernel regression we must use cross-validation to estimate the kernel parameters, such as bandwidth.\nComputing the weights in kernel regression takes  times, whereas computing the weigths  for GP regression takes  time (although there are approximations that can reduce it to \nAlso note that Bayesian linear regression is a special case of a GP. The former works in weight space whereas the latter works in function space.","1725-numerical-issues#17.2.5 Numerical issues":"For notational simplicity, we assume the prior mean is zero, .The posterior mean is . For reason of computation stability, it is unwise to compute directly .Instead we can use a Cholesky decomposition  which takes  time. We then compute , where we have used the backlash operator to represent back-substitution.Given this, we can compute the posterior mean for each test case in  time using:We can also compute the variance using:where Finally, the log marginal likelihood can be computed as:","1726-estimating-the-kernel#17.2.6 Estimating the kernel":"Most models have some free parameters that can play a great role on the predictions.Let assume we perform a 1d regression using GP with a RBF kernel: is the horizontal scale over which the function changes,  control the vertical scale. We assume observation noise with variance .We sampled 20 points from an MVN with covariance given by  for a grid of points , and added observation noise of value We then fit these points using a GP with the same kernel, but with a range of hyperparameters.When changing  from 1 to 3, the function goes from a good fit to very smooth.17.2.6.1 Empirical BayesTo estimate the kernel parameters  (aka hyperparameters), we could use exhaustive grid search with the validation loss as an objective, but this can be slow (this is the approach used by nonprobabilistic methods such as SVMs).We consider here the empirical Bayes approach, which use gradient-based approach, which are faster. We maximize the marginal likelihood (and not likelihood, because we marginalized out the latent Gaussian vector ):Assuming the mean is 0, we have:So the marginal likelihood is:with the dependence of   on  implicit.The first term is a data fit term, the second is a model complexity term (the third is just a constant).If the length scale  is small, the fit will be rather good so  will be small. However,  will be almost diagonal since most points will not be considered ‚Äúnear‚Äù any others, so  will be large.We now maximize the marginal likelihood:where It takes  time to compute , and then  time per hyperparameter to compute the gradient.The form of  depends on the form of the kernel, and which parameters we are taking derivatives with respect to. We often have constraints on the HP such as . In this case, we can define  and then use the chain rule.Given an expression for the log marginal likelihood and its derivative, we can estimate the kernel parameters with any gradient-based optimizer. However, since the objective is not convex, local minima can be a problem so we need multiple restart.Notice that when the noise is high (around ), the marginal likelihood is insensitive to the length scale. Similarly, when the length scale is very short , the kernel interpolates exactly the data.17.2.6.2 Bayesian inferenceWhen we have a small number of data points (e.g. using GP for Bayesian optimization) using a point estimate of the kernel parameters can give poor results.Instead, we can approximate the posterior distribution over the parameters, using slice sampling, Hamiltonian Monte Carlo or sequential Monte Carlo.","1727-gps-for-classification#17.2.7 GPs for classification":"So far, we have used GPs for regression, which uses Gaussian likelihood. In this case, the posterior is also a GP and all the computation can be performed analytically.However, if the likelihood is non Gaussian, such as the Bernoulli likelihood for binary classification, we can no longer compute the posterior exactly.There are various approximation we can make, and we use there the Hamiltonian Monte Carlo method, both for the latent Gaussian  and the kernel parameters .We specify the negative log joint:We then use autograd to compute  and , and use these gradients as input to a Gaussian proposal distribution.We consider a 1d example of binary classification:The SE kernel curves probability toward 0.5 on the edges, because the prior mean function has  and .We can eliminate this artifact by using a more flexible kernel that encodes the knowledge that the probability is monotonically increasing or decreasing. We can use a linear kernel, scale it and add it to the SE kernel:","1729-scaling-dps-to-large-datasets#17.2.9 Scaling DPs to large datasets":"The main issue with GPs (and kernel methods such as SVM) is that it requires inverting the  kernel matrix, which takes  times, making the method too slow for big datasets.We review some speed-up techniques.17.2.9.1 Sparse (inducing-point) approximationsA simple approach to speeding up GPs is to use less data. A better approach is to summarize the  training points into  inducing points or pseudo inputs .This lets us replace  with  where  is the vector of observed function values at training points.By estimating  we can learn to compress the training data and speed up the computation to . This is called a sparse GP. This whole process can be performed using the framework of variational inference.17.2.9.2 Exploiting parallelization and kernel matrix structureIt takes  to compute the Cholesky decomposition of , which is needed to solve the linear system  and compute .An alternative is to use linear algebra methods, often called Krylov subspace methods, which are based on matrix vector multiplication (MVM). These approaches are usually faster because they exploit structure in .Moreover, even if the Kernel matrix doesn‚Äôt have a special structure, matrix multiplication can easily be parallelized over GPUs, unlike Cholesky based methods which are sequential.This is the basis of the GPyTorch package.17.2.9.3 Random feature approximationWe can approximate the feature map for many shift invariant kernels using a randomly chosen finite set of  basis functions, thus reducing the cost to .i) Random features for RBF kernelFor Gaussian RBF kernel, one can show that:where the features vector are:where  and  is a random Gaussian matrix, where the entries are sampled iid from  where  is the kernel bandwidth.The bias of the approximation decreases as  increases. In practice, we compute a single sample Monte Carlo approximation to the expectation by drawing a single random matrix.The features in the equation above are called random Fourier features (RFF).We can also use positive random features instead of trigonometric random features, which can be preferable for some applications, like models using attention. We can use:Regardless of the choice of the features, we can obtain a lower variance estimate by ensuring the rows of  are random but orthogonal. These are called orthogonal random features.Such sampling can be conducted efficiently via Gram-Schmidt orthogonalization of the unstructured Gaussian matrices.ii) Fastfood approximationStoring the random matrix  takes  space and computing  takes  time, where  is the input dimensionality and  is the number of random features.This can become prohibitive when , which it may needs to get any benefits over using the original set of features.Fortunately, we can use the fast Hadamard transform to reduce the memory to  and reduce the time to . This approach has been called fastfood as reference to the original term ‚Äúkitchen sinks‚Äù.iii) Extreme learning machinesWe can use the random features approximation to the kernel to convert a GP into a linear model of the form:where  for RBF kernels. This is equivalent to a MLP with fixed and random input-to-hidden weights.When , this corresponds to an over-parametrized model which can perfectly interpolate the training data.Alternatively, we can use , but stack many nonlinear random layers together and just optimize the output weights. This has been called an extreme learning machine (ELM)."}},"/proba-ml/kernel-methods/sparse-vector-machines":{"title":"17.4 Sparse vector machines","data":{"":"GPs are flexible models but incur a  time for prediction, which can be prohibitive. SVMs solve that problem by estimating a sparse weight vector. SVMs, however, don‚Äôt give calibrated probabilistic outputs.We can get the best of both world by using parametric models, where the feature vector is defined by using basis functions centered on each of the training point:where  is any similarity kernel, not necessarily a Mercer kernel.We can plug this feature vector in any discriminative model, such as logistic regression.Since we have   features, we need some regularization to prevent overfitting. If we fit such model with  regularization, we get L2VM. The results will have good performances but the weight vector  will be dense and depends on  training points.A natural solution is to impose a sparsity-promoting prior on , so that we drop some examples. We call such methods sparse vector machine.","1741-relevance-vector-machine-rvm#17.4.1 Relevance vector machine (RVM)":"The simplest way to ensure  is sparse is to use   regularization. We call this L1VM or Laplace VM since this approach is equivalent to using MAP estimation with Laplace prior for .However, sometimes  regularization doesn‚Äôt yield sufficient sparsity for a given level of accuracy. An alternative approach is automatic relevancy determination (ARD) which uses type II MLE (aka empirical Bayes) to estimate a sparse weight vector.If we apply this technique to a feature vector defined in terms of kernel , we get a method called relevance VM (RVM).","1742-comparison-of-sparse-and-dense-kernel-methods#17.4.2 Comparison of sparse and dense kernel methods":"Below, we compare L2VM, L1VM, RVM and SVM using RBF kernel for 2d binary classification.We cross-validate the SVM to pick  and use the same value to regularize the other models.We see that all models give similar predictive performances, but the RVM is the sparsest so it will be the fastest at run time.We then compare L2VM, L1VM, RVM and SVM using an RBF kernel on 1d regression problem. We see again that RVM is the sparsest, then L1VM, then SVM.We can also observe the sparse coefficients:We provide a more general summary of the different methods:"}},"/proba-ml/graph-embeddings/intro":{"title":"23.1 Introduction","data":{"":"We now study the semantic relationships between training samples . The relationships (aka edges) connect samples (aka nodes) with a similarity.Graphs are the natural data structure for reasoning about this relationships, commonly applied in computational chemistry, social networks, semi-supervised learning, recommender systems and others.Let  be the adjacent matrix, where  is the number of nodes, and let  be its weighted version. Some methods set , while others set a transformation of , like a row-wise normalization.Finally, let  be the matrix of node features.When designing and training a neural network model over graph data, we desire the model to be applicable to node with different connections and neighborhood structures.This is in contrast with neural network designed for images, where each pixel has the same settings.Operations, like Euclidean spatial convolution, can‚Äôt be applied on irregular graphs since this operation relies on geometric priors like shift invariance.These challenges led to the development of Geometric Deep Learning (GDL) which aim at applying deep learning to non-Euclidean data. Among these techniques, Graph Representation Learning (GRL) methods aim at learning low-dimensional Euclidean representation (embeddings) for node and edges.We divide GRL in two classes of problems: unsupervised and supervised GRL.Unsupervised GRL learns embeddings by optimizing a metric which preserves the graph structure of the data.Supervised GRL also learns embeddings but for a specific downstream task, like graph or node classification.Further, the graph structure can be fixed during training and testing, called transductive learning setting, aiming at predicting properties (e.g. in a large social network), or alternatively, the model is expected to answer questions about graphs unseen during training, called inductive learning setting (e.g. classifying molecular structures).Finally, there is also interest in learning non-Euclidean representation learning, which aim at learning non-Euclidean embedding spaces like hyperbolic or spherical spaces.The main motivations is to use a continuous embedding space that is similar to the underlying discrete structure of the input data it tries to embed (e.g. the hyperbolic space is a continuous version of trees)."}},"/proba-ml/kernel-methods/support-vector-machines":{"title":"17.3 Support vector machines (SVM)","data":{"":"We now discuss a form of (non-probabilistic) predictors for classification and regression problems which have the form:By adding some constraints, we can ensure only few coefficient  are non-zero, so that predictions only depends on a subset of the training points, called support vectors.The resulting model is called support vector machine (SVM).","1731-large-margin-classifiers#17.3.1 Large margin classifiers":"Consider a binary classifier of the form , where the decision boundary is given by:In the SVM literature, it is common to use class labels of  rather than . We denote such targets as  instead of .There might be many lines that separate the data, and intuitively we want to pick the one that has the maximum margin. This represents the distance between the closest point to the decision boundary.The model on the left gives the best solution, because more robust to perturbation of the data.How can we compute such a large margin classifier? First, we need to derive an expression of the distance of a point to the decision boundary.We see that:where  is the distance from  to the decision boundary whose normal vector is , and  is the orthogonal projection of  on the decision boundary.We would like to maximize , so we need to express it as a function of :Since , therefore Since we want to ensure that each point is on the correct point of the boundary, we also require .We want to maximize the distance to the closest point, so our final objective is:Note that rescaling the parameters using  don‚Äôt change the distance to the decision boundary, since we divide by .Therefore let‚Äôs define the scale factor such as  that closest point to the decision boundary. Hence  for all .Thus, we get the new objective:Note that it is important to scale the input variables before using an SVM, otherwise the margin measures distance of a point to the boundary using all input dimensions equally.","1732-the-dual-problem#17.3.2 The dual problem":"The last objective is a standard quadratic programming problem since we have a quadratic objective subject to linear constraints. This has   variables for  constraints, and is known as a primal problem.In convex optimization, for every primal problem we can derive a dual problem. Let  be the dual variables, corresponding to Lagrangian multipliers that enforce  inequality constraints.The generalized Lagrangian is:To optimize this, we must find stationary point that satisfies:We do it by computing the partial derivatives wrt  and  and setting them to zero:Hence:Plugging these into the Lagrangian yields:This is called the dual form of the objective. We want to maximize this wrt  subject to the constraints that  and The above objective is a quadratic problem with  variables. Standard QP problems solvers take  time.However, specialized algorithms, such as sequential minimal optimization (SMO), have been developed to avoid using generic QP solvers. These take  to  time.Since this is a convex objective, it must satisfied the KKT conditions, which yields the following properties:Hence either  or the constraint  is active. This latter condition means that example  lies on the decision boundary, so it is a support vector.We denote the set of support vectors by . To perform prediction, we use:To solve for , we can use the fact that for any support vector, we have .Multiplying each side by  and exploiting the fact that , we get We take to mean to obtain a more robust estimate:","1733-soft-margin-classifiers#17.3.3 Soft margin classifiers":"If the data is not linearly separable, there will be no feasible solution in which  for all .We therefore introduce slack variables  and replace the hard constraints with soft margin constraints .The objective becomes:where  is a hyperparameter controlling how many points we allow to violate the margin constraint. If , we recover the previous unregularized hard margin classifier.The corresponding Lagrangian is:where   and  are the Lagrange multipliers.Optimizing out ,  and  gives the dual form:This is identical to the hard margin case, however the constraints are different. The KKT conditions imply that:\nIf , the point is ignored.\nIf  then  so the point lies on the margin.\nIf , then the point can lie inside the margin, and can either be correctly classified if , or misclassified if .\nHence,  is an upper bound on the number of misclassified points.As before, the bias term can be computed using:where  is the set of points having .There is an alternative formulation called -SVM classifier. This involves maximizing:subject to the constraints:This has the advantage that the parameter , which replace the parameter , can be interpreted as an upper bound on the fraction of margin errors (point for which ), as well as a lower bound on the number of support vectors.","1734-the-kernel-trick#17.3.4 The kernel trick":"We have converted the large margin classification problem into a dual problem in   unknowns  which takes  time to solve, which is slow.The benefit of the dual problem is that we can replace all inner product operation  with a call to a positive definite Mercer kernel function, . This is called kernel trick.We can rewrite the prediction equation as:We also need to kernelize the bias term:The kernel trick allows us to avoid dealing with explicit feature representation, and allows us to easily apply classifiers to structured objects, such as strings and graphs.","1735-converting-svm-outputs-into-probabilities#17.3.5 Converting SVM outputs into probabilities":"An SVM classifier produce hard labeling . However, we often want a measure of confidence in our prediction.One heuristic approach is to interpret  as the log-odds ratio . We can then convert an SVM output to probability:where ,  can be estimated using MLE on a separate validation set. This is known as Platt scaling.However, the resulting probabilities are not well calibrated, since there is nothing that justifies interpreting  as the log-odds ratio.To see this, suppose we have 1d data where:\nSince the class-conditional distributions overlap in this interval, the log-odd should be 0 inside it and infinite outside.We train a probabilistic kernel classifier (RVM) and a SVM with Gaussian kernel. Both model can perfectly capture the decision boundary and achieve a generalization error of 25%, which is Bayes optimal in this problem.However, the SVM doesn‚Äôt yield a good approximation of the true log-odds.","1736-connection-with-logistic-regression#17.3.6 Connection with logistic regression":"Data points that are on the (correct side) decision boundary have , the others have . Therefore we can rewrite the soft-margin objective in its primal form as:where  and the hinge loss is defined as:This is a convex, piecewise differentiable upper-bound to the 0-1 loss.By contrast, the (penalized) logistic regression optimizes:where the log loss is given by:The two major differences between both loss is that:\nThe hinge loss is piecewise linear, so we can‚Äôt use regular gradient methods to optimize it.\nThe hinge loss has a region where it is strictly 0. This results in sparse estimates.","1737-multi-class-classification-with-svms#17.3.7 Multi-class classification with SVMs":"SVMs are inherently binary classifiers.One way to convert them to a multi-class classification model is to train  binary classifiers, where the data from class  is treated as positive, and the rest is negative.We then predict the label by using the rule:where . This is known as the one-vs-rest approach (aka one-vs-all).Unfortunately, this has shortcomings:\nThis can result in regions of input space where the predicted label is ambiguous\nThe magnitude of the  scores are not calibrated well with each other, so there are hard to compare.\nEach binary subproblem is likely to suffer from imbalance problem, which can hurt performances: for 10 equally represented classes, each positive class represents only 10% of labels.\nWe can also use the one-vs-one (OVO) approach, in which we train  classifiers to discriminate all pairs . We then classify a point into a class that has the higher number of votes.However this can also result in ambiguities, and this require fitting   models.","1738-how-to-choose-the-regularizer-c#17.3.8 How to choose the regularizer ":"SVMs require to specify the kernel function and the parameter , usually chosen with cross-validation.Note however that  interact quite strongly with kernel parameters, for example with precision .\nIf  is large, corresponding to narrow kernel, we may need heavy regularization, hence small \nIf   is small, a larger value of  should be used.\nThe authors of libsvm recommend using CV over a 2d grid with values  and To choose  efficiently, we can develop a path following algorithm similar to lars.The idea is to start with a small  so that the margin is wide and all points are inside it. By increasing , points will move from inside the margin to outside, and their  value change from 1 to 0 as they cease to be support vectors. When  is maximal, there are no support vectors left.","1739-kernel-ridge-regression#17.3.9 Kernel ridge regression":"The ridge regression equation is:Using the matrix inversion lemma, we can rewrite the ridge estimate as follows:Let‚Äôs now define the dual variables:We can rewrite the primal variables as:So, the solution vector is just a weighted sum of the N training vectors.We plug this at test time to get the predictive mean:where we used the kernel trick, with the dual parameter becoming:In other words:where .This is called ridge kernel regression. The issue with this approach is that the solution vector  is not sparse, so predictions at test time will take  time.","17310-svms-for-regression#17.3.10 SVMs for regression":"Consider the following -regularized ERM problem:where .If we use the quadratic loss, we recover ridge regression. If we then apply the kernel trick, we recover kernel ridge regression.By changing the loss function, we can make the optimal set of basis coefficient  sparse. In particular, consider the variant of the Huber loss function, the epsilon insensitive loss function:This means any point lying in the -tube is not penalized.The corresponding loss function can be written:where  is a regularization constraint.The objective is convex without constraint but not differentiable, because of the absolute value in the loss term. As for the lasso, there is several algorithm we could use.One popular approach is to formulate the problem as a constraint optimization one. We introduce slack variables to represent the degree to which each point lies outside the tube:We can rewrite the objective as:This is a quadratic function of  and must be minimized subject to the above constraint, as well as positive constraint on  and .This is a standard quadratic problem in  variables. By forming the Lagrangian and optimizing as we did above, one can show that the optimal solution is:where  are the dual variables.This time the  vector is sparse, because the loss doesn‚Äôt care about errors smaller than  The degree of sparsity is controlled by  and The  for which   are the sparse vectors, lying on or outside the   tube. They are the only examples we need to keep since:The overall technique is called SVM regression."}},"/proba-ml/learning-with-fewer-labeled-examples/active-learning":{"title":"19.4 Active learning","data":{"":"In active learning, the goal is to identify the true predictive mapping  by querying as few  points as possible. There are three variants.In query synthesis, the algorithm gets to choose any input  and can ask for its corresponding output .In pool-based active learning, there is large, but fixed, set of unlabeled points and the algorithm gets to ask a label for one or more of these pointsFinally, in stream-based active learning the incoming data is arriving continuously, and the algorithm must choose whether it wants to request a label for the current input or not.They are various closely related problems. In Bayesian optimization the goal is to estimate the location of the global optimum  in as few queries as possible. Typically, we fit a surrogate (response surface) model to the intermediate  queries, to decide which question to ask next.In experiment design, the goal is to estimate , using as little data as possible (this can thought of as an unsupervised form of active learning).In this section, we discuss the pool based approach.","1941-decision-theoretic-approach#19.4.1 Decision-theoretic approach":"We define the utility of querying  in terms of the value of information. We define the utility of issuing query  as:where  is the posterior expected loss of taking some future action  given the data  observed so far.Unfortunately, evaluating  for each  is quite expensive, since for each possible response  we might observe, we have to update our beliefs given  to see what effect it might have on our future decisions (similar to look ahead search technique applied to belief states).","1942-information-theoretic-approach#19.4.2 Information-theoretic approach":"In the information-theoretic approach, we avoid using a task specific loss function and focus on learning our model as well as we can.It has been proposed to define the utility of querying  in terms of information gain about the parameter , ie the reduction in entropy:We used the symmetry of the mutual information to get (3), and the advantage of this approach is that we now only have to reason about the uncertainty of the predictive distribution over  and not over .Note that this objective is identical to the expected change in the posterior over the parameter:Eq (3) has an interesting interpretation: the first term prefers example  for which there is uncertainty in the label. Just using it as a criterion selection is called maximum entropy sampling.However, this can have problems with ambiguous or mislabeled examples. The second term in the equation will discourage this behavior, since it prefers examples  that are fairly certain once we know In other words, this equation will select examples  for which the model makes confident predictions which are highly diverse. This approach is called Bayesian active learning by disagreement (BALD)This methods can be used to train classifiers where expert labels are hard to acquire, such as medical images.","1943-batch-active-learning#19.4.3 Batch active learning":"So far, we have assumed a greedy strategy, in which we select a single example . Sometimes, we have a budget to select a set of  samples.In this case, the information criterion becomes:Unfortunately, optimizing this is NP-hard in the horizon length .Fortunately, the greedy strategy is near-optimal in certain conditions.First note that, for any given , the information gain function is: maps a set of labels to a scalar.It is clear that , and that  is non-decreasing, i.e. , due to the ‚Äúmore information never hurts‚Äù principle.It has also been shown that  is submodular.As a consequence, a sequential greedy approach is within a constant factor of optimal. If we combine this greedy technique with BALD objective, we get a method called BatchBALD."}},"/proba-ml/learning-with-fewer-labeled-examples/data-augmentation":{"title":"19.1 Data augmentation","data":{"":"Suppose we have a small training set. In some cases, we may be able to create artificially modified versions of the input vectors, which capture the kinds of variation we expect to see at test time, while keeping the original labels.This is called data augmentation.","1911-examples#19.1.1 Examples":"For image classification tasks, standard data augmentation methods include random crops, zooms and mirror image flips.\nFor signal processing, adding background noise\nFor text, artificially replace characters or words at random in a text document.\nIf we afford to train and test the model many times using different version of the data, we can learn which augmentation work best, using blackbox optimization techniques like RL or Bayesian optimization.We can also learn to combine multiple augmentation together, this is called AutoAugment.","1912-theoretical-justification#19.1.2 Theoretical justification":"Data augmentation generally increases performances (predictive accuracy, robustness), because it algorithmically inject prior knowledge.To see this, recall the ERM training, where we minimize the risk:where we approximate  by the empirical distribution:We can think of data augmentation as replacing the empirical distribution with the following algorithmically smoothed distribution:where  is the data augmentation algorithm, which generate a sample  from a training point , such that the labeled are unchanged.A very simple example would be a Gaussian kernel:This has been called vicinal risk minimization, since we are minimizing the risk in the vicinity of each training point ."}},"/proba-ml/learning-with-fewer-labeled-examples/few-shots-learning":{"title":"19.6 Few-shot learning","data":{"":"People can learn to predict from very few labeled example. This is called few-shot learning (FSL).In the extreme where we only have one labeled example of each class, this is one-shot learning, and if no label are given, this is called zero-shot learning.A common way to evaluate methods for FSL is to use -way, -shot classification, in which we expect the system to learn to classify  classes from  labeled examples for each class.Typically,  and  are very small.Since the amount of labeled data per domain is so scarce, we can‚Äôt expect to learn from scratch. Therefore, we turn to meta-learning.During training, a meta-algorithm  trains on a labeled support set from group  and returns a predictor , which is then evaluated on a disjoint query set also from group .We optimize  over the  groups. Finally, we can apply  to our new labeled support set to get , which is then applied to the test query set.We see on the image above that there is no overlap of classes between groups, therefore the model has to learn to predict classes in general rather than any specific set of labels.We discuss some approach to FSL below.","1961-matching-networks#19.6.1 Matching networks":"On approach to FSL is to learn a distance metric on some other dataset, and then use  inside a nearest-neighbor classifier.Essentially, this defines a semi-parametric model of the form  where  is the small labeled dataset (known as the support set) and  are the parameters of the distance function.This approach is widely used for fine-grained classification tasks, where there are many different but visually similar categories, such as products in a catalogue.An extension of this approach is to learn a function of the form:where  is some kind of adaptive similarity kernel.For example, we can use an attention kernel of the form:where  is the cosine distance.  and  can be the same function.Intuitively, the attention kernel compares  to  in the context of all the labeled example, which provides an implicit signal about which feature dimension are relevant. This is called matching network.We can train  and  using multiple small datasets, as in meta-learning.Let  be a large labeled dataset (e.g. ImageNet), and let  be its distributions of labels. We create a task by sampling a small number of labels (say 25) , and then sample a small support set of examples from  with those labels  and finally sampling a small test set with those same labels .We then train the model to optimize the objective:After training we freeze   and predict on a test support set ."}},"/proba-ml/learning-with-fewer-labeled-examples/intro":{"title":"19. Learning with Fewer Labeled Examples","data":{"":"Many ML models often have many more parameters than we have labeled training examples. A ResNet CNN with 50 layers will have 25 millions parameters, and transformer models can be even bigger!These models are slow to train and may easily overfit. This is particularly a problem when you don‚Äôt have a large labeled training set.We discuss ways to tackle this issue, beyond the generic regularization technique like early stopping, weight decay and dropout."}},"/proba-ml/kernel-methods/mercel-kernels":{"title":"17.1 Mercer Kernels","data":{"":"A Mercer kernel or positive definite kernel is a symmetric function Given a set of  data points, we define the Gram matrix as the following   similarity matrix: is a Mercer kernel iff the Gram matrix is positive definite for any set of distinct input The most widely used kernel is the RBF kernel:Here   corresponds is the bandwidth parameter i.e. the distance over which we expect differences to matter.The RBF measures similarity between two vectors in  using scaled Euclidean distance.","1711-mercers-theorem#17.1.1 Mercer‚Äôs theorem":"Any positive definite matrix  can be represented using an eigendecomposition of the form  where  is the diagonal matrix of eigenvalues  and  is a matrix containing the eigenvectors.Now consider the element  of :where  is the th column of .If we define  we can write:The entries of the kernel matrix can be computed by performing an inner product of some feature vectors that are implicitly defined by the eigenvectors of the kernel matrix.This idea can be generalized to apply to kernel function, not just kernel matrices. For example, consider the quadratic kernel . In 2d, we have:We can write this:where .So, we embedded the 2d input  into a 3d feature space .The feature representation of the RBF kernel is infinite dimensional. However, by working with kernel functions, we can avoid having to deal with infinite dimensional vectors.","1712-popular-mercer-kernels#17.1.2 Popular Mercer kernels":"17.1.2.1 Stationary kernels for real-valued vectorsFor real-valued inputs, , it is common to use stationary kernels, of the form:The value of the output only depends on the distance between the inputs. The RBF kernel is a stationary kernel. We give some examples below.ARD KernelWe can generalize the RBF kernel by replacing the Euclidean distance with the Mahalanobis distance:where .If   is diagonal, this can be written:where:We can interpret  as the total variance and  as defining the characteristic length scale of dimension . If  is an irrelevant input dimension, we can set , so the corresponding dimension will be ignore.This is known as automatic relevancy determination (ARD), hence this kernel is called ARD kernel.Matern kernelsThe SE kernel yields functions that are infinitely differentiable, and therefore very smooth. For many applications, it is better to use the Matern kernel which yields rougher functions.This allow to model small ‚Äúwiggles‚Äù without having to make the overall length scale very small.The Matern kernel has the following form:where  is a modified Bessel function and   the length scale.Functions sample from this GP are -times differentiable, where . As , this approaches the SE kernel.For value , the function simplifies as follow:This corresponds to the Ornstein-Uhlenbeck process which describe the velocity of a particle undergoing Brownian motion. The corresponding function is continuous but very jagged.Periodic kernelsThe periodic kernel capture repeating structure, and has the form:where  is the period.This is related to the cosine kernel:17.1.2.2 Making new kernels from oldGiven a valid kernel , we can create new kernel using any of the following methods:, for any function , for any function polynomial  with nonneg coef, for any psd matrix For example, suppose we start with the linear kernel .We know this is a valid Mercer kernel since the Gram matrix is the (scaled) covariance matrix of the data.From the above rules we see that the polynomial kernel  is also a valid kernel, and contains all monomial of order We can also infer that the Gaussian kernel is a valid kernel. We can see this by noting:Hence:We scale by a constant, then use the exponential, then symmetrically multiply by functions of  and .17.1.2.3 Combining kernels by addition and multiplicationWe can also combine kernels using addition or multiplication:Adding (resp. multiplying) positive-definite kernels together always result in another positive definite kernel. This is a way to get a disjunction (resp. conjunction) of the individual properties of each kernel.17.1.2.4 Kernels for structured inputsKernels are particularly useful for structured inputs, since it‚Äôs often hard to ‚Äúfeaturize‚Äù variable-size inputs.We can define a string kernel by comparing strings in terms of the number of ngrams they have in common.We can also define kernel on graph. For example random walk kernel perform random walk simultaneously on two graphs and count the number of paths in common."}},"/proba-ml/learning-with-fewer-labeled-examples/weakly-supervised-learning":{"title":"19.7 Weakly supervised learning","data":{"":"In weakly supervised learning we don‚Äôt have an exact label for each example in the training set.i) One scenario is having a distribution over labels for each example, rather than a single one. Fortunately, we can still do ML training, by using the cross entropy:where  is the label distribution for example  and  is the predicted distribution.It is often use to replace labels with a ‚Äúsoft‚Äù version, putting e.g. 90% of mass on a label and spreads the remaining 10% uniformly across the other choices, instead of using a delta function. This is called label smoothing and is a useful form of regularization.ii) Another scenario is having a set of example  and having a single label for the entire set , not for individual members .We often assume that if one example in the set is positive, then all the set is labeled positive , without knowing which example ‚Äúcaused‚Äù the positive outcome.However, if all members are negative, the set is negative. This is known as multi-instance learning (MIL) (this technique has recently been used for Covid19 risk-score learning).Various algorithms can be used to solve the MIL problem, depending on the correlation of labels across the sets or the fraction of positive.iii) Yet another scenario is known as distant supervision, in which we use a ground truth label like Married(A, B) to label every sentence in an unlabeled training corpus in which the entity A and B are mentioned as being a positive example of the ‚ÄúMarried‚Äù relation.For example ‚ÄúA and B invited 100 people to their wedding‚Äù will be labeled positive. But this heuristic might include false positives for example ‚ÄúA and B went out to dinner‚Äù, thus the labels will be noisy.We discuss some way of handling it in section 10.4 with robust logistic regression."}},"/proba-ml/learning-with-fewer-labeled-examples/meta-learning":{"title":"19.5 Meta-learning","data":{"":"We can think of a learning algorithm as a function  that maps data to a parameter estimate .The function  usually has its own parameter , such as the initial values for  or the learning rate. We get .We can imagine learning  itself, given a collection of dataset  and some meta-learning algorithm , i.e. .We can then apply  on a new dataset  to learn the parameters . This is also called learning to learn.","1951-model-agnostic-meta-learning-maml#19.5.1 Model agnostic Meta-learning (MAML)":"A natural approach to meta-learning is to use a hierarchical Bayesian model.We can assume that the parameters  come from a common prior , which can be used to help pool statistical strength from multiple data-poor problems.Meta-learning becomes equivalent to learning the prior . Rather than performing full Bayesian inference, we use the following empirical Bayes approximation:where  is a point estimate of the parameters of task , and we use cross-validation approximation to the marginal likelihood.To compute the point estimate of the parameters for the target task , we use  steps of a gradient ascent procedure, starting from  with a learning rate . This is known as model agnostic meta-learning (MAML).This can be shown to be equivalent to an approximate MAP estimate using a Gaussian prior centered at , where the strength of the prior is controlled by the number of gradient steps (this is an example of fast adaptation of the task specific weights from the shared prior )."}},"/proba-ml/learning-with-fewer-labeled-examples/semi-supervised-learning":{"title":"19.3 Semi-supervised learning","data":{"":"Large labeled dataset for supervised learning is often expensive. In automatic speech recognition, modern datasets contain thousand of hours of recording, and annotating the words spoken is slower than realtime.To make matters worst, in some applications, data must be labeled by an expert (such as in medical applications) which can further increase cost.Semi-supervised learning can alleviate the need of labeled data by leveraging unlabeled data. Its general goal is to learn the high level structure of the unlabeled data distribution, and only rely  on labeled data to learn the fine-grained details of a given task.In addition to samples from the joint distribution of data , semi-supervised learning also assumes to have access to the marginal distribution of , namely .Further, we also assume to have many more unlabeled data than labeled data, since, in the example of ASR, recording people talking is cheap.Semi supervised learning is therefore a good fit in the scenario where a large amount of unlabeled data has been collected and we prefer to avoid labeling all of it.","1931-self-training-and-pseudo-labeling#19.3.1 Self-training and pseudo-labeling":"The basic idea of self-training is to infer predictions on unlabeled data, and then use these predictions as labels for subsequent training.Self-training has endured as an approach to semi-supervised learning because of its simplicity and general applicability, and it has recently been called pseudo-labeling, by opposition to the ground truth labels used in supervised learning.Algorithmically, self-training follows of the following two procedures.In the first offline approach:\nPseudo labels are predicted for the entire unlabeled dataset\nThe model is retrained (possibly from scratch) on the combination of the labeled and pseudo-labeled datasets.\nRepeat to 1.\nIn the second online approach:\nPseudo labels are continuously predicted on randomly chosen batches of the unlabeled data\nThe model is immediately retrained on this batch of pseudo labels\nRepeat to 1.\nBoth approaches are popular in practice; the offline one has been shown to be particularly successful when leveraging giant collections of unlabeled data, whereas the online is often used as a step of more complex semi-supervised learning methods.The offline approach can result in training models on stale pseudo-labels since they are only updated each time the model converged, but the online approach can be more computationally expensive since it involves constantly ‚Äúrelabeling‚Äù unlabeled data.However, self-training can suffer from confirmation bias: if the model generates incorrect predictions for the unlabeled data and then is re-trained on them, it can become progressively worse at the intended classification task.We can mitigate confirmation bias by using a ‚Äúselection metric‚Äù to only retain correct pseudo-labels. For example, assuming that model outputs probability for each class, we can retain pseudo-labels whose largest class probability is above some threshold.If the model‚Äôs class probability estimate are well calibrated, then this selection metric rule will only retain labels are highly likely to be correct.","1932-entropy-minimization#19.3.2 Entropy minimization":"Self-training has the implicit effect of encouraging the model to output low-entropy (i.e. high confidence) predictions. This effect is most apparent in the online setting with a cross-entropy loss, where the model minimizes the following loss function on the unlabeled data:This function is minimized when the model assigns all its probability to a single class , i.e.  and \n.A closely-related semi-supervised learning method is entropy minimization, which minimizes the loss:Note that this function is also minimized when the model assigns all of its class probability to a single class.We can make the entropy-minimization loss equivalent to the online self-training loss by replacing the first  term with a ‚Äúone-hot‚Äù vector that assign a probability of 1 for the class with the highest probability.In other words, online self-training minimizes the cross-entropy loss between the model‚Äôs output and the ‚Äúhard‚Äù target , whereas entropy minimization uses the ‚Äúsoft‚Äù target .One way to trade-off between these two extremes is to adjust the ‚Äútemperature‚Äù of the target distribution by raising each probability to  and renormalizing. This is the basis of the mixmatch method.At , this is equivalent to entropy minimization, as , it becomes hard online self-training.19.3.2.1 The cluster assumptionWhy is entropy minimization a good idea? A basic assumption of many semi-supervised learning method is that the decision boundary between class should fall in a low-density region of the data manifold.This assumes that the data corresponding to different classes are clustered together. A good decision boundary, therefore, should not pass through clusters, it should separate them.Semi-supervised learning method that make the ‚Äúcluster assumption‚Äù can be thought of using unlabeled data to estimate the shape of the data manifold and moving the decision boundary away from it.Entropy minimization is one such method, since for smooth decision boundaries, entropy will be minimized when the decision boundary is place in low-density regions of the input space.19.3.2.2 Input-output mutual informationAn alternative justification for the entropy minimization objective was proposed by Bridle, Heading and MacKay, where they shown that it naturally arises from maximizing the mutual information between the data and the label:Note that the first integral is equivalent to taking an expectation over , and the second integral is equivalent to summing over all possible values of the class . We get:Since we have initially sought to maximize the mutual information, we can convert it to a loss function to minimize by negating it:The first term is exactly the entropy minimization objective in expectation.The second term specifies that we should maximize the entropy of the average class prediction over our training set. This encourages the model to predict each possible class with equal probability, which is only appropriate when we don‚Äôt have imbalanced class.","1933-co-training#19.3.3 Co-training":"Co-training is also similar to self-training, but makes an additional assumption that there are two complementary views (i.e. independent sets of features) of the data.After training two models separately on each view, unlabeled data is classified by each model to obtain candidate pseudo-labels. If a particular pseudo-label receives a low-entropy prediction (high confidence) from one model and high-entropy from the other, then that example is added to the training set for the high-entropy model.Then, the process is repeated with the new, larger training datasets.Retaining pseudo-labels when one of the models is confident ideally builds up the training sets with correctly-labeled data.Co-training makes the strong assumption that there are two informative-but-independent views of the data, which may not be true for many problems.The Tri-Training algorithm circumvents this issue by instead using three models that are first trained on independently-sampled (with replacement) subsets of the labeled data. Ideally, this leads to models that don‚Äôt always agree on their predictions.Then, pseudo-labels are predicted by the three models. If two models agree on the pseudo-label of a given example, it is added to the training set for the third model.This can be seen as a selection metric, because it only retains pseudo-labels where two (differently initialized) models agree on the correct label.The models are the re-trained on the combination of the labeled data and the new pseudo-labels, and the whole process is repeated iteratively.","1934-label-propagation-on-graphs#19.3.4 Label propagation on graphs":"If two points are similar, we can expect them to share the same label. This is known as the manifold assumption.Label propagation is a semi-supervised technique that leverages the manifold assumption to assign labels to unlabeled data.It first constructs a graph where the examples are the nodes and the edges are the similarity between points. The node labels are known for labeled example but are unknown for unlabeled example.Label propagation then propagates the known labels across edges of the graph in such a way that there is minimal disagreement in the labels of a given node‚Äôs neighbors.This provides label guesses for the unlabeled data, which can then be used in the usual way for the supervised training of a model.More specifically, the basic label propagation algorithm proceed as follows.Let  denote a non-negative edge weight between   and  that provides a measure of similarity for the two (labeled or unlabeled) examples.Assuming we have  labeled and  unlabeled examples, define the transaction matrix   as having entries: represent the probability of propagating a label from node  to node .Further, define the label matrix , where  is the number of possible classes. The th row of  represents the class probability distribution for example  (the initialization of  for unlabeled examples is not important).Then, repeat until convergence of :\nPropagate: \nRow-normalize: \nClamp the label data: replace the rows corresponding of labeled data by their one-hot representation.\nThe algorithm iteratively uses the similarity of datapoints to propagate labels onto the unlabeled data, in a weighted average way.It can be shown this procedure converges to a single fixed point, whose computation cost mainly involve inverting the matrix of unlabeled-to-unlabeled transition probabilities.The overall approach can be seen as a form of transductive learning since it is learning to predict the labels of a fixed unlabeled dataset, rather than learning a model that generalizes.However, by training a model on the induced labeling, we can perform inductive learning in the usual way.The success of label propagation depends heavily on the notion of similarity used to construct the weights between datapoints.For simple data, measuring the Euclidean distance may be sufficient, but not for more complex and high dimensional data, where it could fail to capture the likelihood that two examples belong to the same class.These weights can also be set arbitrarily according to problem-specific knowledge.","1935-consistency-regularization#19.3.5 Consistency regularization":"Consistency regularization leverages the idea that perturbing a given dataset (or the model itself) shouldn‚Äôt cause the model‚Äôs output to change dramatically.Since measuring consistency this way only uses the model‚Äôs output, it is readily applicable to  unlabeled data, and therefore can be used to create appropriate loss functions for semi-supervised learning. This idea was first proposed with the paper ‚Äúlearning with pseudo-ensembles‚Äù.In its most general form, both the model  and the transformations applied to the input can be stochastic. For example, in computer vision problems we may transform the input by using data augmentation like randomly rotating or adding noise to the image, and the network may include stochastic components like dropout or weight noise.A common and simple form of consistency regularization first samples , where  is the distribution induced by the stochastic input transformations.We minimize the combined loss function over of batch of  labeled data and  unlabeled data:In practice, the first term of the regularizer  is treated as fixed, i.e. gradients are not propagated through it.Many design choices impact the success of this semi-supervised learning approach.i) First, if the parameter  is too large, then the model way not give enough weight to learning the supervised task and will instead start to reinforce its own bad predictions (as with confirmation bias in self-training).Since the model is often poor at the start of training, it is common practice to initialize  to zero and increase its value over the course of training.ii) A second important consideration is the choice of the random transformations . These transformations should be designed so that they don‚Äôt change the label of , and are domain specific.The use of data augmentation requires expert knowledge to determine what kinds of transformations are label preserving and appropriate for a given problem.An alternative technique is virtual adversarial training (VAT), that transforms the input using analytically-found perturbation    designed to maximally change the model‚Äôs output.VAT computes:The approximation is done by sampling  from a multivariate Gaussian distribution, initializing  and then setting:where  is a small constant.VAT then sets:Then proceed to consistency regularization, where  is a scalar hyperparameter that sets the L2 norm of the perturbation applied to the input.iii) Consistency regularization can also profoundly affect the geometry properties of the training objective, and the trajectory of the SGD.For example, the Euclidean distances between weights at different training epochs are significantly larger for objectives that use consistency regularization.It has been shown that a variance of stochastic weight averaging (SWA) can achieve state-of-the-art performance on semi-supervised learning tasks by exploiting the geometric properties of consistency regularization.iv) Finally, we only have to consider the choice of the regularizer function measuring the difference between the network‚Äôs output with and without perturbation.The loss above use the squared L2 distance (aka the Brier score), and it is also common to use the LK divergence, in analogy with the cross-entropy loss (i.e. KL divergence between the ground-truth label and prediction) used for labeled examples.The gradient of the squared-error loss approaches zero as the model‚Äôs prediction with and without perturbation differ more and more, assuming the model uses a softmax nonlinearity on its logit output.Using the squared-error loss therefore has a possible advantage that the model is not updated when its predictions are very unstable.However, the KL divergence has the same scale as the cross-entropy loss used for labeled data, which makes for more intuitive tuning of .","1936-deep-generative-models#19.3.6 Deep generative models":"Generative models provide a natural way of making use of the unlabeled data through learning a model of the marginal distribution by minimizing:Various approaches have leveraged generative models for semi-supervised by developing ways to use  to help produce a better supervised model.19.3.6.1 Variational autoencodersVAE defines a probabilistic model of the joint distribution of data  and latent variables . Data is assumed to be generated by first sampling  and then sampling .For learning, the VAE uses an encoder  to approximate the posterior and decoder  to approximate the likelihood. The encoder and decoder are typically neural nets, and their parameters can be jointly training by maximizing the evidence lower bound (ELBO) of data.The marginal distribution of latent variables  is often chosen to be a simple distribution like a diagonal-covariance Gaussian.  is typically lower-dimensional than , constructed via nonlinear transformations and its dimensions are independent.Therefore, the latent variables can provide a learned representation where data may be more easily separable.In this paper, they call this approach M1 and show that the latent variables can be used to train stronger models when labels are scarce.They also propose an alternative approach to leveraging VAEs called M2, which has the form:where  is a latent variable,  is the latent prior (typically we fix  and ,  the label prior, and  is the likelihood, such as Gaussian, with parameters computed by , a neural net.The main innovation of this approach is to assume that data is generated according to both a latent class variable  as well as the continuous latent variable .i) To compute the likelihood of the labeled data, , we need to marginalize over , by using an inference network of the form:We then use the following variational lower bound:As is standard for VAEs, the main difference being that we observe both  and .ii) To compute the likelihood of the unlabeled data, , we need to marginalize over  and , with an inference network of the form:Note that  acts like a discriminative classifier, that imputes the missing labels. We then use the following variational lower bound:Note that the discriminative classifier  is only used to compute the log likelihood of the unlabeled data and is undesirable.We get the overall objective function:where   controls the relative weight of generative and discriminative learning.The probabilistic model used in M2 is just one of the many ways to decompose the dependencies between the observed data, the class labels and the continuous latent variables.There also many other ways other than variational inference to perform approximate inference.Overall the main advantage of the generative approach is that we can incorporate domain knowledge, e.g. the missing data mechanism, since the absence of a label may be informative about the underlying data.19.3.6.2 Generative adversarial networks (GANs)GANs are a popular class of generative models that learn an implicit model of the data distribution.They consist of:\na generator network which maps samples from a latent distribution to the data space\na critic network, which attempts to distinguish between the outputs of the generator and the samples from the true data distribution\nThe generator is trained to generate samples that the critic classifies as ‚Äúreal‚Äù.Since GANs don‚Äôt produce a latent representation of a given example and don‚Äôt learn an explicit model of the data distribution, we can‚Äôt used the same approaches used for VAEs.Instead, semi-supervised learning with GANs is typically done by modifying the critic so that it outputs either a class label or ‚Äúfake‚Äù, instead of classifying ‚Äúreal‚Äù vs ‚Äúfake‚Äù.For labeled data, the critic is trained to output the appropriate class label, and for unlabeled real data, it is trained to raise the probability of any class labels.Let  denote the critic with  outputs corresponding to  classes plus a ‚Äúfake‚Äù class, and let  denote the generator which takes as input samples from the prior distribution .We use a standard cross-entropy GAN loss as originally proposed in the GAN paper. The critic‚Äôs loss is:This tries to maximize the probability of the correct class for the labeled examples, to minimize the probability of the fake class for the unlabeled examples and to maximize the probability of the fake class for generated examples.The generator loss is:19.3.6.3 Normalizing flowsNormalizing flows are a tractable way to define deep generative models. They define an invertible mapping , from the data space to the latent space.The density in data space can be written starting from the density in the latent space using the change of variable formula:We can extend this to semi-supervised learning. For class labels , we specify the latent distribution conditioned on a label  as:The marginal distribution of  is then a Gaussian mixture.The likelihood for labeled data is then:And the likelihood for the unlabeled data is:For semi-supervised learning we can then maximize the joint likelihood of the labeled  and unlabeled  data:over the parameters  of the bijective function , which learns a density model for a Bayes classifier.Given a test point , the model predictive distribution is:where we have assumed .We can make predictions for a test point  with the Bayes decision rule:","1937-combining-self-supervised-and-semi-supervised-learning#19.3.7 Combining self-supervised and semi-supervised learning":"We can combine self-supervised and semi-supervised learning.For example, we can:\nUse SimCLR to perform self-supervised representation learning on the unlabeled data\nFine-tune this representation on a small labeled dataset (as in transfer learning)\nApply the trained model back to the original unlabeled dataset, and distill the predictions from this teacher model  into a student model .\nThis last approach is known as knowledge distillation, and consists in training one model on the predictions of another.That is, after fine-tuning , we can train  by minimizing:where  is a temperature parameter applied to the softmax output, which is used to perform label smoothing.If  has the same form as , this is known as self-training (as discussed section in 19.3.1).However, the student  is often a smaller than the teacher  (for example,  might be a high capacity model, and  is a lightweight version that runs on a phone)."}},"/proba-ml/linear-algebra/intro":{"title":"7.1 Introduction","data":{"711-notation#7.1.1 Notation":"VectorA vector   is usually written as a column vector:The vector of all ones is , of all zeros is .The unit or one-hot vector  is a vector of all zeros except entry  which is .MatrixA matrix  is defined similarly. If  the matrix is square.The transpose of a matrix results in flipping its row and columns: . Some of its properties:\nA symmetric square matrices satisfies . The set of all symmetric matrices of size  is denoted .TensorA tensor is a generalization of a 2d array:The rank of a tensor is its number of dimensions.We can stack rows (resp columns) of a matrix horizontally (resp vertically) to get a C-contiguous or row-major order (resp F-contiguous or columns-major order).","712-vector-spaces#7.1.2 Vector spaces":"SpansWe can view a vector  as a point in n-dimensional Euclidean space. A vector space is a collection of such vector, which can be added together and scale by scalar .A set of vector  is linearly independent if no vector can be represented as a linear combination of the others.The span of  is the set of all vectors that can be obtained its linear combinations:It can be shown that if each  form a set of linearly independent vectors, then:and therefore any vectors of  can be written as a linear combination of this set of vector.A basis  is a set of linearly independent vector that spans the whole space:There are often multiple basis to choose from, a standard one is the coordinate vectors .Linear mapsA linear map is the transformation  such that:\nOnce the basis of  is chosen, a linear map is completely determined by computing the image basis of .Suppose  we can compute  and store it in .We can then compute any  for any  by:The range of  the space reached by the matrix :The nullspace is:Linear projectionThe projection of  onto the span  with , is:And for a full rank matrix , with  , we can define the projection of  on the range of","713-vector-and-matrix-norms#7.1.3 Vector and matrix norms":"Vector normsA norms is any function  that satisfies the following properties:\nnon-negativity: \ndefiniteness: \nabsolute value homogeneity: \ntriangle inequality: \nCommon norms:\np-norm: , for \n or Manhattan: \n or Euclidean: also note the square euclidean: \nmax-norm or Chebyshev: \nzero-norm: . This is a pseudo norm since it doesn‚Äôt respect homogeneity.\nMatrix normsi) Induced normsSuppose we think of a matrix  as defining a linear function .We define the induced norm of  as the maximum amount by which  can lengthen input:In the special case of  (the Euclidean norm), the induced norm is the spectral norm:with  the th singular valueii) Element-wise normsIf we treat a  matrix as a vector of size , we can define the -norm:when , we get the Frobenius norm:If  is expensive but  is cheap, we can approximate the F-norm with the Hutchinson trace estimator:with iii) Schatten normsThe Schatten -norms arise when applying the element-wise -norm to the singular values of a matrix:When , we get the nuclear norm (or trace norm):since .Using this as a regularizer encourages many singular values to become zero, resulting in a low-rank matrix.When , we get the Frobenius norm.When , we get the Spectral norm","714-properties-of-a-matrix#7.1.4 Properties of a matrix":"TraceThe trace of a square matrix  is:We have the following:\nThis last property allows to define the trace trick:In some case,  might be expensive but   cheap, we can use the Hutchinson trace approximator:with DeterminantThe determinant is a measure of how much a square matrix change a unit volume when viewed as a linear transformation.With :\n iff  is singular\nIf  is not singular, \n, where  are the eigenvalues of \nFor a positive definite matrix,  where  is a lower triangle Cholesky decomposition:soRankThe column (resp row) rank is the dimension spanned by the columns (resp rows) of a matrix, and its a basic fact that they are the same: \nFor , If equality,  is said to be full rank (it is rank defficient otherwise).\nFor , \nFor , \nA square matrix is invertible iff full rank\nCondition numberAll norms that follow are .The condition number of a matrix measures of numerically stable any computation involving  will be:If  is close to 1, the matrix is well conditioned (ill-conditioned otherwise).A large condition means  is nearly singular. This a better measure to nearness of singularity than the size of the determinant.For exemple,  leads to  but , indicating that even though  is nearly singular, it is well conditioned and  simply scales the entry of  by 0.1.","715-special-types-of-matrices#7.1.5 Special types of matrices":"Diagonal matrices\nAll elements are 0 except the diagonal.We can convert a vector to a diagonal matrix with this notationIdentity matrix \nDiagonal of ones, with the identity property:Block diagonal matrixNon zero only on the main diagonal:Band-diagonal matrix\nNon-zero entries along k-sides of the diagonal:Triangular matrix\nUpper (resp lower) have only non-zero on the diagonal and above (resp below)Useful property: the diagonal is the eigenvalues, therefore Positive definite matricesA symmetric matrix  is positive definite iff:If equality is possible, we call  positive semi-definite.The set of all positive definite matrices is denoted A matrix can also be negative definite or indefinite: there exists  and  such that:Note that a matrix having all positive coefficient doesn‚Äôt imply being positive definite ex: Conversely, it can have negative coefficient.For a symmetric, real matrix, a sufficient condition for positive definiteness is having the diagonal coefficient ‚Äúdominate‚Äù their rows:The Gram-matrix  is always positive semidefinite.If  is full rank and , then  is positive definite.Orthogonal matrices\n are orthogonal if \n is normalized if \nA set of vectors that is pairwise orthogonal and normalized is called orthonormal\n is orthogonal if all its columns are orthonormal (note the difference of notation)If all entries of  are complex valued, we use the term unary instead of orthonormal\nIf  is orthogonal:In otherwords, One nice property is norm invariance when operating on a vector:with  and   non-zeroSimilarly, the angle is unchanged after a transformation with an orthogonal matrix:In summary, orthogonal matrices are generalization of rotation (if ) and reflections (if ) since they preserves angle and lenghts."}},"/proba-ml/learning-with-fewer-labeled-examples/transfer-learning":{"title":"19.2 Transfer learning","data":{"":"Many data-poor tasks have similarity to other data rich tasks. For example, classifying rare bird species, where we don‚Äôt have a large labeled dataset, bears similarity with classifying all species of birds.We therefore expect that training first a classifier on a large dataset of birds, then continuing training on a small dataset of rare birds could produce better performances that training on the rare birds dataset alone.This is called transfer learning, since we are transferring information from one dataset to another, via a shared set of parameters.More precisely:\nWe first perform a pre-training phase, in which we train a model with parameter  on a large source dataset , which may be labeled or unlabeled.\nWe then perform a second fine-tuning phase on the small labeled target dataset .","1921-fine-tuning#19.2.1 Fine-tuning":"Suppose now we already have a pretrained classifier  such as a CNN, that works well for inputs  (e.g. natural images) and output  (e.g. ImageNet labels), where the data comes from a distribution  similar to the one used in training.Now we want to create a new model  that works well for inputs  (e.g. bird images) and outputs  (e.g. fine-grained bird labels), where the data comes from a  different distribution  than p.We will assume that the set of possible inputs is the same, so  (e.g. both are RGB images) or that we can easily transform inputs from domain  to , (e.g. from RGB to grayscale by only keeping the luminance channel). If we can‚Äôt make this easy transformation, we can use domain adaptation.However, the output domains are usually different, i.e. . For example,  could be ImageNet labels and   could be medical labels.In this case, we need to ‚Äútranslate‚Äù the output of the pretrained model to the new domain. This is easy to do with neural networks: we simply replace the final layer of the original layer by a new one.For example, suppose:where .Then we can construct:where  and  is the shared nonlinear feature extractor.After performing this ‚Äúmodel surgery‚Äù, we can fine-tune the new model with parameters .If we treat  as ‚Äúfrozen parameters‚Äù, then the resulting model  is linear in its parameters, so we have a convex optimization problem for which many simple and efficient fitting method exist. This is particularly helpful for rare labels.However, a linear ‚Äúdecoder‚Äù may be too limiting, so we can also allow  to be fine-tuned as well, with a lower learning rate to prevent the values moving too far from the values estimated on .","1922-adapters#19.2.2 Adapters":"Fine-tuning on all parameters is usually slow, and every new tasks required a new model to be trained, making task sharing hard.An alternative approach is to keep the pre-trained model untouched, but to add new parameters  to customize the feature extraction process for each task. This idea is called Adapter.In the figure a) above, we insert two shallow bottleneck MLPs inside each transformer layer, one after the MHA and once after the feed-forward layers. Note that these MLPs have skip connections, so that they can be initialized to implement the identity mapping.If the transformer layer has features of dimensionality  and the adapter uses a bottleneck of size , this introduces  new parameter per layer. These adapters MLPs, the layer norm parameter and the final output head are trained for each new task, but all remaining parameters are frozen.On several NLP benchmarks, this is found to give better performance than fine-tuning, while only needing about 1-10% of the original parameters.In figure b) , we add a  convolution layer  to a CNN, which is analogous to the MLP adapter to the transformer case. This can be added in series or in parallel.The series adapter is:where a matrix  is reshaped with  to be applied to each spatial location in parallel.If we insert this after a regular convolution layer , we get:This can be interpreted as a low rank multiplicative perturbation of the original filter .The parallel adapter is:This can be interpreted as a low rank additive perturbation of the original filter .In both cases, setting  ensures the adapter layers can be initialized to the identity transformation. Both methods requires  parameters per layer.","1923-supervised-pre-training#19.2.3 Supervised pre-training":"The pre-training task may be supervised or unsupervised; the main requirement are that it can teach the model basic structure about the problem domain and it is sufficiently similar to the downstream fine-tuning task.The diversity and scale of dataset like ImageNet explain why it has become a de-facto pre-training task for transfer learning in computer vision.However, ImageNet pre-training has been shown to be less helpful when the domain of fine-tuning task is quite different from the natural images (e.g. medical images). In some cases where it is helpful, it seems to be more of a speedup trick (by warm-starting optimization at a good point) rather than an essential step: one can obtain similar performances on the downstream task by training from scratch long enough.Supervised pre-training is less common in non-visions applications.","1924-unsupervised-pre-training-self-supervised-learning#19.2.4 Unsupervised pre-training (self-supervised learning)":"It is increasingly common to use unsupervised pre-training, because unlabeled data are often easy to acquire (e.g. unlabeled images or text document from the web).Pre-training tasks that use unlabeled data are often called self-supervised rather than unsupervised, because the latter often refer to generative approaches, that predict outputs unconditionally.We use the term self-supervised learning (SSL) because the labels are created by the algorithm, rather than being provided by a human.Both supervised and SSL are discriminative tasks, since they require predicting outputs given inputs.We can list three main broad groups of SSL heuristics:19.2.4.1 Imputation tasksOne approach to self-supervised learning is to solve imputation tasks. We partition the input  into two parts , and then try to predict the hidden part  given the visible .This can be seen as a fill-in-the-blank task or cloze task in NLP.19.2.4.2 Proxy tasksAnother approach to SSL is to solve proxy tasks aka pretext tasks.We create pairs of inputs  and then train a Siamese networks classifier of the form:where  is some functions that performs representation learning, and  is some label that capture the relationship between  and .For instance, suppose  is an image patch and  is some transformation of  that we control, such as random rotation. If this scenario,  could be the rotation angle.19.2.4.3 Contrastive tasksThe most popular approach to use SSL is to use various kinds of contrastive tasks.The basic idea is to create pairs of examples that are semantically similar using data augmentation, and their ensure that their distance is the embedding space is closer than the distance of some two unrelated examples.This is the same idea used in deep metric learning, with the difference that the algorithm creates its own pairs, rather than relying on an external measure of similarity, such as labels.We give two examples below.19.2.4.4 SimCLRSimCLR stands for Simple contrastive learning of visual representation‚Äù and has shown SOTA performances on transfer learning and semi-supervised learning.Each input  is converted into two augmented views  and  which are semantically equivalent versions of the input generated by some data transformation  and  (for images, these could be random crop).We then sample negative example  from the dataset, that represent semantically different images (in practice, these are the other examples in the minibatch).Next, we define some feature mapping  where  is the size of the input and  is the size of the embedding.We finally try to maximize the similarity of the similar views, while minimizing the similarity of the different views:In practice, we use cosine similarity, so we -normalize the representations produced by  (omitted in the equation above).In this figure, we assumed , where the intermediate representation  is the one that will be used later for fine-tuning and  is an additionA critical ingredient to the success of SimCLR is the choice of data augmentation methods. By using random crops, we can force the model to predict local views from global views, as well as adjacent views of the same image.After cropping, images are resized back to the same shape, and some random flipping can also be applied some fraction of time. Color distortion is also needed so that the algorithm doesn‚Äôt ‚Äúcheat‚Äù by comparing images using their color histograms.SimCLR relies on a large batch training, in order to ensure a sufficiently diverse set of negatives. When this is not possible, we can use a memory bank of past negative embeddings, which can be updated using exponential moving averaging. This is known as momentum contrastive learning or MoCo.19.2.4.5 CLIPCLIP stands for ‚ÄúContrastive Language-Image Pre-training‚Äù. This is a contrastive approach to representation learning which uses a massive corpus of 400M (images, text) from the web.Let  be the th image and  its matching text. Rather than trying to predict the exact words associated with the image, it is easier to determine if  is more likely to be the correct text compared to , for some other text string  in the minibatch.Similarly, the model can try to predict if  is more likely to be matched than , for a given .Let  be the embedding of the image and  the embedding of the text. We normalize both embeddings to get the unit-norm version of the embeddings:The vector of pairwise logits (similarity score) is:We now train the parameters of the two embeddings functions  and  to minimize the following loss, averaged over the minibatches of size :where  is a one hot encoding of label  and the cross-entropy defined as:In practice, the normalized embeddings are scaled by a temperature parameter which is also learned. This controls the sharpness of the softmax.In their paper, they considered using a ResNet and a vision transformer for the function  and a text transformer for the function . They used a very large minibatch of  and trained for many days on hundreds of GPUs.After the model is trained, it can be used for zero-shot classification of an image  as follows:\nEach of the  possible class labels for a given dataset is converted into a text string  that might occur on the web. For example ‚Äúdog‚Äù becomes ‚Äúa photo of a dog‚Äù.\nWe compute the normalized embeddings:  and .\nWe compute the softmax probabilities: \nRemarkably, this approach can perform as well as standard supervised learning on tasks like ImageNet classification, without ever being explicitly trained on specific labeled dataset.Of course, the images of ImageNet come from the web, and the model has seen similar images before. Nevertheless, its generalization to new tasks and robustness to distribution shift are impressive.One drawback of this approach, however, is that it is quite sensitive to how class labels are converted to textual form, e.g. it is necessary to use strings of the form ‚Äúphoto of guacamole, type of food‚Äù for food classification, added by hand.This is called prompt engineering and is needed since the raw class names can be ambiguous across (and sometimes within) a dataset.","1925-domain-adaptation#19.2.5 Domain adaptation":"Consider a problem in which we have inputs from a different domain, such as source domain  and target domain , but a common set of output labels (this is the ‚Äúdual‚Äù of the transfer learning task where the input domains are the same but the output domains are different).For example, domains could be product reviews and movie reviews.We assume we don‚Äôt have labeled examples from the target domain. Our goal is to fit the model on the source domain, and then modify its parameters so it works on the target domain. This is called (unsupervised) domain adaptation.A common approach to this problem is to train a source classifier in such a way that it can‚Äôt distinguish whether the input is coming from the source or target distribution. In this case, it will only be able to use features that are common to both. This is called domain adversarial learning.Let  be a label that specifies the domain of an example . We want to optimize:where , ,  and .This objective minimizes the loss on the classification of  but maximizes the loss on the auxiliary task of classifying the source domain .This can be implemented by the gradient sign reversal trick and is related to GANs."}},"/proba-ml/linear-algebra/eigenvalue-decomposition":{"title":"7.4 Eigenvalue decomposition (EVD)","data":{"741-basics#7.4.1 Basics":"For a square matrix ,  is an eigenvalue of  if:with  eigenvector.Intuitively, multiplying  by  results in a vector in the same direction that , scaled by Since  is also an eigenvector of , we consider eigenvectors to be normalized with length 1.The equation above gives us:Therefore, this has a non-zero solution iff  has a non-empty nullspace, which is only the case if this matrix is singular:Properties:\nThe rank of  is equal to its number of non-zero eigenvalues\nIf  is non-singular,  is an eigenvalue of  with corresponding eigenvectors: \nThe eigenvalues of a triangular matrix are just its diagonal entries.","742-diagonalization#7.4.2 Diagonalization":"We can write all eigenvectors equations simultaneously:with  and If the eigenvectors are linearly independent, then  is invertible and:A matrix that can be written in this form is called diagonalizable.","743-eigenvalues-of-symmetric-matrices#7.4.3 Eigenvalues of symmetric matrices":"When  is real and symmetric, it can be shown that its eigenvalues are real and its eigenvectors are orthonormal:So in matrix notation: , hence  is an orthogonal matrix.We can represent  as:Thus multiplying by any symmetrical matrix  can be viewed as multiplying by a rotation matrix , a scaling matrix  and inverse rotation with .Since ,  is easily invertible:","744-geometry-of-quadratic-forms#7.4.4 Geometry of quadratic forms":"is positive definite iff : is called a quadratic form.In 2d, we have:","745-standardizing-and-whitening-data#7.4.5 Standardizing and whitening data":"We can standardize  so that each column has a mean of 0 and a variance of 1. To remove the correlations between the columns, we need to whiten the data.Let   be the empirical covariance matrix and its diagonalization.We can define the PCA whitening matrix as:Let , we can check that:The whitening matrix is not unique, since any rotation  will maintain the whitening property .For example, with with,  the SVD decomposition of .This is called the Mahalanobis whitening or ZCA (zero-phase component analysis). The advantage of ZCA over PCA is that the resulting matrix is closer to the original matrix ‚Äîin the least square sense.When applied to images, the ZCA vectors still look like images.","746-power-method#7.4.6 Power method":"This method is useful to compute the distribution of massive transition matrices (like Google PageRank).Let  be a matrix with orthonormal eigenvectors  and eigenvalues Let  be an arbitrary vector in the range of , such that .We have:So by multiplying with  and renormalizing at each iteration:So:To compute , we define the Reyleight coefficient:Therefore in our case:","747-deflation#7.4.7 Deflation":"Suppose we have computed  using the power method, we now compute the subsequent .We project out the  components from :This is called matrix deflation. We can now run the power method on  to find , the largest eigenvalue on the subspace orthogonal to .Deflation can be used to implement PCA, by computing the first  eigenvectors of a covariance matrix.","748-eigenvectors-optimize-quadratic-forms#7.4.8 Eigenvectors optimize quadratic forms":"Consider the following constrained optimization problem:for a symmetric matrix .A standard way to solve a constrained optimization problem is by forming the Lagrangian:Therefore:So the problem is reduced to , and the only solutions to maximize  given  are the eigenvectors of ."}},"/proba-ml/linear-algebra/matrix-inversion":{"title":"7.3 Matrix inversion","data":{"731-square-matrix#7.3.1 Square matrix":"The inverse of a square matrix  is . It is the unique matrix such that: is only defined if , that is if  is not singular.Some properties:For a simple 2x2 matrix, we have:For a block diagonal matrix, we invert each block separately:"}},"/proba-ml/linear-algebra/singular-value-decomposition":{"title":"7.5 Singular Value Decomposition (SVD)","data":{"":"SVD generalizes EVD to rectangular matrices. Even for square matrices, an EVD does not always exist, whereas an SVD always exists.","751-basics#7.5.1 Basics":"Any real  matrix can be decomposed as:with and  orthogonal matrices (left and right singular vectors):\n is a  matrix, containing the rank  singular values .\nThe cost of computing the SVD is","752-connection-between-evd-and-svd#7.5.2 Connection between EVD and SVD":"If  is real, symmetric, and positive definite, then the singular values are equal to the eigenvalues:More generally, we have:Hence:\n and \n and \nIn the economy size SVD:","753-pseudo-inverse#7.5.3 Pseudo inverse":"The Moore-Penrose pseudo-inverse of  is denoted  and has the following properties:If  is square and non-singular, then If  and all columns of   are linearly independent (i.e.  is full-rank):In this case,   is the left inverse of  (but not its right inverse):We can also compute the pseudo-inverse using the SVD:When , the right inverse of  is:and we have:","754--svd-and-the-range-and-null-space#7.5.4  SVD and the range and null space":"We show that the left and right singular vectors form an orthonormal basis for the range and nullspace.We have:where  is the rank of .Thus   can be written as any linear combination of left singular vectors :with dimension .For the nullspace, we define  as a linear combination of the right singular vectors for the zero singular values:Then:Hence:with dimension .We see that:This is also written:","755-truncated-svd#7.5.5 Truncated SVD":"Let  where we use the  first columns. This can be shown to be the optimal rank  approximation of  in the sense of:If , we incur some error: this is the truncated SVD.The number of parameters needed to represent an  matrix is ."}},"/proba-ml/linear-algebra/matrix-calculus":{"title":"7.8 Matrix calculus","data":{"781-derivatives#7.8.1 Derivatives":"For a scalar-argument function , we define its derivative at a point  the quantity:","782-gradients#7.8.2 Gradients":"We can extend this definition to vector-argument function , by defining the partial derivative of  w.r.t where  is the th unit vector.The gradient of a function is its vector of partial derivatives:Where the operator  maps a function  to another function The point at which the gradient is evaluated is noted:","783-directional-derivative#7.8.3 Directional derivative":"The directional derivative measure how much  changes along a direction in space :Note that:","784-total-derivative#7.8.4 Total derivative":"Suppose the function has the form , we define the total derivative w.r.t  as:Multiplying by , we get the total differential:This represents how much  changes when we change .","785-jacobian#7.8.5 Jacobian":"Consider . The Jacobian of this function is an  matrix of partial derivatives:7.8.5.1 Vector ProductThe Jacobian vector product (JVP) is right multiplying  by :Similarly, the vector Jacobian product (VJP) is left multiplying by :7.8.5.2 Composition of featureThe Jacobian of the composition of two features  is obtained with the chain rule:Let  and , we have:","786-hessian#7.8.6 Hessian":"For  that is twice differentiable, the Hessian is the   symmetric matrix of second partial derivatives:The Hessian is the Jacobian of the gradient.","787-gradients-of-commonly-used-functions#7.8.7 Gradients of commonly used functions":"7.8.7.2 Functions that map vectors to scalars7.8.7.3 Functions that map matrices to scalarQuadratic forms:Traces:Determinants:"}},"/proba-ml/linear-algebra/other-matrix-decompositions":{"title":"7.6 Other matrix decompositions","data":{"761-lu-factorization#7.6.1 LU Factorization":"We can factorize any square matrix  into a lower and upper triangle matrix  and :We might need to reorder the rows at each iteration so that the first element of  is non-zero: if  then either  or  is singular.where  is a permutation matrix, i.e. a square binary matrix where  means permuting row  with row .","762-qr-decomposition#7.6.2 QR Decomposition":"Suppose we have  a set of linearly independent basis vectors. We want to find a series of orthonormal vectors  that span the successive subspace of We have:and thus:with  and","763-cholesky-decomposition#7.6.3 Cholesky decomposition":"Any symmetric positive definite matrix can be decomposed as  where  is a lower triangular with real, positive diagonal elements. The complexity is .7.6.3.1 Application: sampling from an MVNLet . We can easily sample from  since it requires sampling from  separate 1d Gaussians.Let , we then set .We can check that:"}},"/proba-ml/linear-algebra/solving-systems-of-linear-equation":{"title":"7.7 Solving systems of Linear Equation","data":{"":"We can formulate linear equations in the form of .If we have  equations and  unknowns, then  will be a  matrix and  a  vector.\nIf  and  is full rank, there is a single unique solution\nIf  the system is underdetermined so there is no unique solution\nIf  the system is overdetermined, not all the lines intersect at the same point","771-solving-square-systems#7.7.1 Solving square systems":"When , we can solve the LU decomposition for .The point is that  and  are triangular, so we don‚Äôt need to compute the inverse and can use back substitution instead. For We start by solving , then we substitute it into  and iterate.Once we have , we can apply the same computation for .","772-solving-under-constrained-systems-least-norm-estimation#7.7.2 Solving under-constrained systems (least norm estimation)":"We assume  and   to be full-rank, so there are multiple solutions:Where  is any solution. It is standard to pick the solution minimizing the  regularization:We can compute the minimal norm solution using the right pseudo inverse:We can also solve the constrained optimization problem by minimizing the norm:Therefore:Finally we find the right pseudo inverse again:","773-solving-over-constrained-systems-least-square-estimation#7.7.3 Solving over-constrained systems (least square estimation)":"If , we‚Äôll try to find the closest solution satisfying all constrained specified by , by minimizing the least square objective:We know that the gradient is:Hence the solution is the OLS:With   the left pseudo inverse.We can check that this solution is unique by showing that the Hessian is positive definite:Which is positive definite if  is full-rank since for any  we have:"}},"/proba-ml/linear-discriminant-analysis/generative-vs-discriminative-classifiers":{"title":"9.4 Generative vs Discriminative classifiers","data":{"":"A model of the form  is generative since it can be used to generate features given the target.A model of the form  is discriminative since it can only be used to discriminate between the targets.","941-advantages-of-the-discriminative-classifiers#9.4.1 Advantages of the Discriminative Classifiers":"Better predictive accuracy because  is often much simpler to learn than .\nCan arbitrarily handle feature preprocessing, for example, polynomial expansion of the feature inputs, or replace strings with embeddings. It is hard to do with generative models since the new features can be correlated in complex ways.\nWell-calibrated probabilities. Some generative models like NBC make strong and often invalid independence assumptions, leading to extreme posterior class probabilities (near 0 and 1).","942-advantages-of-the-generative-classifiers#9.4.2. Advantages of the Generative Classifiers":"Easy to fit, the NBC only needs counting and averaging, whereas Logistic Regression needs to solve a convex optimization problem and Neural Nets non-convex optimizations.\nCan easily handle missing values by filling them with the fitted generative model (can be the empirical mean for each variable)\nCan fit classes separately, we estimate each class density independently. With discriminative models, however, all parameters interact so we need to retrain the model if we add new classes.\nCan handle unlabeled training data, it is easy to use generative models for semi-supervised learning, in which we combine labeled data  and unlabeled data . Discriminative classifiers have no uniquely optimal way to leverage .\nMay be more robust to spurious features, a discriminative model  may choose features that discriminate  well on the training set, but hardly generalize outside of it. By contrast, generative models  may be better at capturing the causal mechanism underlying the data generative process, and hence be more robust to distribution shifts.","943-handling-missing-features#9.4.3 Handling missing features":"With a generative model, we can easily deal with missing parts of  during training or testing (we assume a MAR situation).For example, suppose we don‚Äôt have access to , we have to compute:If we make the Naive Bayes assumption, it leads to:"}},"/proba-ml/linear-algebra/matrix-multiplication":{"title":"7.2 Matrix multiplication","data":{"":"The product of  and   is  with:Note that the number of columns of  must match the number of rows of .Matrix multiplication complexity averages , faster methods exists (like BLAS) and hardware parallelizing the computation, like TPU or GPU.\nMatrix multiplication is associative: \nMatrix multiplication is distributive: \nIn general, it is not commutative:","721-vector-vector-products#7.2.1 Vector-vector products":"Inner productGiven two vectors , their size is  . The inner product is the scalar:Note that Outer productGiven two vectors  and  (they no longer have the same size), the outer product is a matrix :","722-vector-matrix-products#7.2.2 Vector-matrix products":"Given matrix  and , the product is This can be viewed as inner product on rows:Or linear combination on columns:In this latter view, A is a set of basis vectors defining a linear subspace.","723-matrix-matrix-products#7.2.3 Matrix-matrix products":"The main view is a inner product between row vectors of  and columns vectors of :If  and , then","724-matrix-manipulations#7.2.4 Matrix manipulations":"Summing slicesLet , we can average rows of  by pre-multiplying a vector of ones:Conversely, we can average columns by post-multiplying.Hence, the overall mean is given by:ScalingIf we pre-multiply  by a diagonal matrix , we scale each row by :Conversely, we scale columns by post-multiplying  with the diagonal matrix Therefore, the normalization operation can be written:where  is the empirical mean vector and  the empirical standard deviation vector.Scatter matrixThe sum of squares matrix   is:The scatter matrix  is:The Gram matrix  is:The square pairwise distance between  and  is:This can also we written as:where","725-kronecker-products#7.2.5 Kronecker products":"and , then the Kronecker product belongs to :Some useful properties:","726-einstein-summation#7.2.6 Einstein summation":"Einsum consists in removing the  operator for matrix and tensor products:And for more complex operations:Einsum can perform computations in an optimal order, minimizing time and memory allocation."}},"/proba-ml/linear-discriminant-analysis/gaussian-discriminant-analysis":{"title":"9.2 Gaussian Discriminant Analysis","data":{"":"The GDA is a generative classifier where the class conditional densities are multivariate Gaussian:The corresponding class posterior has the form:","921-quadratic-decision-boundaries#9.2.1 Quadratic decision boundaries":"The log posterior over the class labels is:This is called the discriminant function. The decision function between two classes,  , and , is a quadratic function of .This technique is called Quadratic Discriminant Analysis (QDA).","922-linear-decision-boundaries#9.2.2 Linear decision boundaries":"We now consider the case of shared covariance, : is a residual term across classes that can be removed.This is called the Linear Discriminant Analysis (LDA).","923-connection-between-lda-and-logistic-regression#9.2.3 Connection between LDA and Logistic Regression":"From the LDA equation, we can write:\nIn LDA, we first fit the Gaussians (and class prior) to fit the joint likelihood , then we use  to derive \nIn Logistic Regression, we estimate  directly to maximize the conditional likelihood \nIn the binary case, this resume to:With:And by setting:We end up with the binary logistic regression:The MAP decision rule is:In the case where  and , we end up with projecting  on the  line and compare it to the centroid, .","924-model-fitting#9.2.4 Model fitting":"We now discuss how to fit the GDA model using MLE. The likelihood is:Hence the log-likelihood is:We can optimize  and  separately:\nFrom section 4.2.4, we have that the MLE of the prior is \nFrom section 4.2.6, the MLE for the Gaussian is:\nUnfortunately, the MLE for  can easily overfit if . Below are some workarounds.9.2.4.1 Tied covarianceIf we tied covariance matrices , we get linear boundaries. This also allows pooling from all samples:9.2.4.2 Diagonal covarianceIf we force  to be diagonal, we reduce the number of parameters from  to , which avoids the overfitting problem. However, we lose the ability to capture the correlation between features (this is the Naive Bayes assumption).We can restrict further the model using a tied diagonal matrix: this is the diagonal LDA.9.2.4.3 MAP estimationForcing the covariance to be diagonal is a rather strong assumption. Instead, we can perform a MAP estimation from section 4.5.2:This technique is called regularized discriminant analysis (RDA).","925-nearest-centroid-classifier#9.2.5 Nearest Centroid Classifier":"If we assume a uniform prior over classes, we compute the most probable labels as:This is called the nearest centroid classifier, where the distance is measured using squared Mahalanobis distance.We can replace this distance with any other distance metrics:We can learn distance metrics with one simple approach:The class posterior becomes:We can optimize  using gradient descent applied to the discriminative loss. This is called nearest class mean metric learning.This technique can be used for one-shot learning of new classes since we just need to see a single labeled  per class (assuming we have a good .","926-fisher-lda#9.2.6 Fisher LDA":"Discriminant analysis is a generative approach to classification, that requires fitting MVN to the feature. This might be problematic in high dimensions.An alternative could be to reduce the features to  and then to fit MVN, by finding  so that  .\nThe simplest way is using a PCA, but this is a non-supervised technique, and not taking the labels into account could lead to suboptimal low-dimensional features\nAlternatively, we could use a gradient-based method to optimize the log-likelihood, derived from the posterior in the low dimensional space, as we did with the nearest class mean metric learning\nA third approach relies on eigenvalues decomposition, by finding  so that the low dimensional data can be classified as well as possible using a Gaussian class-conditional density model: this is Fisher‚Äôs linear discriminant analysis (FLDA)\nThe drawback of FLDA is that we don‚Äôt take into account the dimensionality , since we reduce it to .In the two classes cases, we are seeking a single vector  onto which we project the data.9.2.6.1 Derivation of the optimal 1d projectionWe now estimate the optimal direction . The class-conditional means are: is the projection of the mean on the line, and  is the data projection. The variance of the projected data is:The goal is to maximize the distance between the conditional means, while minimizing the variance of the projected clusters. We maximize:where:\n is the between-class scatter matrix\n is maximize when:with Therefore, if  is invertible:Therefore:And since we only care about the direction and not the scale:If ,  is proportional to , which is intuitive.9.2.6.2 Multiple classesWe now define:We have the following scatter matrices:The objective function is:This is maximize by:where  are the  leading eigenvectors of , assuming  non singular.We see that FLDA discriminates classes better than PCA, but is limited to  dimensions, which limits its usefulness."}},"/proba-ml/linear-discriminant-analysis/intro":{"title":"9. Linear Discriminant Analysis","data":{"":"We consider classification models of the form:where  is the prior over the class labels, and  is the class conditional density for .\nThe overall model is a generative model since it specifies the distribution over the feature , \nBy contrast, a discriminative model directly estimates the class posterior"}},"/proba-ml/linear-discriminant-analysis/naive-bayes-classifiers":{"title":"9.3 Naive Bayes classifiers","data":{"":"We discuss a simple generative approach where all features are considered conditionally independent given the class label.Even if this assumption is not valid, it results in good classifiers. Since the model has only  features, it is immune to overfitting.We use the following class conditional density:Hence, the posterior is:where  is the prior probability density, and  are all the parameters.","931-example-models#9.3.1 Example models":"We need to specify the conditional likelihood, given the nature of the features:\nIf  is binary, we can use the Bernoulli distribution:\nwhere  is the probability that  in class .This is called the Bernoulli multivariate naive model. This approach to MNIST has a surprisingly good 84% accuracy.\nIf  is categorical, we can use the categorical distribution:\nIf , we use the Gaussian distribution:","932-model-fitting#9.3.2 Model fitting":"We can fit the Naive Bayes Classifier using the MLE:So:The MLE for the prior is:The MLE for  depends on the class conditional density:\nIf   is binary, we use the Bernoulli, so:\nIf  is categorical, we use the Categorical, so:\nIf , the Gaussian gives us:","933-bayesian-naive-bayes#9.3.3 Bayesian Naive Bayes":"We now compute the posterior distribution over the parameters. Let's assume we have categorical features, so:where .We show in section 4.6.3 that the conjugate is the Dirichlet distribution:Similarly, we use a Dirichlet distribution for the prior:We can compute the posterior in close form:Where  and We derive the posterior predictive distribution as follows.The prior over the label is given by:where For the features, we have:where is the posterior mean of the parameters.If , this reduces to the MLEIf , we add 1 to the empirical count before normalizing. This is called Laplace smoothing.In the binary, case this gives us:Once we have estimated the parameter posterior, the predicted distribution over the label is:This gives us a fully Bayesian form of naive Bayes, in which we have integrated out all the parameters (the posterior mean parameters)","934-the-connection-between-naive-bayes-and-logistic-regression#9.3.4 The connection between naive Bayes and Logistic regression":"Let , so  is a one-hot encoding of feature .The class conditional density is:So the posterior over classes is:which can be written as softmax:which corresponds to the multinomial logistic regression.The difference is that the NBC minimizes the join likelihood , whereas the logistic regression minimizes the conditional likelihood"}},"/proba-ml/linear-regression/least-squares-linear-regression":{"title":"11.2 Least squares linear regression","data":{"1121-terminology#11.2.1 Terminology":"We consider models of the form:The vector  is known as weights. Each coefficient  specifies the change in output we expect if we change the corresponding input  by 1 unit. is the bias and corresponds to the output when all inputs are 0. This captures the unconditional mean of the response By assuming  to be written as , we can absorb the term   in .If the input is multi-dimensional , this is called multivariate linear regression:Generally, a straight line will underfit most datasets, but we can apply nonlinear transformations to the input features .As long as the parameters of  are fixed, the model remains linear in the parameters, even if it‚Äôs not linear in the raw inputs.Polynomial expansion is an example of non-linear transformation: if the input is 1d and we use an expansion of degree , we get","1122-least-square-estimation#11.2.2 Least square estimation":"To fit a linear regression model to the data, we minimize the negative log-likelihood on the training set:If we only focus on optimizing  and not , the NLL is equal to the residual sum of square (RSS):11.2.2.1 Ordinary least squareWe can show that the gradient is given by:We set it to zero and obtained the ordinary least square solution:The quantity   is the left pseudo-inverse of the (non-square) matrix .We check that the solution is unique by showing that the Hessian is positive definite:If  is full rank (its columns are linearly independent) then  is positive definite, hence the least square objective has a global minimum.11.2.2.2 Geometric interpretation of least squaresWe assume , so there are more equations than unknown, the system is over-determined.We seek a vector  that lies in the linear subspace spanned by , as close as possible to :Since , there exists some weight vector  such that:To minimize the norm of the residual , we want the residual to be orthogonal to every column of :Hence, our projected value is given by:11.2.2.3 Algorithmic issuesEven if we can theoretically invert  to compute the pseudo-inverse , we shouldn‚Äôt do it since  could be ill-conditioned or singular.A more general approach is to compute the pseudo-inverse using SVD. scikit-learn.linear_model uses scipy.linalg.lstsq, which in turn uses DGELSD, which is an SVD-based solver implemented by the Lapack library, written in Fortran.However, if , it can be faster to use QR decomposition:Since  is upper triangular, we can solve it using back substitution, avoiding matrix inversion.An alternative to matrix decomposition is to use an iterative solver, such as the conjugate gradient method (assuming  is symmetric positive definite) and the GMRES (generalized minimal residual method) in the general case.11.2.2.4 Weighted least squaresIn some cases, we want to associate a weight with each example. For example, in heteroskedastic regression, the variance depends on the input:thus:where .One can show that the MLE is given by:","1123-other-approaches-to-computing-the-mle#11.2.3 Other approaches to computing the MLE":"11.2.3.1 Solving for offset and slope separatelyAbove, we computed  at the same time by adding a constant  column to .Alternatively, we can solve for  and  separately:Thus, we can first estimate  on centered data, then compute .11.2.3.2 1d linear regressionIn the case of 1d inputs, the above formula reduces to:where  and 11.2.3.3 Partial regressionWe can compute the regression coefficient in the 1d case as:Consider now the 2d case:  , where The optimal regression coefficient for  is  where we keep  constant:We can extend this approach to all coefficients, by fixing all inputs but one.11.2.3.4 Online computing of the MLEWe can update the means online:One can show that we can also update the covariance online as:We find similar results for .To extend this approach to  dimensions inputs, the easiest approach is SGD. The resulting algorithm is called least mean square.11.2.3.5 Deriving the MLE from a generative perspectiveLinear regression is a discriminative model of the form , but we can also use generative models for regression.The goal is to compute the conditional expectation:Suppose we fit  using a MVN. The MLEs for the parameters of the joint distribution are the empirical means and covariance:Hence we have:which we can rewrite as:using Thus we see that fitting the joint model and then conditioning it is the same as fitting the conditional model. However, this is only true for Gaussian models.11.2.3.6 Deriving the MLE for After estimating , it is easy to show that:which is the MSE on the residuals.","1124-measuring-goodness-of-a-fit#11.2.4 Measuring goodness of a fit":"11.2.4.1 Residuals plotWe can check the fit of the model by plotting the residuals  vs the input , for a chosen dimension.The residual model assumes a  distribution, therefore the residual plot should show a cloud of points around 0 without any obvious trend. This is why b) above is a better fit than a).We can also plot the prediction  against , which should show a diagonal line.11.4.2.2 Prediction accuracy and R2We can assess the fit quantitatively using the RSS or the RMSE:The R2, or coefficient of determination is a more interpretable measure:It measures the variance in the prediction relative to a simple constant prediction ."}},"/proba-ml/linear-regression/intro":{"title":"11. Linear Regression","data":{"":"The key property of linear regression is the expected value of the output is assumed to be a linear function of the input:"}},"/proba-ml/linear-regression/lasso-regression":{"title":"11.4 Lasso regression","data":{"":"In ridge regression, we assumed a Gaussian prior on the parameters, which is often a good choice since it leads to smaller parameters and limits overfitting.However, sometimes we want the parameters to be exactly 0, i.e. we want  to be sparse so that we minimize the  norm:This is useful because it can perform feature selection, as , if , the feature  is ignored.The same idea can be applied to non-linear models (like DNN) and encourage the first layer weight to be sparse.","1141-map-estimation-using-a-laplace-prior-ell_1-regularization#11.4.1 MAP estimation using a Laplace prior ( regularization)":"We now focus on MAP estimation using the Laplace prior:where:Laplace put more weight on 0 than Gaussian, for the same scale.To perform MAP estimation of a linear regression model with this prior, we minimize the objective:This method is called lasso ‚Äúleast absolute shrinkage and selection operator‚Äù or  regularization.Note we could also use the q-norm:For , we can get even sparser solutions but the problem becomes non-convex. Thus, the  norm is the tightest convex relaxation of the  norm.","1142-why-does-the-ell_1-yields-sparse-solutions#11.4.2 Why does the  yields sparse solutions?":"The lasso objective is a non-smooth objective that can be expressed with the Lagrangian:where  is an upper bound to the weight. A tight bound corresponds to a high .From the optimization theory, we know that the optimal solution lies at the intersection of the constraint surface and the lowest level of the objective function.When we relax the constraint , we grow the constraint surface until reaching the objective. The  dice is more likely to hit the objective function on the corner rather than on the side, especially in high dimensions. The corners correspond to the sparse solution, lying on the coordinate axes.Meanwhile, there are no corners on the  ball, it can intersect the objective function everywhere; there is no preference for sparsity.","1143-hard-vs-soft-thresholding#11.4.3 Hard vs soft thresholding":"One can show that the gradient for the smooth part is:with:where  is  without the component .Setting the gradient to zero gives the optimal update for feature  while keeping the other constant:with  the residuals error obtained by using all features but .Let‚Äôs add the penalty term. Unfortunately,  is not differentiable whenever . We can still compute the subgradient at this point.Depending on the value of , the solution to  can occur at 3 different values of  as follows:\nIf , the feature is strongly negatively correlated with the residual, then the subgradient is zero at \nIf , the feature is weakly correlated with the residual, then the subgradient is zero at \nIf , the feature is strongly correlated with the residual, the subgradient is zero at \nIn summary:where:The slope of the soft-thresholding shrunk line doesn‚Äôt match the diagonal (the OLS), meaning that event big coefficients are shrunk to zero.Consequently, Lasso is a biased estimator. A simple solution known as debiasing is to first estimate which elements are non-zero using lasso, then re-estimate the chosen coefficient using least squares.","1144-regularization-path#11.4.4 Regularization path":"If , we get the OLS, which is dense. As we increase it, we reach  where .This value is obtained when the gradient of the NLL cancels out the gradient of the penalty:As we increase , the solution vector  get sparser, although not necessarily monotonically. We plot the regularization path:","1146-variable-selection-consistency#11.4.6 Variable selection consistency":"regularization is often used to perform feature selection. A method that can recover the true set of meaningful features when  is called model selection consistent.Let‚Äôs create:\na sparse signal , consisting of 160 randomly placed  spikes\na random design matrix \na noisy observation  , with \nwith The second row is the  estimate using . We see that it finds all coefficients , but they are too small due to shrinkage.The third row uses the debiasing technique and recovers the right coefficient values.By contrast, the last row represents the signal recovered by the OLS, which is dense.To pick the optimal value of  and perform the feature selection, we use cross-validation. It is important to note that CV will lead to good prediction accuracy, which is not necessarily the same as recovering the true model.This is because Lasso performs selection and shrinkage i.e. the chosen coefficients are brought closer to , so the CV will pick a value of  that is not too large. This will result in a less sparse model with irrelevant variables.However, various extensions to the basic method have been designed that are model selection consistent. See Statistical Learning with Sparsity (2015)","1147-group-lasso#11.4.7 Group lasso":"In standard  regularization, we assume a 1:1 correspondence between a parameter and its variable. But in more complex settings, there may be many parameters associated with a single variable, so that each variable has its weight vector  and the overall weight vector has a block structure .To exclude the variable , we have to force the entire  block to be : this is group sparsity.11.4.7.1 Applications\nLinear regression with categorical inputs, where we use a one-hot-encoding vector of length \nMultinomial logistic regression: The ‚Äôth variable will be associated with  different weight (one per class)\nMulti-task learning, similarly each feature is associated with  different weighs (one per output task)\nNeural networks: The ‚Äôth neuron will have multiple inputs, so we set all incoming weights to zero to turn it off. Group sparsity can help us learn the neural net structure.\n11.4.8.1 Penalizing the two-normTo encourage group sparsity, we partition the parameter vector into  groups, then we minimize:where  is the two-norm of the group weight vector. If the NLL is least-squares, this method is the group lasso. Note that if we had the squared of the two norms, the model would be equivalent to ridge regression.By using the square root, we are penalizing the radius of a ball containing the group weights: the only way for the radius to be small is if all elements are small too.Another way to see why the two-norm enforces group sparsity is to consider the gradient of the objective. If we only have one group of two parameters:\nIf  is small, the gradient will be close to 1 and the  will be shrunk to zero\nIf  is large, then if  is small the gradient will be close to  and  will not get updated much.\n11.4.8.2 Penalizing the infinity normA variant of this technique replaces the two-norm with the infinity norm:This will also result in group sparsity, since if the largest element of the group is forced to zero, all the others will be zero.We have a true signal , with , divided into 64 groups of size 64.We randomly pick 8 groups and assigned them to non-zero values.We generate a random design matrice  and compute  like before.We see that the group lasso (c) does a much better job than vanilla lasso (b). With the  norm, elements of the same groups have similar magnitude.","1148-elastic-net-ridge-and-lasso-combined#11.4.8 Elastic net (ridge and lasso combined)":"In group lasso, we need to specify the structure ahead of time. For some problems, we don‚Äôt know the group structure, and yet we would still like highly correlated coefficients to be treated as an implicit group.Elastic net can achieve this, named after being ‚Äúlike a stretchable fishing net that retains all the big fish‚Äù. It has the following objective:This is strictly convex (assuming , so there is a unique global minimum, even if  is not full rank.It can be shown that any strictly convex penalty on  will exhibit a group effect, and coefficients of highly correlated variables will be the same.If two features are identically equal, so , one can show that their estimate .By contrast with Lasso, we might have , and  and vice versa. Resulting in less stable estimates.Also, when , lasso can only select up to  non-zero coefficients, while elastic net doesn‚Äôt have such limits, thus exploring more subsets of variables.","1149-optimization-algorithms#11.4.9 Optimization algorithms":"Below, we expand on the method used to solve the lasso problem and other -regularized convex objectives.11.4.9.1 Coordinate descentSometimes, it‚Äôs easier to optimize all variables independently. We can solve for the th coefficient while keeping all others constant.We can either loop through all variables, select them at random, or update the steepest gradient. This approach has been generalized to GLM with the glmnet library.11.4.9.2 Projected gradient descentWe convert the non-differentiable  penalty into a smooth regularizer. We use the split variable trickwhere  and Thus we get the smooth but constrained optimization problem:In the case of Gaussian likelihood, the NLL is the least square loss, and the objective becomes a quadratic program (see section 8.5.4).One way to solve this is to use projected gradient descent (see section 8.6.1). We can enforce the constraint by projecting onto the positive orthant, using . This operation is denoted .The projected gradient update takes the form:where  is the unit vector of all ones.11.4.9.3 Proximal gradient descentProximal gradient descent can be used to optimize smooth function with non-smooth penalty, like the .In section 8.6.2, we showed that the proximal gradient for the  penalty corresponds to the soft thresholding. Thus the update is:Where the soft threshold is applied element-wise. This is called the iterative soft threshold algorithm (ITSA). If we combine it we Nesterov momentum, we get the fast ISTA or FISTA method, which is widely used to fit sparse linear models.11.4.9.4 LARSThe Least Angle Regression and Shrinkage (LARS) generates a set of solutions for different , starting with an empty set up to a full regularization path.The idea is that we can quickly compute  from  if , this is known as warm starting.If we only want a single value , it can be more efficient to start from  down to . This is known as the continuation method or homotopy.LARS starts with a large  that only selects the feature that is the most correlated to . Then,  is decreased until finding a feature with a correlation of the same magnitude as the first, but with the residual, defined as:where  is the active set of features at step .Remarkably, we can find the value of  analytically, by using a geometric argument (hence ‚Äúleast angle‚Äù). This allows the algorithm to quickly jump where the next point of the regularization path where the active set changes.This repeats until all variables are added."}},"/proba-ml/linear-regression/regression-splines":{"title":"11.5 Regression Splines","data":{"":"As we have seen, we can use polynomial basis functions to create nonlinear mappings, even though the model remains linear in parameters.One issue is that polynomials are a global approximation to the function, we can achieve more flexibility by using a series of local approximations.We will restrict ourselves to 1d to better understand the notion of locality. We can approximate the function using:where  is the th basis function.A common way to define such basis functions is to use B-splines (‚ÄùB‚Äù stands for basis and ‚Äúsplines‚Äù refers to a piece of flexible material used by artists to draw curves)","1151-b-spline-basis-functions#11.5.1 B-spline basis functions":"A spline is a piecewise polynomial of degree , where the locations of the pieces are defined by a set of knots .The polynomial is defined on each of the intervals . The function has continuous derivatives of order  at its knot points.It is common to use cubic splines, where , ensuring the function has first and second-order derivatives at each knot.We don‚Äôt focus on the inner working of the splines, it suffices to say we can use scikit-learn to convert the  matrix to , with  the number of knots.We see that for any given input, only  basis functions are active (non-zero). For the piecewise constant function, , for linear , and for cubic .","1152-fitting-a-linear-model-using-a-spline-basis#11.5.2 Fitting a linear model using a spline basis":"After computing the design matrix , we can use it to fit least squares (or ridge regression, having regularization is often better).We use a dataset of temperature having a semi-periodic structure, a fit it using cubic splines. We chose 15 knots according to the quantiles of the data.We see that the fit is reasonable, adding more knots would have resulted in a better fit, with a risk of overfitting, we can select these using model selection, like grid-search plus with cross-validation.","1153-smoothing-splines#11.5.3 Smoothing splines":"Smoothing splines are related to regression splines, but use  knots, the number of datapoints. They are non-parametric methods since the number of parameters is not fixed a priori but grows with the size of the data.To avoid overfitting, smoothing splines rely on  regularization.","1154-generalized-additive-models-gam#11.5.4 Generalized additive models (GAM)":"A generalized additive models extend the notion of spline to -dimensional inputs, without interaction between features. It assumes a function of the form:where each  is a regression or smoothing spline.This model can be fitted using backfitting, which iteratively fits each  to the partial residuals generated by the other terms.We can extend GAM beyond the regression case, e.g. to classification, by using a link function as in generalized linear models."}},"/proba-ml/linear-regression/ridge-regression":{"title":"11.3 Ridge regression","data":{"":"Maximum likelihood estimation can result in overfitting, a simple solution is to use MAP estimation with zero-mean Gaussian prior:We compute the MAP as:Therefore we are penalizing the weights that become too large in magnitude. This is called  regularization or weight decay.We don‚Äôt penalize the term  since it doesn‚Äôt contribute to overfitting.","1131-computing-the-map-estimate#11.3.1 Computing the MAP estimate":"The MAP estimation corresponds to computing the minimizing the objective:we have:Hence:11.3.1.1 Solving using QRNaively performing the matrix inversion can be slow and numerically unstable. We propose a way to convert the problem to a standard least square, where we can apply QR decomposition as previously seen.We assume the prior has the form  where  is the precision matrix.We can emulate this prior by augmenting our training data as:where   is the Cholesky decomposition.We now show that the RSS of this expended data is equivalent to the penalized RSS on the original data:Hence, the MAP estimate is given by:And then solving using standard OLS method, by computing the QR decomposition of . This takes 11.3.1.2 Solving using SVDIn this section, we assume , which is a framework that suits ridge regression well. In this case, it is faster to compute SVD than QR.Let , with\n, so that \n, so that \nOne can show that:In other words, we can replace the -dimensional vector  with -dimensional vector  and perform our penalized fit as before.The resulting complexity is , which is less than  if","1132-connection-between-ridge-regression-and-pca#11.3.2 Connection between ridge regression and PCA":"The ridge predictions on the training set are given by:with:Hence:In contrast, the least square prediction are:If , the direction  will have a small impact on the prediction. This is what we want, since small singular values corresponds to direction with high posterior variance. These are the directions ridge shrinks the most.There is a related technique called principal components regression, a supervised PCA reducing dimensionality to  followed by a regression. However, this is usually less accurate than ridge, since it only uses  features, when ridge uses a soft-weighting of all the dimensions.","1133-choosing-the-strength-of-the-regularizer#11.3.3 Choosing the strength of the regularizer":"To find the optimal , we can run cross-validation on a finite set of values and get the expected loss.This approach can be expensive for large set of hyper-parameters, but fortunately we can often warm-start the optimization procedure, using the value of  as a initializer for .If we set , we start from a high amount of regularization and gradually diminish it.We can also use empirical Bayes to choose , by computing:where  is the marginal likelihood.This gives the same result as CV estimate, however the Bayesian approach only fit a single model, and  is a smooth function of , so we can use gradient-based optimization instead of a discrete search."}},"/proba-ml/linear-regression/robust-linear-regression":{"title":"11.6 Robust linear regression","data":{"":"It is common to model the noise in linear regression with a Gaussian distribution , where  . In this case, maximizing the likelihood is equivalent to minimizing the sum of squared residuals.However, if we have outliers in our data, this can result in a bad fit because squared error penalizes deviations quadratically, so points far from the mean have more effect.One way to achieve robustness is to use a likelihood with a heavy tail, assigning a higher likelihood to outliers without perturbing the straight line to explain them.We discuss several alternative probability distributions for the response variable.","1161-laplace-likelihood#11.6.1. Laplace Likelihood":"If we use the Laplace distribution as our likelihood, we have:The robustness arises from the replacement of the squared term by an absolute one.11.6.1.1 Computing the MLE using linear programmingLinear programming solves equations of the form:where ,  and  is the set of  linear constraints we must satisfy.Let us define , where .We want to minimize the sum of residuals, so we define .We need to enforce , but it is sufficient to enforce , which we can encode as two linear constraints:which we can rewrite as:where the first  entries are filled with .We can write these constraints in the form , with:","1162-student-t-likelihood#11.6.2 Student-t likelihood":"To use the Student distribution in a regression context, we can make the mean a linear function of the inputs:where we use the location-scale t-distribution pdf:We can fit this model using SGD or EM.","1163-huber-loss#11.6.3 Huber loss":"An alternative to minimize the NLL using Laplace or Student distribution is using the Huber loss:This is the equivalent of using the  loss for errors smaller than  and the  loss for larger errors.This is differentiable everywhere, consequently using it is much faster than using the Laplace likelihood, since we can use standard smooth optimization methods (like SGD) instead of linear programming.The parameter  controls the degree of robustness and is usually set by cross-validation.","1164-ransac#11.6.4 RANSAC":"In the computer vision community, a common approach to regression is to use random sample consensus (RANSAC).We sample a small set of points, fit the model to them, identify outliers (based on large residuals), remove them, and refit the model on inliers.We repeat this procedure for many iterations before picking the best model.A deterministic alternative to RANSAC is fitting the model on all points to compute , then iteratively remove the outliers and refit the model with remaining inliers to compute .Even though this hard thresholding makes the problem non-convex, this usually rapidly converges to the optimal estimate."}},"/proba-ml/logistic-regression/intro":{"title":"10. Logistic Regression","data":{"":"Logistic regression is a discriminative classifier  where .If , this is a binary logistic regression, otherwise, this is a multinomial logistic regression (or alternatively, multiclass)."}},"/proba-ml/neural-networks-for-images/common-architectures":{"title":"14.3 Common architectures for image classification","data":{"":"CNNs are commonly used to perform image classification, which is the task of estimating the function:where   is the number of channels and  is the number of classes.We now briefly review various CNNs architectures developed over the years to solve image classification classes.For more models, see this extensive review of CNN's history.Also see timm, now hosted by huggingface, for PyTorch implementations.","1431-lenet#14.3.1 LeNet":"One of the earliest CNN was named after its creator, Yann LeCun. It was designed to classify images of handwritten digitals and was trained on the MNIST dataset.It achieves 98.8 % accuracy after a single epoch, against 95.9% for the MLP.Of course, classifying digits is of limited utility. Classifying entire strings also requires image segmentation, so LeCun and its colleague devised a way to combine CNN with a conditional random field.This system was deployed by US postal services.","1432-alexnet#14.3.2 AlexNet":"It was not until 2012 and the AlexNet paper by Alex Krizhevsky that mainstream computer vision researchers paid attention to CNNs.In this paper, the (top 5) error rate on ImageNet has been reduced from 26% to 15%, which was a dramatic improvement.Its architecture is very similar to LeNet with the following differences:\nDeeper, with 8 layers instead of 5, excluding pooling layers\nUses ReLU non-linearities instead of tanh\nUses Dropout for regularization instead of weight decay\nStacks several convolutional layers on top of each other instead of strictly alternating them with pooling. This has the advantage of enlarging the receptive field (three stacked  filters will have a receptive size of ). This is better than using a single filter of size  because the stacked filters have nonlinearities between them, and have fewer parameters in total than the  filter.\nNote that AlexNet has 60M parameters (much more than the 1M labeled examples), mostly due to the 3 FCN layers at the output.Fitting this model required using 2 GPUs (since GPUs memory was lower at the time) and is considered an engineering tour de force.","1433-googlenet-inception#14.3.3 GoogLeNet (Inception)":"The name of this model is a pun between Google and LeNet.The model innovates with the Inception block, named after the movie Inception, in which the phrase ‚ÄúWe need to go deeper‚Äù became a popular Meme.Inception block employs multiple parallel paths, each with a different filter size. This lets the model learn what the optimal filter size should be at each level.The overall model consists of 9 inception blocks followed by global average pooling.","1434-resnet#14.3.4 ResNet":"The winner of the 2015 ImageNet challenge was a team at Microsoft who proposed ResNet.The key idea is to replace  with residual block: only needs to learn the residual, or difference, between the input and output of this layer, which is a simpler task. has the form conv-BN-ReLU-conv-BN.\nPadding on the residual  ensures the spatial dimension matches the output of the convolution layer \n1x1 convolution ensures the channel numbers of the residual match the channel numbers of the convolution layer output\nThe use of residual blocks allows to train very deep network since the gradient can flow directly from the output to the earlier layers, using skip connections.Resnet-18 architecture can be represented as:(Conv : BN : Max ) : (R : R) : (R‚Äô : R) x 3 : Avg : FCwhere R is a residual block, R‚Äô is a residual block with skip-connection with stride 2, and FC is a fully connected layer.The input size gets reduced spatially by  because of the factor of 2 for each R‚Äô block (the stride is 2), plus the MaxPool and the initial Conv-7x7Therefore a 224x224 image gets reduced to a 7x7 image before going into the global average pooling layer.Better performances are achieved by scaling the ResNet-18 up to ResNet-152.PreResnet shows that the signal is still being attenuated because of the posterior nonlinearity. They instead propose:Now it is very easy for the network to learn the identity function. If we use ReLu, we do so by setting all the weights and biases of the layer to zero so that .By doing so, they scaled the model to 1001 layers.Wide Resnet suggests an alternative, with a lot of channels per layer.","1435-densenet#14.3.5 DenseNet":"An alternative to Resnet is to concatenate the residual instead of adding it:Thus the overall model has the form:The dense connectivity increases the number of parameters since the channels get stacked depthwise. We can compensate by adding 1x1 convolutions in between, along with pooling layers with a stride of 2 to reduce the spatial dimension.DenseNets can outperform ResNets since all computed features are directly accessible to the output layer. However, they can be more expensive to compute.","1436-neural-architecture-search#14.3.6 Neural architecture search":"Many CNNs are similar in their design and simply rearrange blocks and hyper-parameters.In 2022, ConvNext is considered the state of the art for a wide variety of vision tasks and was created on top of a ResNet, by adapting training and architecture decisions from Swin-T (a ViT model).We can automate this discovery process by using black-box (derivation-free) optimization models to find architecture that minimizes the validation loss. This is called AutoML, or in deep learning Neural Architecture Search (NAS)."}},"/proba-ml/neural-networks-for-images/generating-images":{"title":"14.6 Generating images by inverting CNNs","data":{"":"A CNN trained for image classification is a discriminative model of the form , returning a probability distribution over  class labels.In this section, we convert this model into a conditional generative image model of the form . This will allow us to generate images belonging to a specific class.","1461-converting-a-trained-classifier-into-a-generative-model#14.6.1 Converting a trained classifier into a generative model":"We can define a joint distribution over images and labels .If we select a specific label value, we can create a conditional generative model .Since  is not an invertible function, the prior  will play an important role as a regularizer.One way to sample from this model is to use Metropolis Hasting algorithm, were the energy function is:Since the gradient is available, we then make the update:where:This is called the Metropolis adjusted Langevin algorithm (MALA).As an approximation we skip the rejection part and accept every candidate, this is called the unadjusted Lanvin algorithm and we used for conditional image generation.Thus, we get an update over the space of images that looks like a noisy SGD, but the gradient are taken w.r.t the inputs instead of the weights:\n ensures the image is plausible under the prior\n ensures the image is plausible under the likelihood\n is a noise term to enforce diversity. If set to 0, the method a deterministic algorithm generating the ‚Äúmost likely image‚Äù for this class.","1462-image-priors#14.6.2 Image priors":"We now discuss priors to regularize the ill-posed problem of inverting a classifier. Priors, combined with the initial image, determine the kinds of outputs to generate.14.6.2.1 Gaussian priorThe simplest prior is , assuming the pixels have been centered. This can prevent pixels to have extreme values.The update due to the prior term has the form:The update is (assuming  and ):This can generate the following samples:14.6.2.2 Total variation (TV) priorTo help generating more realistic images, we can add extra regularizers like total variation or TV norm of an image. This is the integral of the per-pixel gradients, approximated as:where  is the pixel value of at location  for channel .We can rewrite this in term of the horizontal and vertical Sobel edge detector applied to each channel:Using:discourage images from having high frequency artifacts.Gaussian blur can also be used instead of TV, with similar effects.","1463-visualizing-the-feature-learned-by-a-cnn#14.6.3 Visualizing the feature learned by a CNN":"Activation maximization (AM) optimizes a random image to maximize the activation of a given layer. This can be useful to get a better grasp of the features learned by the ConvNet.Below are the results for each layer of an AlexNet, using a TV prior. Simple edges and blobs are recognized first, and then as depth increase, we find textures, object parts and finally full objects.This is believed to be roughly similar to the hierarchical structure of the visual cortex.","1464-deep-dream#14.6.4 Deep Dream":"Instead of generating images that maximize the feature map of a given layer or class labels, we now want to express these features over an input image.We view or trained classifier as a feature extractor. To amplify features from layers , we define a loss function of the form:whereis the feature vector for layer .We can now use gradient descent to optimise this loss and update our original image. By using the output as input in a loop, we iteratively add features to it.This is called DeepDream, because the model amplifies features that were only initially hinted.The output image above is a hybrid between the original image and ‚Äúhallucinations‚Äù of dog parts, because ImageNet contains contains so many kinds of dogs.","1465-neural-style-transfer#14.6.5 Neural style transfer":"Neural style transfer give more generative control to the user, by specifying a ‚Äúcontent‚Äù image  and a ‚Äústyle‚Äù image . The model will try to generate new image  that re-renders by applying the style .14.6.5.1 How it worksStyle transfer works by optimizing the loss function:where  is the total variation prior.The content loss measures similarity between  to  by comparing feature maps of a pre-trained ConvNet  in the layer :We can interpret the style as a statistical distribution of some kinds of image features. Their location don‚Äôt matter, but their co-occurence does.To capture the co-occurence statistics, we compute the Gram matrix for the image using feature maps at layer :The Gram matrix is a  matrix proportional to the uncentered covariance.Given this, the style loss is:14.6.5.2 Speeding up the methodIn the Neural style transfer paper, they used L-BFGS to optimize the total loss, starting from white noise.We can get faster results if we use Adam and we start from an actual image instead of noise. Nevertheless, running optimization for every single content and style image is slow.i) Amortized optimizationInstead, we can train a network to predict the outcome of this optimization. This can be viewed as a form of amortized optimization.We fit a model for every style image:We can apply this to new content images without having to reoptimize.ii) Conditional instance normalizationMore recently, it has been shown that we can train a network that take both the content image and a discrete style representation as inputs:This avoids training a separate network for every style instances.The key idea is to standardize the features at a given layer using scale and shift parameters that are specific to the style.In particular, we use conditional instance normalization:where  and  are the mean and std of feature maps at a given layer, and  and  are parameters for style .This simple trick is enough to capture many kind of styles.iii) Adaptative instance normalizationThe drawback of the above techniques is that they are limited to a set of pre-defined styles.Adaptative instance normalization proposes to generalize this by replacing  and  by learned parameters from another network, taking arbitrary style image as input:"}},"/proba-ml/neural-networks-for-images/common-layers":{"title":"14.2 Common layers","data":{"1421-convolutional-layers#14.2.1 Convolutional layers":"We describe the basics of 1d and 2d convolution and their role in CNNs.14.2.1.1 Convolution in 1dThe convolution between two functions  is:If we use discrete functions, we evaluate\n at the points  to yield the filter \n at the points  to yield the feature vector \nThe above equation becomes:Cross-correlation is a close operation where we don‚Äôt ‚Äúflip‚Äù the weight vector. After eliminating the negative indices:If the weight vector is symmetric, as it‚Äôs often the case, then cross-correlation and convolution are the same.In deep learning, cross-correlation is often called ‚Äúconvolution‚Äù, and we will follow this convention.14.2.1.2 Convolution in 2dIn 2d, the cross-correlation equation becomes:where the 2d filter  has size .For instance, convolving a  input  with a  kernel :We can think of 2d convolution as feature detection: if  represents an edge, the activation will ‚Äúlight-up‚Äù where an edge is present in the input.The result  is therefore called a feature map.14.2.1.3 Convolution as matrix-vector multiplicationSince convolution is a linear operator, we can represent it by matrix multiplication. By flattening  into , we get:We see that CNNs are like MLPs where the weight matrices have a special spatial sparse structure.This implements the idea of translation invariance and reduces the number of parameters compared to a dense layer.14.2.1.4 Boundary conditions and padding\nValid convolution, where we only apply the filter to ‚Äúvalid‚Äù parts of the image, produces outputs that are smaller than inputs\nSame convolution uses zero-padding (adding a border of 0s) on the input to ensure the size of input and output are the same.\n14.2.1.5 Strided convolutionsSince every output pixel is generated by a weighted combination of inputs in its receptive field, neighboring outputs will have similar values because their inputs are overlapping.We can reduce this redundancy by choosing a stride different than 1.Given:\nthe input has size \nthe filter ,\nwe use zero padding on each side of size  and \nwe use strides of size  and \nThe output size will be:14.2.1.6 Multiple inputs and output channelsIf the input image is not grayscale but has multiple channels (RGB or hyper-spectral bands for satellite images) we can extend the definition of convolution using a 3d weight matrix  or tensor:where  is the stride (same for height and width for simplicity) and  is the bias term.Each weight matrix can only detect a single type of pattern, but we usually want to detect many.We can do this by using a 4d weight matrix, where the filter to detect feature  in channel  is stored in The convolution becomes:which is exactly the formula used in PyTorch Conv2d.Each vertical cylindrical column is a set of output features at , this is called a hypercolumn.Each element is a different weighted combination of the  features in the receptive field of the layer below.14.2.1.7 Pointwise () convolutionSometimes, we want to take a weighted combination of the feature at a given location, instead of across locations. This can be achieved with pointwise (or 1x1) convolution.This changes the number of channels from  to  and can be thought of as a linear layer applied to a 2D input.","1422-pooling-layers#14.2.2 Pooling layers":"Convolution preserves spatial information, this property is known as equivariance.Sometimes, we want to be invariant to the location, e.g. we want to know if an object is inside the image or not, regardless of its location.This can be achieved using max pooling, which just computes the maximum over its incoming values. An alternative is to use average pooling, replacing the max by the mean.If we average over all locations in the feature map, this is called global average pooling. This converts a feature map dimensions from  to .This can then be reshaped as a -dimensional vector, passed to a fully connected layer to convert it into a -dimensional vector, then finally passed to a softmax output.The use of global average pooling means we can use images of any size since the final feature map will always be converted to a  dimensional vector before being mapped to a distribution over the  classes.This is available under the MaxPool2d operator in PyTorch","1423-putting-it-all-together#14.2.3 Putting it all together":"A common design pattern is to create a CNN by alternating convolution layers with max-pooling layers, followed by a linear classification at the end.We omit normalization layers in this example since the model is shallow.This design pattern first appeared in Fukushima‚Äôs neocognitron and in 1998 Yann LeCun used it in its eponymous LeNet model, which also used SGD and backprop to estimate its parameters.","1424-normalization-layers#14.2.4 Normalization layers":"To scale this design to deeper networks and avoid vanishing or exploding gradient issues, we use normalizing layers to standardize the statistics of the hidden units.14.2.4.1 Batch normalizationThe most popular normalization layer is batch normalization (BN). This ensures the distribution of activations within a layer has zero mean and unit variance, when average across the samples in a minibatch:where  is the minibatch containing example ,  and   are learnable parameters for this layer (since this operation is differentiable).The empirical means and variance of layers keep changing, so we need to recompute  and  for each minibatch.At test time, we might have a single input, so we can‚Äôt compute batch statistics. The standard solution is to:\nCompute  and  for each layer using the full batch\nFreeze these parameters and add them along  and \nAt test time, use these frozen parameters instead of computing the batch statistics\nThus, when using a model with BN, we need to specify if we are using it for training or inference.PyTorch implements this with BatchNorm2d.For speed, we can combine a frozen batch norm layer with the previous layer. If the previous layer computed , using BN yields:We can write the combined layers as the fused batch norm:where  and The benefits of batch normalization for CNN can be quite dramatic, especially for the deeper ones. The exact reasons are still unclear, but BN seems to:\nmake the optimization landscape a bit smoother\nreduce the sensitivity to the learning rate (allow a larger one)\nIn addition to computation advantages, it also has statistical advantages since it plays the role of a regularizer. It can be shown to be equivalent to a form of approximate Bayesian inference.However, the reliance on a minibatch of data can result in unstable estimates when training with small batch sizes, even though **batch renormalization** partially addresses this using rolling means on batch statistics.14.2.4.2 Other kinds of normalization layersWhile batch normalization struggles with small batch size, we can pool statistics from other dimensions of the tensor:\nLayer norm pools over channel, height, and width and match on the batch index\nInstance norm pools do the same but for each channel separately\nGrouping norm generalizes this to group of channels\nMore recently, **filter response normalization (FRN)** has been proposed as an alternative to batch norm that works well on image classification and object detection, even when batch size is 1.It is applied to each channel and each batch like instance norm, but instead of standardizing it divide by the mean squared norm:Since there is no mean centering, the output can drift away from 0, which can have detrimental effects, especially with ReLU activations.To compensate for it, the authors suggest adding a thresholded linear unit (TLU) at the output, that has the form:where  is a learnable offset.14.2.4.3 Normalization-free networksRecently, normalizer-free networks have been proposed for residual networks. The key is to replace batch norm with adaptive gradient clipping, as an alternative way to avoid training instabilities.We therefore use:but adapt the clipping strength dynamically.The resulting model is faster to train and more accurate than other competitive models trained with batch norm."}},"/proba-ml/neural-networks-for-images/intro":{"title":"14.1 Introduction","data":{"":"This chapter considers the case of data with a 2d spatial structure.To see why MLP isn‚Äôt a good fit for this problem, recall that the th element of a hidden layer has value . We compare  to a learned pattern , and the resulting activation is high if the pattern is present.However, this doesn‚Äôt work well:\nIf the image has a variable dimension , where  is the width,  is the height and  is the number of channels (e.g. 3 if the image is RGB).\nEven if the image is of fixed dimension, learning a weight of  parameters (where  is the number of hidden units) would be prohibitive.\nA pattern occurring in one location may not be recognized in another location. The MLP doesn‚Äôt exhibit translation invariance, because the weights are not shared across locations.\nTo solve these problems, we use convolutional neural nets (CNN), which replace matrix multiplications with convolutions.The basic idea is to chunk the image into overlapping patches and compare its patch with a set of small weight matrices or filters that represent parts of an object.Because the matrices‚Äô weights are small (usually  or ), the number of learned parameters is considerably reduced.And because we use convolution to perform template matching, the model will be translationally invariant."}},"/proba-ml/neural-networks-for-images/other-forms-of-convolution":{"title":"14.4 Other forms of convolution","data":{"1441-dilated-convolution#14.4.1 Dilated convolution":"By using striding and stacking many layers of convolution together, we enlarge the receptive field of each neuron.To cover the entire image with filters, we would need either many layers or a large kernel size with many parameters, which would be slow to compute.Dilated convolution increases the receptive field without increasing the number of parameters:in comparison, regular convolution uses","1442-transposed-convolution#14.4.2 Transposed convolution":"Transposed convolution to the opposite of convolution, producing a larger output from a smaller input.","1443-depthwise-separable-convolution#14.4.3 Depthwise separable convolution":"Standard convolution uses a filter of size , requiring a lot of data to learn and a lot of time to compute with.Depthwise separable convolution simplifies this operation by first applying 2d weight convolution to each input channel, then performing 1x1 convolution across channels:Compared to regular convolution, a 12x12x3 input with a 5x5x3x256 filter will be performed using:\nA 5x5x1x1 filter across space to get an 8x8x3 output\nA 1x1x3x256 filter across channels to get an 8x8x256 output\nSo, the output has the same size as before, with many fewer parameters.For this reason, separable convolution is often used in lightweight CNN like MobileNet and other edge devices."}},"/proba-ml/neural-networks-for-images/other-discriminative-vision-tasks":{"title":"14.5 Solving other discriminative vision tasks with CNNs","data":{"":"We briefly discuss other tasks using CNNs, each task introduces a new architectural innovation.","1451-image-tagging#14.5.1 Image tagging":"In many problems, we may want to label each object present in a single image. This is known as image tagging, an application of multi-label prediction.The output space becomes , where  is the number of tag types.Since the labels are independent, we should replace the final softmax with a set of  logistic units.Image tagging is often a more sensible objective than image classification since when images have many objects in them, it can be challenging to say which one we should be labeling.Indeed, Andrej Karpathy, who created the ‚Äúhuman performance benchmark‚Äù on ImageNet noted:\nBoth [CNNs] and humans struggle with images that contain multiple ImageNet classes (usually many more than five), with little indication of which object is the focus of the image. This error is only present in the classification setting, since every image is constrained to have exactly one correct label. In total, we attribute 16% of human errors to this category.","1452-object-detection#14.5.2 Object detection":"In object detection, we return a set of bounding boxes representing the locations of objects of interest, together with their class labels.Face detection is a special case with only one class of interest.The simplest way to tackle this problem is to convert it into a closed-world problem, in which there is a finite number of possible locations (and orientations) any object can be in.These candidate locations are known as anchor boxes. For each box, we train the model to predict what categories of object (if any) it contains.We can also perform regression to predict the offset of the object from the anchor center (allowing subgrid spatial location).We learn a function of the form:where  is the number of channels,  is the number of anchor boxes in each dimension and  is the number of class labels.For each box location  we predict 3 outputs:\nan object presence probability \nan object categories \ntwo 2d offset vectors, , that can be added to the anchor center to obtain the top left and bottom right corners\nMany models of this type have been introduced, including the **single shot detector (SSD)** model and the **YOLO (you only look once)** model.See this review of object detection models.","1453-instance-segmentation#14.5.3 Instance segmentation":"In image segmentation, the goal is to predict the label and the 2d mask of each object.This can be done by applying a semantic segmentation model to each box from the image detection algorithm, classifying each pixel as object or background.","1454-semantic-segmentation#14.5.4 Semantic segmentation":"In semantic segmentation, we have to predict a class  for each pixel.In contrast to instance segmentation above, all car pixels get the same labels, so semantic segmentation does not differentiate between objects.We can combine semantic segmentation of ‚Äústuff‚Äù (sky, road) and instance segmentation of ‚Äúthings‚Äú (car, people) into a framework called panoptique segmentation.A common way to tackle semantic segmentation is by using an encoder-decoder architecture.\nThe encoder applies standard convolution to map inputs to a 2d bottleneck of high level properties with coarse spatial resolution (this can used dilated convolution to capture a larger input field)\nThe decoder map the 2d bottleneck back to a full-sized image using transposed convolution.Since the bottleneck loses information, we add skip connections between encoder and decoder layers of the same level.\nWith this shape, this model is known as U-net.Similar encoder-decoder architecture can be used for other image-to-image tasks, like:\ndepth-estimation: predict the distance from camera  for each pixel\nsurface normal prediction: predict the orientation of the surface , at each image patch\nWe can train a model to do all these tasks simultaneously, using multiple output heads.","1455-human-pose-estimation#14.5.5 Human pose estimation":"We can train an object detector to detect people and predict their 2d shape, but we can also train the model to predict the location of a fixed set of skeletal keypoints (e.g. the location of the hands and neck).This is called human pose estimation, with techniques such as PersonLab and OpenPose.See this 2019 review.We can also predict 3d properties of each object, by training models on synthetic 3d image datasets generated by computer graphics. See **DensePose.**"}},"/proba-ml/neural-networks-for-sequences/1d-CNNs":{"title":"15.3 1d CNNs","data":{"":"CNNs are usually for 2d inputs, be can alternatively be applied to 1d sequences.1d CNNs are an interesting alternative to RNNs because they are easier to train, since they don‚Äôt need to maintain long term hidden state.","1531-1d-cnns-for-sequence-classification#15.3.1 1d CNNs for sequence classification":"In this section, we learn the mapping , where  is the length of the input,  the number of feature per input, and  the size of the output vector.We begin with a lookup of each of the word embeddings to create the matrix .We can then create a vector representation for each channel with:This implements a mapping from  to .We then reduce this to a vector  using a max-pooling over time:Finally, a fully connected layer with a softmax gives the prediction distribution over the  label classes.","1532-causal-1d-cnns-for-sequence-generation#15.3.2 Causal 1d CNNs for sequence generation":"To use 1d CNNs in generative context, we must convert them into causal CNNs, in which each output variable only depends on previously generated variables (this is also called convolutional Markov model).We define the model as:where  is the convolutional filter of size .This is like regular 1d convolution except we masked future inputs, so that  only depends on past observations. This is called causal convolution.We can use deeper model and we can condition on input features .In order to capture long-range dependencies, we use dilated convolution.Wavenet perform text to speech (TTS) synthesis by stacking 10 causal 1d convolutional layer with dilation rates 1, 2, 4‚Ä¶, 512. This creates a convolutional block with an effective receptive field of 1024.They left-padded each input sequence with a number of zeros equal to the dilation factor, so that dimensions are invariant.In wavenet, the conditioning information  is a set of linguistic features derived from an input sequence of words; but this paper shows that it‚Äôs also possible to create a fully end-to-end approach, starting with raw words rather than linguistics features.Although wavenet produces high quality speech, it is too slow to run in production. Parallel wavenet distils it into a parallel generative model."}},"/proba-ml/logistic-regression/multinomial-logistic-regression":{"title":"10.3 Multinomial logistic regression","data":{"":"In multinomial (or multiclass) logistic regression, the labels are mutually exclusive. This has the following form:with:\n is the input vector\n the weight matrix, if we ignore the bias term , Because of the normalization condition, we can set , so that \n the softmax function\nThis can be written as:In multi-label logistic regression, we want to predict several labels for a given example.This can be viewed as , with the form:","1031-linear-and-non-linear-boundaries#10.3.1 Linear and non-linear boundaries":"Logistic regression computes linear decision boundaries in the input space.In this example we have  classes and .We can transform the inputs to create non-linear boundaries (here quadratic):","1032-maximum-likelihood-estimation#10.3.2 Maximum Likelihood Estimation":"10.3.1.1 ObjectiveThe NLL is given by:where:\n is the one-hot encoding of the label, meaning \nTo find the optimum, we need to solve , where  is a vectorized version of . We derive the gradient and Hessian and prove that the objective is convex.10.3.1.3 Deriving the gradientWe derive the Jacobian of the softmax function:If we have 3 classes, the Jacobian matrix is:In the matrix form, this can be written:Let‚Äôs now derive the gradient of the NLL w.r.t , the vector of weights associated to the class :We can repeat this operation for all classes and sum all examples, to get the  matrix:10.3.1.4 Deriving the HessianWe can show that the Hessian of the NLL of a single example is:Where  is the Kronecker product.The block  submatrix is given by:","1033-gradient-based-optimization#10.3.3 Gradient-based optimization":"Based on the binary results, using the gradient to perform SGD is straightforward. Computing the Hessian is expensive, so one would prefer approximating it with quasi-Newton methods like the limited memory BFGS.Another similar method to IRLS is Bound optimization.","1034-bound-optimization#10.3.4 Bound optimization":"If  is a concave function we want to maximize, we can obtain a valid lower bound by bounding its Hessian i.e. find  such that .We can show that:The update becomes:This is similar to a Newton update, except we use a fixed matrix . This gives us some of the advantages of the second-order methods at lower computational cost.We have seen that the Hessian can be written as:In the binary case:The update becomes:Compared to the IRLS:where Thus we see that this lower bound is faster since  can be precomputed.","1035-map-estimation#10.3.5 MAP estimation":"The benefits of  regularization hold in the multi-class case, with additional benefits about parameter identifiability. We say that the parameters are identifiable iff there is a unique value that maximizes the likelihood, i.e. the NLL is strictly convex.We can arbitrarily ‚Äúclamp‚Äù , say for example for the class  we define The parameters will be identifiable.If we don‚Äôt clamp  but add a  regularization, the parameters will still be identifiable.At the optimum, we can show that we have  for , therefore the weights automatically satisfy a sum-to-zero constraint, making them uniquely identifiable.","1037-hierarchical-classification#10.3.7 Hierarchical classification":"When the predicted labels can be structured as a taxonomy, it makes sense to perform hierarchical classification.We define a model with a binary target for all of its leaves. Then we apply label smearing (we activate the label of a parent when a child is activated).If we train a multi-label classifier, it will perform hierarchical classification.However, since the model doesn‚Äôt capture that some labels are mutually exclusive, we add exclusion constraints to labels that are siblings. For example, we enforce:since these 2 labels are children of the root node. We can further the partition as:","1038-handling-large-number-of-classes#10.3.8 Handling large number of classes":"In this section, we discuss the issues that arise when there is a large number of potential classes, like in NLP.10.3.8.1 Hierarchical softmaxIn regular softmax, computing the normalizing term can become the bottleneck since it has a complexity of .However, by structuring the labels as a tree, we can compute any labels in  times, by multiplying the probabilities of each edge from the root to the leaf.A good way to structure it is to use Huffman encoding, with the most frequent labels on top.10.3.8.2 Class Imbalance and the long tailAnother issue is having very few examples for most labels, resulting in extreme class imbalance and the model focusing only on the most common labels.i) One method to mitigate this is to set the bias term such as:so that the model will match the empirical prior even if . As the model adjusts its weights, it learns input-dependent deviations from the prior.ii) Another popular approach is resampling to make it more balance during training. In particular, we sample a point from class  with probability:If , we recover the instance-balanced sampling, where the common classes will be sampled more often.If , we recover class-balanced sampling, where we sample a class uniformly at random, and then sample an instance of this class.We can consider other options, like , called the square-root samplingiii) We can also consider the nearest class mean classifier:where If we replace  with learned features , like after a Neural Net trained using the cross-entropy loss on the original unbalanced dataset, we can get very good performance on the long tail."}},"/proba-ml/neural-networks-for-sequences/efficient-transformers":{"title":"15.6 Efficient transformers","data":{"":"Regular transformers takes  in time and space complexity, for a sequence of length , which makes them impracticable for long sequences.To bypass this issue, various efficient variants have been proposed.","1561-fixed-non-learnable-localized-attention-patterns#15.6.1 Fixed non-learnable localized attention patterns":"The simplest modification of the attention mechanism is to restrain it to a fixed non-learnable localized window.We chunk a sentence into  blocks of size , and attention is performed only in each blocks. This gives a complexity of .If  this can give a substantial computational improvement.Other approaches leverage strided/dilated windows, or hybrid patterns, where several fixed attention patterns are combined together.","1562-learnable-sparse-attention-patterns#15.6.2 Learnable sparse attention patterns":"A natural extension of the above is to use learnable attention patterns, with the attention still restricted to pairs of token within a single partition. We distinguish the hashing and clustering approaches.In the hashing approach, all tokens are hashed and the partitions correspond to the hashing-buckets. This is how the Reformer uses locality sensitive hashing (LSH), with a complexity of  where  is the dimension of tokens‚Äô embeddings.This approach requires to set of queries to be identical to the set of keys, and the number of hashes used for partitioning can be a large constant.In the clustering approach, tokens are clustered using standard algorithms like K-Means. This is known as the clustering transformer. As in the block-case, if  equal sized blocks are used, the complexity of the attention module is reduced to .In practice, we choose , yet imposing the clusters to be similar in size is difficult.","1563-memory-and-recurrence-methods#15.6.3 Memory and recurrence methods":"In some approaches, a side memory module can be used to access several tokens simultaneously.  This method has the form of a global memory algorithm.Another approach is to connect different local blocks via recurrence. This is used by Transformer-XL methods.","1564-low-rank-and-kernel-methods#15.6.4 Low-rank and kernel methods":"The attention matrix  can be directly approximated by a low rank matrix, so that:where  with .We can then exploit this structure to compute  in  time. Unfortunately, for softmax attention,  is not low rank.In Linformer, they transform the keys and values via random Gaussian projection. Then, they approximate the softmax attention in this lower dimensional space using the Johnson-Lindenstrauss Transform.In **Performer,** they show that the attention matrix can be computed using a (positive definite) kernel function:The first term is equal to , where:and the two other terms are independent scaling factors.To gain a computational advantage, we can show that the Gaussian kernel can be written as the expectation of a set of random features:where  is a random feature vector derived from , either from trigonometric or exponential functions (the latter ensure positivity of all features, which gives better results).Therefore we can write:where:And we can write the full attention matrix as:where  have rows encoding random features corresponding to queries and keys. We can get better results by ensuring these random features are orthogonal.We can create an approximation of  by sampling a single vector of the random features  and , using a small value .We can then approximate the entire attention operation with:This is an unbiased estimate of the softmax attention operator."}},"/proba-ml/neural-networks-for-sequences/attention":{"title":"15.4 Attention","data":{"":"So far, we have only considered hidden activations as , where  is a fixed set of learnable weights.However, we can imagine a more flexible model where we have a set of  feature vectors , and the model dynamically decides which feature to use, based on the similarity between an input query  and a set of keys .If  is most similar to , we use . This is the basic idea behind attention mechanisms.Initially developed for sequence models, they are now used in a broader set of tasks.","1541-attention-as-a-soft-dictionary-lookup#15.4.1 Attention as a soft dictionary lookup":"To make this lookup operation differentiable, instead of retrieving a single value , we compute a convex combination as follow:where  is the ith attention weight, and , , computed as:with the attention score function .In some cases, we want to restrict attention to a a subset of the dictionary. For example, we might pad sequences to a fix length (for efficient mini-batching), and ‚Äúmask out‚Äù the padded locations. This is called masked attention.We implement this by setting the attention score to large negative number like , so that the exponential output will be zero (this is analogous to causal convolution).","1542-kernel-regression-as-non-parametric-attention#15.4.2 Kernel regression as non-parametric attention":"Kernel regression is a nonparametric model of the form:The similarity  is computed by using a density kernel in the attention score:where  is called the bandwidth. We then define .Since the score are normalized, we can drop the  term and write it:Plugging this into the first equation gives us:We can interpret this as a form of nonparametric attention, where the queries are the test points , the keys are the training points   and the values are the training labels .If we set , we obtain the attention matrix  for test input :The size of the diagonal band of figure 15.17a narrows when  augment, but the model will start to overfit.","1543-parametric-attention#15.4.3 Parametric attention":"Comparing a scalar query (test point) to each of the scalar values in the training set doesn‚Äôt scale well to large training sets, or high-dimensional inputs.In parametric models, we have a fixed set of keys and values, and we compare keys and queries in a learned embedding space.One general way to do it is assuming  and  might not have the same size, so we compare them by mapping them in a common embedding space of size .This gives us the following additional attention function:with , A more computationally efficient approach is to assume the keys and queries both have the same size .If we assume these to be independent random variables with mean 0 and unit variance, the mean of their inner product is 0 and their variance is , since .To ensure the variance of the inner product stays 1 regardless of the size of the inputs, we divide it by .We can define the scaled dot-product attention:In practice, we deal with minibatch of  vectors at a time. The attention weighted output is:with  and the softmax function applied row-wise.","1544-seq2seq-with-attention#15.4.4 Seq2Seq with attention":"In the seq2seq model from section 15.2.3, we used a decoder in the form:where  represents the fixed-length encoding of the input . We usually set , the final state of the encoder (or average pooling for bidirectional RNN).However, for tasks like machine translation, this can result in poor predictions, since the decoder doesn‚Äôt have access to the input words themselves.We can avoid this bottleneck by allowing the output words to ‚Äúlook at‚Äù the input words. But which input should them look at, since each language has its own word order logic?We can solve this problem in a differentiable way by using soft attention, as proposed by this paper. In particular, we can replace the fix context vector  in the decoder with a dynamic context vector  computed as:where the query is the hidden state of the decoder at the previous step, and both the keys and values are all the hidden state of the encoder. When the RNN has multiple hidden layers, we take the one at the top).We then obtain the next hidden state as:We can train this model the usual way on sentence pairs.We can observe the attention weights computed at each step of decoding to determine which input words are used to generate the corresponding output.Each output word  was sampled from  (query, vertical axis), and each line represent the attention of this decoder hidden state with all the encoder hidden states (keys, horizontal axis).","1545-seq2vec-with-attention-text-classification#15.4.5 Seq2Vec with attention (text classification)":"We can also use attention with sequence classifiers. For example, this paper applies an RNN classifier to predict the death of patients. It uses a set of electronic health record as input, which is a time series containing structured data as well as unstructured text (clinical note).Attention is useful for identifying the ‚Äúrelevant‚Äù parts of the inputs.","1546-seqseq2vec-with-attention-text-pair-classification#15.4.6 Seq+Seq2Vec with attention (text pair classification)":"Our task is now to predict whether two sentences (premises and hypothesis) are in agreement (premises entails the hypothesis), in contradiction or neutral.This is called textual entailment or natural language inference. A standard benchmark is Stanford Natural Language Inference (SNLI) corpus, consisting in 550,000 labeled sentence pairs.Below is a solution to this classification problem presented by this paper.Let  be the premise and  by the hypothesis, with  the words embedding vectors.First, each word in the premise attends to each word in the hypothesis, to compute an attention weight:where  is an MLP.We then compute a weighted average of the matching words in the hypothesis:We compare  and  by mapping their concatenation to a hidden space using an MLP :Finally, we aggregate over the comparison to get an overall similarity of premise to hypothesis:We can similarly get an overall similarity of hypothesis to premises by computing:At the end, we classify the output using another MLP :We can modify this model to learn other kinds of mappings from sentence pairs to label. For instance, in the semantic textual similarity task, we predict how semantically related two inputs are.","1547-soft-vs-hard-attention#15.4.7 Soft vs hard attention":"If we force the attention heatmap to be sparse, so that each output only attends to a single input location instead of a weighted combination of all of them, we perform hard attention.We compare these two approaches in image captioning:Unfortunately, hard attention is a nondifferentiable training objective, and requires methods such as reinforcement learning to fit the model.It seems that attention heatmaps can explain why a model generates a given output, but their interpretability is controversial.See these papers for discussion:Is attention interpretable?On identifiability of transformersAttention is not explanationAttention is not not explanation"}},"/proba-ml/neural-networks-for-sequences/language-models":{"title":"15.7 Language models and unsupervised representation learning","data":{"":"We have discussed how RNN and transformer (decoder only) can be used as language models, which are generative sequence models of the form:where each  is a discrete token, such as a word or wordpiece.The latent state of these models can be used as a continuous vector representation of the text. Instead of using the one-hot vector  or a learned embedding of it, we use the hidden state , which depend on all the previous words of the sentence.The advantage of this approach is that we can pre-train the language model on large corpus in a unsupervised way, before fine-tuning it on a smaller, task-specific dataset in supervised way (this is called transfer learning).","1571-embeddings-for-language-models-elmo#15.7.1 Embeddings for Language Models (ELMo)":"The basic idea of ‚ÄúEmbeddings for Language Models‚Äù is to fit two RNNs, one left-to-right and one right-to-left, before combining their hidden state to come up with an embedding for each word.Unlike a biRNN which needs input-output pair, ELMo is trained in a unsupervised way, to minimize the NLL of the input sequence:where  and  are the share parameters of the embedding and softmax output layers, and  are the parameters of the RNNs.After training, we define the contextual representation , where  represents the number of layers in the LSTM.We then learn a task-specific set of linear weights to get the final context-specific embedding of each token:where  is the task id.If we are performing a syntactic task like part-of-speech (POS) tagging (i.e. labeling each word as a noun, adjective, verb), then the task will put more weight on the lower layers.If we are performing a semantic task, like word sense disambiguation (WSD), then the task will learn to put more weight on the higher layers.In both case, we only need a small amount of task-specific labeled data, since we are just learning a single weight vector to map from  to the target labels .","1572-bidirectional-embeddings-representations-from-transformer-bert#15.7.2 Bidirectional Embeddings Representations from Transformer (BERT)":"Like ELMo, BERT is a non-causal model that can create representation of text but can‚Äôt generate text. It has a very close architecture than the first version of GPT while outperforming it on major downstream supervised tasks.This is due to a better pre-training that leverages a fill-in-the-blank (or cloze) task and a next sentence prediction (NSP) task.15.7.2.1 Masked language model taskThe model is training on the negative log pseudo-likelihood, computed at the masked locations:where  is the random mask, indicating the tokens to replace by the special [MASK] token.A training sentence has the form:\nLet‚Äôs make [MASK] chicken! [SEP] It [MASK] great with orange sauce.\nwhere [SEP] is a special token inserted between two sentences. The desired target for this example is ‚Äúsome‚Äù and ‚Äútastes‚Äù.The conditional probability is obtained by applying a softmax to the final hidden vector:where  is the masked input sentence and  is the embedding of token .This is similar to a denoising autoencoder.15.7.2.2 Next sentence prediction taskIn addition to the masked language task, BERT also trained on NSP, in which the model is trained to classify whether a sentence follows another.The input has the form:\nCLS A1 A2 ... Am SEP B1 B2 ... Bn SEP\nwhere [CLS] is a special token marking the class:  if sentence B follows sentence A, and  otherwise.This pre-training objective can be useful for sentence-pair classification tasks, such as textual entailment or textual similarity.Note that this pre-training is considered unsupervised or self-supervised, since the label can be generated automatically, by selecting the following or random sentences.When performing NSP, the input tokens go through three different embedding layers: one per token, one for each segment label (sentence A or B), and one for each location (using learned positional embeddings).These are then added.BERT then uses a transformer encoder to learn a mapping from this input embedding sequence to an output embedding sequence, which gets decoded into word labels or a class label.15.7.2.3 Fine-tuning BERT for NLP applicationsAfter pre-training, we can use BERT for various downstream tasks by performing supervised fine-tuning.We can perform:a) Single sentence classification by feeding the feature vector associated to the [CLS] token to a MLP, which maps it to the desired label space.Since each output attends to all inputs, the [CLS] hidden vector will summarize the entire sentence.b) Sentence pair classification by feeding two input sentences, then classify the [CLS] tokenc) Single sentence tagging, in which we map a label to each word instead of the entire sentence.In part of speech (POS), we annotate each word as a verb, noun, adjective.Another application is noun phrase chunking in which we annotate the span of each noun phrase. The span is encoded using the BIO notation: B is beginning, I-x for inside and O for outside of any entity.We can associate types with each noun phrase to distinguish person, location, organization and other. The label space becomes {B-Per, I-Per, B-Loc, I-Loc, B-Org, I-Org, Outside}.This is called named entity recognition (NER) and is key to information extraction.d) Question answering, where the first input sentence is the question, the second is the background text, and the output specifies the start and end positions of the answer in the background text.The start location  and end location  are obtained by applying two different MLPs to a pooled version of the encoding output of the background tokens.The output of the MLPs is a softmax over locations and at test time we extract the span  that maximizes the sum of scores , for .BERT achieves state-of-the-art performance on many NLP tasks. It has been shown that it rediscover the standard NLP pipeline, in which different layers perform tasks like POS tagging, parsing, NER, detection, semantic role labeling (SRL) or coreference resolution.See this jax tutorial.","1573-generative-pre-training-transformer-gpt#15.7.3 Generative Pre-training Transformer (GPT)":"GPT is a causal generative model, that uses a masked transformer as the decoder.In the original GPT paper, they jointly optimize on a large unlabeled dataset and a small labeled dataset:where:is the classification loss of the labeled data and:is the language model loss of the unlabeled data.GPT-2 is a larger version of GPT, trained on a larger web corpus called WebText. They also eliminated any task specific training, and only train it as a language model.More recently, OpenAI proposed GPT-3, which is an even larger version but based on the same principles. It was trained on an 800GB English-language web corpus called The Pile.15.7.3.1 Application of GPTGPT can generate text given an initial input prompt. The prompt can specify a task, if the model fulfilled the task out of the box, we say if performed zero-shot task transfer.For example, to perform abstractive summarization to some input text  (in contrast to extractive summarization which just select a subsample of words), we sample from:where  is a special token added to end of the input text, with which the user hopes to trigger the transformer decoder into a state of summarization mode.A better way to tell the model what task to perform is to train it on input-output pairs, as we discuss below.","1574-text-to-text-transfer-transformer-t5#15.7.4 Text-to-Text Transfer Transformer (T5)":"Many models (like BERT) are trained in an unsupervised way and fine-tuned on a specific tasks.T5 proposes to train a single model to perform multiple tasks, by telling it which task to perform as part of the input sentence and train it as a seq2seq transformer model.The model is pre-trained on  where  is a masked version of the input  and  are the missing tokens to be predicted. We then fine-tune it on multiple supervised  pairs.The unsupervised dataset is C4 (Colossal Clean Crawled Corpus), a 750GB corpus of web text. This is used to train a BERT-like denoising objective. For example:\nx' = \"Thank you <X> me to your party <Y> week\"\nx'' = \"<X> for inviting <Y>last <EOS>\"\nWhere  and  are tokens that are unique to this example.The supervised datasets are manually created and taken from the literature.","1575-discussion#15.7.5 Discussion":"Models like BERT and GPT-3 have gained a lot of interest, but some doubt remains about there capacity to ‚Äúunderstand‚Äù a language in a meaningful way, beyond rearranging word patterns seen in their massive datasets.For this example this article showed that the human-level performances of BERT in the ‚ÄúArgument Reasoning Comprehension Task‚Äù exploits spurious statistical cues in the dataset.By tweaking slightly the dataset, performances can be reduced to chance-level.For other criticisms of the model, see this and this article."}},"/proba-ml/neural-networks-for-sequences/transformers":{"title":"15.5 Transformers","data":{"":"The transformer model uses attention in both the encoder and decoder, thus eliminating the need for RNN.It has been used in a wide diversity of sequence generation tasks, like machine translation music generation, protein sequence generation and image generation (treating images as a rasterized 1d sequence).","1551-self-attention#15.5.1 Self-attention":"We saw in section 15.4.4 how the decoder section of an RNN could attend to the encoder to capture contextual embedding of each input.We can modify this architecture so the encoder attends to itself. This is called self-attention.Given a sequence of token , with , self-attention can generate output of the same size using:where the query is  and keys and values are all the inputs .To use this in a decoder, we can set , and , so that all previously generated outputs are available. At training time, all the outputs are already known, so we run self-attention in parallel, overcoming sequential bottleneck of RNNs.In addition to improve speed, self attention can give improved representation of context. For instance translating into french the sentences:\n‚ÄúThe animal didn‚Äôt cross the street because it was too tired‚Äù\n‚ÄúThe animal didn‚Äôt cross the street because it was too wide‚Äù\nThis phrase is ambiguous because ‚Äúit‚Äù can refer to the animal or the street, depending on the final adjective. This is called coreference resolution.Self attention is able to resolve this.","1552-multi-headed-attention#15.5.2 Multi-headed attention":"If we think about attention matrix like a kernel matrix, we naturally want to use multiple attention matrix to capture different notion of similarity. This is the basic idea behind multi-headed attention (MHA).Given a query , keys and values , , we define the th attention head as:where ,   and  are projection matrices.We then stack the  heads together, and project to :with .If we set , we can compute all heads in parallel.See this code snippet.","1553-positional-encoding#15.5.3 Positional encoding":"Vanilla self-attention is permutation invariant, hence ignores the input word ordering. Since this can lead to poor results, we can concatenate or add positional encodings to the word embeddings.We can represent positional encodings as the matrix , where  is the sequence length and  is the embedding size.The original Transformer paper suggests to use sinusoidal basis:where  corresponds to some maximum sequence length.For , we have:Below, we see that the leftmost columns toggle fastest. Each row has a blueprint representing its position in the sequence.This representation has two advantages:\nIt can be computed for arbitrary sequence size , unlike a learned mapping from integers to vectors.\nThe representation of one location is linearly predictable from any other: , where  is a linear transformation.\nTo see this last point, note that:If  is small, then . This provide a useful form of inductive bias.Once we have computed the position embeddings , we need to combine them with the word embeddings :We could also concatenate both matrix, but adding takes less space.Additionally, since the  embeddings are learned, the model could simulate concatenation by zeroing the first  dimensions of  and the last  dimensions of .","1554-putting-it-all-together#15.5.4 Putting it all together":"A transformer is a seq2seq model using self-attention in the encoder and decoder rather than an RNN.The encoder uses positional encoding, followed by a series of  encoder blocks, each of which uses multi-head self-attention, residual connections and layer normalization.\ndef EncoderBlock(X):\n    Z = LayerNorm(MultiHeadAttn(Q=X, K=X, V=X) + X)\n\t\tE = LayerNorm(FeedForward(Z) + Z)\n\t\treturn E\ndef Encoder(X, N):\n    E = POS(Embed(X))\n    for n in range(N):\n        E = EncoderBlock(E)\n    return E\nThe decoder has a more complex structure.The previous generated outputs  are shifted and then combined with a positional embedding.Then, they are fed to a causal (masked) multi-head attention model, before combining the encoder embeddings in another MHA.Finally, the probability distribution over tokens are computed in parallel.\ndef DecoderBlock(Y, E):\n    Z_1 = LayerNorm(MultiHeadAttn(Q=Y, K=Y, V=Y) + Y)\n    Z_2 = LayerNorm(MultiHeadAttn(Q=Z_1, K=E, V=E) + Z_1)\n    D = LayerNorm(FeedForward(Z_2) + Z_2)\n    return D\ndef Decoder(Y, E, N):\n    D = POS(Embed(Y))\n    for n in range(N):\n        D = DecoderBlock(D, E)\n    return D\nSee this notebook for a tutorial.Note that:i) During training, teacher forcing is applied by using masked softmax. It processes all tokens of a sentence in a single pass, instead of looping for each one.During inference however, we use a for-loop on the num_steps. If we consider a single sentence (), the initial decoder input is only [[‚Äù<bos>‚Äù]] (beginning of sequence):Then, we take as input for the next loop the maximum of the output probabilities. Hence, the input  stays a single token across loops.However, state persists the input  of the previous loops for each decoder layer, by concatenating it with the new input, resulting in .In consequence, the first attention query is , but the key and values are the .ii) The word embeddings of the source (resp. target) language are located in the embedding layer of the encoder (resp. decoder).","1555-comparing-transformers-cnns-and-rnns#15.5.5 Comparing transformers CNNs and RNNs":"We visually compare three different architectures to map a sequence  to another sequence  :For a 1d CNN with kernel size  and and  feature channels, the time to compute is , which can be done in parallel. We need a stack of  layers (or  if we use dilated convolution, to ensure all pairs of inputs communicate.For a RNN, the computational complexity is , because for a hidden state of size  we have to perform matrix-vector multiplication at each step.Finally, for self-attention models, every output is directly connected to every input. However, the computational cost is , which is fine for short sequence where . For longer sequence, we need fast versions of attention, called efficient transformers.","1556-transformers-for-images#15.5.6 Transformers for images":"CNNs are the most common model type for processing image data, since they have useful built-in inductive bias, locality (due to small kernel), equivariance (due to weight tying) and invariance (due to pooling).Surprisingly, transformers can also perform well at image classification, but they need a lot of data to compensate for the lack of relevant inductive bias.The first model of this kind is ViT (vision transformer), which chop images into 16x16 patches, project each patch into an embedding space, and passes these patches as a  sequence to a transformer.The input is also prepended with a special [CLASS] embedding, . The output of the encoder is a set of encodings , the model maps  to the target class , and is trained in a supervised way.After supervised training, the model is fine-tuned on various downstream classification tasks, an approach known as transfer learning.When trained on a ‚Äúsmall dataset‚Äù like ImageNet (1k classes, 1.3m images), ViT can‚Äôt outperform a pretrained ResNet model known as BiT (Big transfer).However, when trained on a bigger dataset, like ImageNet-21k (21k classes, 14m images) or the Google-internal JFT dataset (18k classes, 303m images), ViT outperforms BiT at transfer learning, and matches ConvNext performances.ViT is also cheaper to train than ResNet at this scale (however, training is still expensive, the large ViT model on ImageNet-21k takes 30 days on a Google Cloud TPUv3 with 8 cores).","1557-other-transformer-variants#15.5.7 Other transformer variants":"Many extensions of the transformer have been proposed.For example, Gshard scales up transformers to even more parameters by replacing some of the feed forward dense layer with a mixture of experts regression module. This allows for sparse conditional computation, where a subset of the model (chosen by a gate) is used.Conformer adds convolutional layer inside the transformer, which is helpful for various speech recognition tasks."}},"/proba-ml/neural-networks-for-tabular-data/backpropagation":{"title":"13.3 Backpropagation","data":{"":"In this section, we‚Äôll cover the famous backpropagation algorithm which can be used to compute the gradient of a loss function applied to the output of the network w.r.t to the parameters in each layer.","1331-forward-vs-reverse-mode-differentiation#13.3.1 Forward vs reverse mode differentiation":"Consider a mapping of the form , where  andWe can compute the Jacobian  using the chain rule:Recall that the Jacobian can be written as: means the output is a scalar.If  (i.e. the number of features in the input is smaller than the dimension of the output), it is more efficient to compute the Jacobian for each column of  using a Jacobian vector product (JVP), in a right to left manner:This can be computed using forward mode differentiation. Assuming  and , the cost of computation is .If  (this is the most frequent scenario, e.g. if the output is a scalar), it is more efficient to compute the Jacobian for each row using a vector Jacobian product (VJP), in a left to right manner:This can be done using reverse mode differentiation. Assuming  and , the cost of computation is .Both algorithms 5 and 6 can be adapted to compute JVPs and VJPs of any collection of input vectors by accepting  or  as input.Initializing these vectors to the standard basis is only useful to produce the complete Jacobian as output.","1332-reverse-mode-differentiation-for-mlp#13.3.2 Reverse mode differentiation for MLP":"Let‚Äôs now consider layers with learnable parameters. We consider the example of a scalar output, so that the mapping has the form .For example, consider a   loss for a MLP with one hidden layer:We can represent this as the following feedforward model:with:We can compute the gradient wrt the parameters in each layer using the chain rule:where each  is a -dimensional gradient row vector,  being the number of parameters of the layer .Each Jacobian  is an  matrix and can be computed recursively:We now have to define the VJP  of each layer.","1333-vjp-of-common-layers#13.3.3 VJP of common layers":"13.3.3.1 Cross entropy layerwhere  is the predicted probability for class :and  is the true label for class .The Jacobian is:13.3.3.2 Element wise non-linearitySince it is element wise, we have The  element of the Jacobian is given by:In other words, the Jacobian wrt the input is:The VJP becomes For example, if , we have:The subderivative at 0 is any value in , but we often take .13.3.3.3 Linear layerwhere i) The Jacobian wrt the input is:since:Then the VJP is:ii) The Jacobian wrt the parameters is:which is complex to deal with.Instead, taking a single weight:Hence:where  occurs on the th location.Therefore we have:So the VJP can be represented by a matrix:13.3.3.4 Putting all togetherAnd","1334-computation-graph#13.3.4 Computation Graph":"MLP are simple kind of DNN with only feed forward passes. More complex structures can be seen as DAGs or computation graphs of differentiable elements.For example, here we have:The  is computed once for each child , this quantity is called the adjoint. This then gets multiplied by the Jacobian  of each child.The graph can be computed ahead of time, by using a static graph (this is how Tensorflow 1 worked), or it can be computed just in time by tracing the execution of the function on a input argument (this is how Tensorflow eager mode, Jax, and PyTorch works).The later approach makes it easier to work on a dynamic graph, where the paths can change wrt the inputs."}},"/proba-ml/logistic-regression/robust-logistic-regression":{"title":"10.4 Robust Logistic Regression","data":{"":"When we have outliers in our data, due to label noise, robust logistic regression help avoid adversarial effects on the model.","1041-mixture-model-for-the-likelihood#10.4.1 Mixture model for the likelihood":"One of the simplest ways to achieve robust logistic regression is to use a mixture likelihood:This predicts that each label is generated uniformly at random with a probability , and otherwise is generated using the regular conditional model.This approach can also be applied to DNN and can be fit using standard methods like SGD or Bayesian inference methods like MCMC.","1042-bi-tempered-loss#10.4.2 Bi-tempered loss":"Examples far from the decision boundary but mislabeled will have undue adverse effects on the model if the loss is convex.Tempered lossThis can be overcome by replacing the cross entropy loss with a ‚Äútempered‚Äù version, using a temperature parameter  to ensure the loss from outliers is bounded.The standard cross-entropy loss is:The tempered cross-entropy is:when all the mass of  is on  (one-hot encoding) this simplifies to:Here   is the tempered log:which is monotonically increasing and concave, and reduces to the standard logarithm when  is 1.This is also bounded below by  for , therefore the tempered cross-entropy is bounded above.Transfer functionObservation near the decision boundary but mislabeled needs to use a transfer function  with a heavier tail than the softmax.The standard softmax is:The tempered softmax, with  is:where:when , we find back the standard softmax.Finally, we need to compute , this needs to satisfy:Combining the tempered loss with the tempered transfer function is bi-tempered logistic regression."}},"/proba-ml/neural-networks-for-tabular-data/intro":{"title":"13.1 Introduction","data":{"":"Linear models make the strong assumption of linear relationships between inputs and outputs.A simple way of increasing the flexibility of linear models is to perform feature transformation by replacing  by . For example, polynomial extension in 1d uses The model now becomes:This is still linear in the parameters , which makes the fitting easy since the NLL is convex, but specifying the feature transform manually is very limiting.A natural solution is to endow the feature extractor with its own parameters:where .We can repeat this process recursively to create more complex patterns:where  is the function at level .Deep neural networks (DNN) encompass a large family of models in which we compose differentiable functions in DAG (direct acyclic graph).The  layers above are the simplest example where the DAG is a chain: this is called Feed Foward Neural Net (FFNN) or multilayer perceptron (MLP).An MLP assumes the input is a fixed-dimensional vector , often called tabular data or structured data, since the data is stored into a  design matrix, in which each column has a specific meaning (age, length, etc).Other kinds of DNNs are more suited to unstructured data (text, image), where each element (pixel or word) is meaningless alone. Convolutional neural networks (CNN) are historically performant on images, transformers on sequences, and graph neural networks (GNN) on graphs."}},"/proba-ml/neural-networks-for-sequences/recurrent-neural-nets":{"title":"15.2 Recurrent neural networks (RNNs)","data":{"":"A RNN is a model mapping a sequence of inputs to a sequence of outputs in a stateful way.The prediction  depends on the input  but also on the hidden state of the model  that gets updated over time, as the sequence is processed.We can use these models for sequence generation, sequence classification and sequence translation.See this intro to RNNs by Karpathy.","1521-veq2sec-sequence-generation#15.2.1 Veq2Sec (sequence generation)":"We now discuss how to learn functions of the form , were  is the size of the input vector, and  is an arbitrary-length sequence of vectors, each of size .The output sequence  is generated one token at the time. At each time step, we sample  from the hidden state  and then ‚Äúfeed it back in‚Äù in the model to get  (which also depends on ).In this way, we define a conditional generative model of the form , which capture dependencies between output tokens.15.2.1.1 ModelsThe RNN corresponds to the following conditional generative model:where we define  as the initial hidden state distribution, often deterministic.Also, remember that  is a random variable from which we sample .The output distribution is given by:For categorical values:For numerical values:The hidden state is computed deterministically:where:Therefore  implictly depends on all past observations, as well the optional fix input .Thus an RNN overcomes the limitation of a standard Markov models, in that they can be unbounded in memory. This makes the RNN theoretically as powerful as a Turing machine.In practice, the length of the memory is determined by the size of the latent state and the strength of the parameters.When we generate from a RNN, we sample from  and feed it to compute  deterministically. The stochasticity only comes from this sampling.15.2.1.2 ApplicationsRNNs can be used to generate sequences unconditionally ( or conditionally on .Unconditional sequence generation is also called language modeling and aims at learning the joint probability distribution over sequences of discrete tokens, i.e. models of the form The result on the simple RNN below are not very plausible, but we can create better models with more data, measuring their performances using perplexity.We can also make the generated sequence depends on some input vector . In the example below,  is the embedding generated by a ConvNet on a image and the RNN is used to generate captions.See this Pytorch implementation of the model above.","1522-seq2vec-sequence-classification#15.2.2 Seq2Vec (sequence classification)":"We now predict fixed-length output with variable length sequences inputs. We learn a function of the form: .The simplest approach is to use the final state of the RNN as input to the classifier:We often can get better results if we let the hidden states depends on future and past context.We do so by using two RNNs: one recursively computing hidden states in the forward direction, and another one in the backward direction.This is called bi-directional RNN. We have:Then we define  to be the representation of the hidden states at time step , compounding past and future information.Finally, we average pool over these hidden states to get the final classifier:","1523-seq2seq-sequence-translation#15.2.3 Seq2Seq (sequence translation)":"In this section, we consider learning functions of the form .We consider two cases:\n, the input and output sequences have the same length (aligned)\n (unaligned)\n15.2.3.1 Aligned caseWe can think of the aligned case as a dense sequence labeling where we predict one label per location.This RNN corresponds to:where  is the initial state.We can get better results by letting the decoder looks into the past and future of , by using a bi-directional RNN.We can create more expressive models by stacking multiple hidden chains on top of each other.This corresponds to:The output is given by:15.2.3.2 Unaligned caseWe learn a mapping from one sequence of length  to another of length .We use an encoder to learn embeddings  using the last hidden state (or average pooling over time steps for biRNN).We then generate a sequence using a decoder This is the encoder-decoder architecture.Neural machine translation is an important application (as opposed to the older approach called statistical machine translation).","1524-teacher-forcing#15.2.4 Teacher forcing":"When training a language model, the likelihood of a sequence of words  is given by:In an RNN, we set the output  and the input .Note that we condition on the ground truth from the past , not the labels generated by the model.This is called teacher forcing since the teacher‚Äôs values are forced fed into the model at each time step (i.e.  is set to ).An issue of teacher forcing is that the model is only trained on correct inputs, so during testing it may not know what to do if it encounter an input sequence  generated from the previous steps that deviate from what it saw during training.The common solution is to use scheduled sampling. It starts with teacher forcing, then at some random time steps, feeds in samples from the model instead. The fraction this happens increases gradually.An alternative solution is using models where MLE works better (1d CNN or Transformers).","1525-backpropagation-through-time#15.2.5 Backpropagation through time":"We compute the MLE of the RNN by solving:To compute the MLE, we have to compute gradients of the loss w.r.t parameters. To do this, we unroll the computational graph, and then apply the backpropagation algorithm.We compute the following model:where   are the output logits and we dropped the bias term for notational simplicity.We assume  are the true target labels at time step , and define the loss as:We need to compute the derivative: . The latter term is easy, since it is local to each time step.However, the first two terms depend on the hidden state, thus require to work backwards in time.We simplify the notation by defining:where  is the flattened version of the weights  and  stacked together.We focus on computing , by chain rule we have:we expand the last terms as follow:If we expand it recursively, we get the following:However, this is takes  to compute overall.It is standard to truncate the sum to the most recent  terms. It is possible to adaptively choose a suitable truncation parameter , but in practice it is set to the length of the subsequence in the current minibatch.When using BPTT, we can train the model with batches of short sequences, created with non-overlapping windows from the original sequence.If the subsequences are not ordered by time steps, we need to reset the hidden state for each batch.","1526-vanishing-and-exploding-gradients#15.2.6 Vanishing and exploding gradients":"RNNs with enough hidden units can in principle remember inputs from long in the past.Unfortunately, the gradient can vanish or explode through time, since we recursively multiply by  at each time step forward, and by the Jacobian  backward.A simple heuristic is to use gradient clipping. More sophisticated methods attempt to control the spectral radius   of the forward mapping  as well as the backward mapping .The simplest way to control the spectral radius is to randomly initialize  so that  and then keep it fixed. We only learn, resulting in a convex optimization problem.This is called an echo state network (ESN). A similar approach called liquid state machine (LSM) uses binary-valued neurons (spikes) instead of continuous ones.A more generic term for both ESNs and LSMs is reservoir computing. Another approach to this problem is to use constrained optimization to ensure  stays orthogonal.An alternative to explicitly controlling the spectral radius is to modify the architecture of the RNN itself, to use additive rather than multiplicative weights, similar to residual nets. This significantly improves training stability.","1527-gating-and-long-term-memory#15.2.7 Gating and long term memory":"We now focus on solutions were we update the hidden state in an additive way.15.2.7.1 Gating recurrent units (GRU)We assume , where  is the batch size and  is the vocabulary size. Similarly,  where  is the number of hidden units.The reset gate and update gate are computed as:Note that each elements of  and  is in  because of the sigmoid.We then define a candidate next vector using:This combines the old memories that are not reset with the new inputs.If the entries of the reset gate  are close to 1, we get the regular RNN update.If these entries are close to 0, we don‚Äôt use memories and the update is the regular MLP.Once we have computed the new candidate state, we compute the actual new state:When an update gate value  is close to 1, we dismiss the new inputs and capture long term dependencies.15.2.7.2 Long short term memory (LSTM)LSTM is a more sophisticated model than GRU, even if it pre-dates it by almost 20 years.The main idea is to augment the state  with a memory cell .We need three gates to control this cell:\nthe output  determines what gets read out\nthe input  determines what gets read in\nthe forget  determines when we should reset the cell\nWe compute them as:We then compute a new candidate cell:The actual update to the cell is a combination between the old cell or the candidate cell:When  and , this can remember long terms dependencies.Finally, we compute the hidden state as a transformed version of the cell, provided the output gate is on:Note that  plays both the role of the output of the cell and the hidden state of the next step. This lets the model remember what it has just output (short term), whereas  deals with the long term memory.Many variants of the LSTM have been introduced, but this architecture has good performances for most tasks.","1528-beam-search#15.2.8 Beam search":"The simplest way to generate from a RNN is to use greedy decoding, where we compute at each step:We can repeat this process until we generate the ‚Äúend of sentence‚Äù token.However, this will not generate the MAP sequence:The reason is that locally optimum at step  might not be on the globally optimum path (see figure below, where the likelihood is the product of all blue items).For hidden Markov models, we can use Viterbi decoding (which is an example of dynamic programming) in  where  is the vocabulary size.But for RNN, computing the global optimum takes , since the hidden state is not a sufficient statistic for the data.Beam search is a faster heuristic, where we only keep the  best path at any time step. We then expand for every  possible way, generating  candidates. We only takes the  best, and iterate.This algorithm takes .We can also sample the top  outputs without replacement. This is called stochastic beam search, where we add Gumbel noise to perturb the model partial probability at each step."}},"/proba-ml/logistic-regression/binary-logistic-regression":{"title":"10.2 Binary Logistic Regression","data":{"":"Binary logistic regression corresponds to the following model:where  are the weights and bias. In other words:and we call the log odds:","1021-linear-classifiers#10.2.1 Linear classifiers":"If the loss for misclassifying each class is the same, the class is given by:with:For  and  on the plan and  the normal at , we have:With This defines the decision boundary between both classes.","1022-nonlinear-classifiers#10.2.2 Nonlinear classifiers":"We can often make a problem linearly separable by preprocessing the input.Let:\nThen:which is the decision boundary of a circle.The resulting function is still linear, which simplifies the learning process, and we gain power by learning the parameters of . We can also use polynomial expansion up to the degree , the model becomes more complex and risk overfitting.","1023-maximum-likelihood-estimation#10.2.3 Maximum Likelihood Estimation":"10.2.3.1 Objective functionwhere  is the probability of class 110.2.3.3 Deriving the gradientThen, we compute:We plug it back into the previous equation:10.2.3.4 Deriving the HessianGradient-based optimizers will find a stationary point where , either a global or local optimum.To be sure the optimum is global we must show that the objective is convex, by showing that the Hessian is semi-positive definite:With We see that  is positive definite since for all non-zero vector  we have:This follows since  for all , because of the sigmoid function.Consequently, the NLL is strictly convex, however in practice, values of  close to 0 and 1 might cause the Hessian to be singular. We can avoid this by using  regularization.","1024-stochastic-gradient-descent#10.2.4 Stochastic gradient descent":"We can use SGD to solve the equation:where the loss here is the NLL.If we use a mini-batch of size 1, we have:Since we know the objective is convex we know this will converge to the global optimum, provided we decay the learning rate appropriately.","1025-perceptron-algorithm#10.2.5 Perceptron Algorithm":"A perceptron is a binary classifier:This can be seen as a modified logistic regression, where the sigmoid is replaced by a Heaviside function. Since the Heaviside is not differentiable, we can‚Äôt use gradient-based optimization to fit the model.Rosenblatt proposed a perceptron learning algorithm, where we only make updates for wrong predictions (we replaced soft probabilities with hard labels):\nIf  and , the update is \nIf  and , the update is \nThe advantage of Perceptron is that we don‚Äôt need to compute probabilities, which is useful when the label space is vast.The disadvantage is that this method will only converge if the data is linearly separable, whereas SGD for minimizing the NLL will always converge to the globally optimal MLE.In section 13.2, we will generalize perceptrons to nonlinear functions, enhancing their usefulness.","1026-iteratively-weighted-least-square#10.2.6 Iteratively weighted least square":"Being a first-order method, SGD can be slow, especially when some directions point steeply downhill, while others have a shallow gradient.In this situation, it can be much faster to use second-order methods that take the curvature of the space into account. We focus on the full batch setting (with  small) since it is harder to make second-order methods work in the stochastic setting.The hessian is assumed to be positive-definite to ensure the update is well-defined.If the hessian is exact, the learning rate can be set to 1:with:and:Since  is diagonal, we can rewrite:The optimal weight  is a minimizer of:Fisher scoring is the same as IRLS except we replace the Hessian with its expectation i.e. we use the Fisher information matrix. Since the Fisher information matrix is independent of the data, it can be precomputed. This can be faster for problems with many parameters.","1027-map-estimation#10.2.7 MAP estimation":"Logistic regression can overfit when the number of parameters is high compared to the number of samples. To get this wiggly behavior, the weight values are large (up to +/-50 compared to 0.5 when K=1).One way to reduce overfitting is to prevent weights from being so large, is performing MAP estimation using a zero mean Gaussian prior:The new objectives and gradient takes the form:","1028-standardization#10.2.8 Standardization":"Using the previous isotropic prior  assumes that all weights are of the same magnitudes, which assumes features are of the same magnitudes as well.To enforce that hypothesis, we can use standardization:An alternative is min-max scaling, ensuring that all inputs lie within the interval ."}},"/proba-ml/neural-networks-for-tabular-data/multilayer-perceptrons":{"title":"13.2 Multilayer perceptrons (MLP)","data":{"":"In section 10.2.5, we explained that the perception is a deterministic version of logistic regression:where  is the Heaviside step function.Since the decision boundaries of the perceptron are linear, they are very limited in what they can represent.","1321-the-xor-problem#13.2.1 The XOR problem":"One of the most famous problems of the perceptron is the XOR problem. Its truth table is:We visualize this function and see that this is not linearly separable, so a perception can‚Äôt represent this mapping.However, we can solve this problem by stacking multiple perceptrons together, called a multilayer perceptron (MLP).Above, we use 3 perceptions (, and , where the first two layers are hidden units since their output is not observed in the training data.This gives us:where the activation are , the overline is the negative operator,  is OR, and  is AND.By generalizing, we see the MLP can represent any logical function.However, we obviously want to avoid learning the weights and biases by hand and learn these parameters from the data.","1322-differentiable-mlps#13.2.2 Differentiable MLPs":"The MLP we discussed involves the non-differentiable Heaviside function, which makes it difficult to learn. This is why the MLP under this form was never widely used.We can replace the Heaviside  with a differentiable activation function The hidden units at layer  are defined by the linear transformation of the previous hidden units passed element-wise through an activation function:If we now compose  of these functions together, we can compute the gradient of the output w.r.t the parameters in each layer using the chain rule. This is called backpropagation.We can then pass the gradient to any optimizer and minimize some training objectives.","1323-activation-functions#13.2.3 Activation functions":"If we use a linear activation function, then the whole model is equivalent to a regular linear model. For this reason, we need to use non-linear activation functions.Sigmoid and tanh functions were common choice, but they saturate for large positive or negative values, making the gradient close to zero, so any gradient signal from higher layers won‚Äôt be able to propagate back to earlier layers.This is known as the vanishing gradient problem and makes it hard to train models with gradient descent.The most common non-saturating activation function is rectified linear unit (ReLU):","1324-example-models#13.2.4 Example models":"13.2.4.1 MLP for classifying 2d data into 2 categoriesThe model has the following form:13.2.4.2 MLP for image classificationTo apply MLP to 2d images (), we need to flatten them to 1d vectors (). We can then use a feedforward architecture.On MNIST, the model achieves a test set accuracy of 97%.CNNs are better suited to handle images by exploiting their spatial structure, with fewer parameters than the MLP.By contrast, with a MLP we can randomly shuffle the pixels without affecting the output.13.2.4.3 MLP for text classificationWe need to convert a variable-length sequence of words  into a fixed dimensional vector , where  is the vocabulary size and  are one-hot-encoding vectors.The first layer is an embedding matrix , which converts a sparse dimensional vector into a dimensional embedding.Next, we convert this set of  embeddings into a fixed-size vector using global average pooling.Then we can pass it to an MLP.This has the form:If we use , an embedding size of , and a hidden layer of size  , and apply this model to the IMDB movie review sentiment classification dataset, we get an accuracy of 86% on the validation set.The embedding layer has the most parameters, and with a training set of only 25k elements, it can easily overfit.Instead of learning these parameters in a supervised way, we could reuse unsupervised pre-training of word-embedding models. If  is fixed, we have much fewer parameters to learn.13.2.4.4 MLP for heteroskedastic regressionWe can also use MLP for regression. ‚ÄúHeteroskedastic‚Äù means the output-predicted variance is input-dependent.This function has two outputs to compute  and .We can share most of the layers with these two functions by using a common ‚Äúbackbone‚Äù and two outputs ‚Äúheads‚ÄùFor the  head, we use a linear activation , and for the , we use the soft-plus activation .If we use linear heads and a non-linear backbone, the model is given by:This model suits datasets where the mean grows linearly over time, with seasonal oscillations, and the variance quadratically. This is a simple example of stochastic volatility, as can be seen in financial or temperature data with climate change.The model is underconfident at some points when it considers a fix  since it needs to adjust the overall noise model and can‚Äôt make adjustments.","1325-the-importance-of-depth#13.2.5 The importance of depth":"One can show that an MLP with one hidden layer can approximate any smooth function, given enough hidden parameters, to any desired level of accuracy.The literature has shown that deep networks work better than shallow ones. The idea is that we can build compositional or hierarchical models, where later layers can leverage the feature extracted by earlier layers.","1326-the-deep-learning-revolution#13.2.6 The deep learning revolution":"Although the idea behind DNN date back several decades ago, they started to become widely used only after the 2010s. The first area to adopt these methods was the field of automatic speech recognition (ASR).The moment that got the most attention was the introduction of deep CNN to the ImageNet challenge in 2012, which significantly improve the error rate from 26% to 16%. This is was a huge jump since the benchmark only improved by 2% every year.The explosion in the usage of DNNs has several factors:\nThe availability of cheap GPUs, that can massively reduce the fitting time for CNNs\nThe growth of large, labeled datasets, allows models to gain in complexity without overfitting (ImageNet has 1.3M of labeled images)\nHigh-quality open-source software like Tensorflow (by Google), PyTorch (by Facebook), and MXNet (by Amazon). These libraries support automatic differentiation and scalable gradient-based optimization."}},"/proba-ml/neural-networks-for-tabular-data/other-kinds-of-ffn":{"title":"13.6 Other kinds of feedforward networks","data":{"1361-radial-basis-function-networks#13.6.1 Radial basis function networks":"Consider a 1-layer neural network where the hidden units are given by:where  are a set of  centroids, and  is a kernel function (see section 17.1).We use the simplest kernel function, the Gaussian kernel:where  is called the bandwith. This is called a radial basis function (RBF) kernel.A RBF network has the form:where . The centroids can be fixed or learned using unsupervised learning methods like K-means.Alternatively, we can associate one centroid per datapoint,  with . This is an example of non-parametric model, since the number of parameters grow with the size of the data.If  the model can perfectly interpolate the data and overfit. However, by ensuring the weights  are sparse, the model will use a finite number of weights and will result in a sparse kernel machine.Another way to avoid overfitting is to use a Bayesian approach, by marginalizing the weights, this gives rise to the Gaussian process model.13.6.1.1 RBF network for regressionWe can use RBF for regression by defining:Below, we fit 1d data to a model with  uniformly spaced RBF clusters, with the bandwith  ranging from low to large.When the bandwidth is small, the predicted function is wiggly since points far from the clusters will lead to predicted value of 0.On the contrary, if the bandwidth is large, each points is equally close to all clusters, so the prediction is a straight line.13.6.1.2 RBF network for classificationWe can use RBF in classification by considering:RBF are able to solve the XOR problem.","1362-mixtures-of-experts#13.6.2 Mixtures of experts":"When considering regression, it is common to assume an unimodal output distribution. However, this won‚Äôt work well with one-to-many functions, in which each input is mapped to multiple outputs.There are many real-world problems like 3d pose estimation of a person given an image, colorization of a black and white image, and predicting future frames of a video sequence.Any model trained to maximize likelihood using a unimodal output density (even non-linear and flexible ones) will have poor performances on one-to-many functions since it will produce a blurry average output.To prevent this problem of regression to the mean, we can use a conditional mixture model. We assume the output is a weighted mixture of  different outputs, corresponding to different modes of the output distribution for each input .In the Gaussian case, this becomes:where:Here,  predicts which mixture to use,  and  predict the mean and the variance of the kth Gaussian.This is called a Mixture of Expert (MOE), where we choose submodels that are experts on some region of the input space, with a gating function .By picking the most likely model expert for a given , we can activate a subset of the model. This is an example of conditional computation.13.6.2.1 Mixture of linear expertsThe model becomes:where  is the kth output from the softmax. Each expert corresponds to a linear regression with different parameters.As we can see in the figure above, if we take the average prediction from all experts, we obtain the red curve, which fits our data poorly. Instead, we only predict using the most active expert to obtain the discontinuous black curve.13.6.2.2 Mixture density networksThe gating and expert functions can be any kind of conditional probabilistic model, like DNN. In this situation, the model becomes a mixture density network (MDN).13.6.2.3 Hierarchical MOEsIf each network is a MOE itself, we obtain a hierarchical mixture of experts.An HME with  levels can be thought of as a ‚Äúsoft‚Äù decision tree of depth , where each example is passed through every branch of the tree, and the final prediction is the weighted average."}},"/proba-ml/optimization/bound-optimization":{"title":"8.7 Bound optimization","data":{"871-the-general-algorithm#8.7.1 The general algorithm":"Our goal is to maximize some function , such as the log-likelihood. We construct a tight lower-bound surrogate function  such that:If these conditions are met, we use the update:This guarantees the monotonic increases of the original objective:This iterative procedure will converge to a local minimum of the .If  is quadratic, then the bound optimization is similar to Newton‚Äôs method, which repeatedly fits and optimizes a quadratic approximation.The difference is that optimizing  is guaranteed to improve the objective, even if it is not convex, whereas Newton may overshoot or lead to a decrease in the objective since it is a quadratic approximation, not a bound.","872-the-em-algorithm#8.7.2 The EM Algorithm":"8.7.2.1 Lower boundThe goal of the EM is to maximize the log-likelihood:where  are the visible and  the invisible variables.To push the  into the sum, we consider arbitrary distributions :Eq 2. used Jensen‚Äôs inequality since   is a concave function and  is called Evidence Lower Bound (ELBO), optimizing it is the basis of variational inference.8.7.2.2 E stepWe estimate the hidden variables We can maximize this lower bound by setting . This is called the E step.We can define:Then we have  and  as required.8.7.2.3 M StepWe compute the MLE by maximizing  w.r.t , where  are the distributions computed during the E step at iteration .Since  is constant w.r.t , we can remove it. We are left with:"}},"/proba-ml/neural-networks-for-tabular-data/training-neural-nets":{"title":"13.4 Training neural networks","data":{"":"We now discuss how to fit DNNs to data. The standard approach is to use MLE, by minimizing the NLL:In principle, we can use the backprop algorithm and compute the gradient of this loss and pass it to an optimizer. Adam (section 8.4.6.3) is a popular choice, due to its ability to scale to large datasets (by virtue of being of SGD-type) and converges fairly quickly (by virtue of using diagonal preconditioning and momentum).However, in practice, this may not work well. In addition to practical issues, there are theoretical limitations. In particular, the loss of DNNs is not convex, so we will generally not find the global optimum.Nevertheless, SGD can often find surprisingly good solutions.","1341-tuning-the-learning-rate#13.4.1 Tuning the learning rate":"It is essential to tune the learning rate, to ensure convergence to a good solution (see section 8.4.3).","1342-vanishing-and-exploding-gradient#13.4.2 Vanishing and exploding gradient":"When training very deep networks, the gradient tends to become very small (vanishing) or very large (exploding), because the signal is passed through a series of layers that either amplify it or diminish it.Consider the gradient of layer :If  is constant across layers, the contribution of the gradient from the final layer, , to layer  will be . Thus the behavior of the system depends on the eigenvectors of .Although  is a real-valued matrix, it is not symmetric, so its eigenvalues and eigenvectors can be complex-valued, with the imaginary components corresponding to oscillatory behavior.Let  be the spectral radius of  (the largest absolute eigenvalues). If , the gradient can explode, if  the gradient can vanish.We can counter the exploding problem with gradient clipping, in which we cap the value of the magnitude of the gradient to  if it becomes too large:However, the vanishing problem is more difficult to solve. There are various solutions:\nModify the activation functions (see next section)\nModify the architecture so that the updates are additive rather than multiplicative (residual networks)\nModify the architecture to standardize the activation at each layer, so that the distribution of activations over the dataset remains constant during training (batch norm).\nCarefully choose the initial values of the parameters","1343-non-saturating-activation-functions#13.4.3 Non-saturating activation functions":"As already mentioned in section 13.2.3, the sigmoid activation function and  function saturates for small and large values of activations, vanishing gradient.For the sigmoid of a linear layer, we have:Hence, if the activation  is close to 0 or 1, the gradient is close to 0.13.4.3.1 ReLUThe rectified linear unit is defined as:Its gradient is:Therefore, with a linear layer, we have:Hence the gradient will not vanish, as long as  is positive.However, if some of the weights are initialized with large negative values, then some activations will go to zero and their gradient too. The algorithm will never be able to escape this situation, so some hidden units will stay permanently off. This is called the ‚Äúdead-ReLU‚Äù problem.13.4.3.2 Non-saturating ReLUThe leaky ReLU has been proposed to solve the dead-ReLU issue:with , which allows some signal to be passed back to earlier layers, even when the input is negative.If we allow the parameter  to be learned, this is called parametric ReLU.Another popular choice is ELU (exponential linear unit):This has the advantage of being a smooth function.A slight variant of ELU is known as SELU (self-normalizing ELU):Surprisingly, it has been proven that by setting  and  carefully, the activation function ensures that the output of each layer is standardized (providing the input is also standardized), even without the use of techniques such as batch norm.13.4.3.3 Other choicesSwitch or SiLU (sigmoid rectified unit) appears to work well for image classification benchmarks and is defined as:Another popular choice is GELU (Gaussian Error Linear Unit):where  is the cdf of the standard normal distribution:We see that this is not a convex or monotonic function, unlike most activation functions.We can think of GELU as a ‚Äúsoft‚Äù RELU since it replaces the step function with the Gaussian cdf.See PyTorch's non-linear activations module for an exhaustive list.","1344-residual-connections#13.4.4 Residual connections":"One solution to the vanishing problem is to use residual networks or ResNet. This is a feedforward model in which each layer has the form of a residual block:where  is a standard shallow non-linear mapping (e.g. linear-activation-linear).Residual connections are often used conjointly with CNNs, but can also be used in MLPs.A model with residual connections has the same number of parameters as without it but is easier to train. The reason is that gradient can flow directly from the output to the earlier layers.To see this, note that the activation at the output layer can be derived as:We can therefore compute the gradient of the loss wrt the parameters of the th layer as follows:We see that the gradient at layer  depends directly on the gradient at layer , independently from the network depth.","1345-parameter-initialization#13.4.5 Parameter initialization":"Since the objective function of DNNs is non-convex, the initial values of parameters can play a big role in the final solution, as well as how easy the function is to train (i.e. how well signal can flow backward and forward in the model).13.4.5.1 Heuristic initialization schemeIt has been shown that sampling parameters from a standard normal distribution with fixed variance can lead to exploding activation or gradient.To see why, consider a linear unit  where , , :To keep the variance from blowing up, we need to ensure  during forward pass, where  is the fan-in (input connections).When doing the backward pass, we need , where  is the fan-out.To satisfy both requirements we set:or equivalently:This is known as Xavier initialization (use it for linear, tanh, logistic and softmax activation function).In the special case of , we use , this is LeCun initialization (use it for SELU).Finally,  is Hue initialization (use it for ReLU and variants).13.4.5.2 Data-driven initializationWe can adopt a data-driven approach to parameter initialization, like layer-sequential unit-variance (LSUV), working as follow:\nInitialize the weights (fully connected or convolutional) using orthonormal matrices, by drawing  then using QR or SVD decomposition.\nFor each layer , compute the variance of activation across a minibatch\nRescale the weights as \nThis can be viewed as orthonormal initialization combined with batch normalization applied on the first mini-batch. This is faster than full-batch normalization and can work just as well.","1346-parallel-training#13.4.6 Parallel training":"Training large networks can be slow, and specialized hardware for matrix-matrix multiplication like graphics processing units (GPU) and tensor processing unit (TPU) can boost this process.If we have multiple GPU, we can further speed things up.The first approach is model parallelism, in which we partition the model across different machines. This is quite complicated since this requires tight communication between machine and we won‚Äôt discuss it further.The second approach is data parallelism, which is much simpler as it is embarrassingly parallel. For each training step :\nWe split the minibatch across  machines to get .\nEach machine  computes its own gradient \nWe sum all gradient on a central machine \nWe broadcast the summed gradient back to all machines, \nEach machine update its parameters using \nSee this tutorial for a toy implementation in Jax, and this tutorial for a more robust solution in PyTorch."}},"/proba-ml/neural-networks-for-tabular-data/regularization":{"title":"13.5 Regularization","data":{"":"The previous section discussed computational issues arising when training DNNs. We now focus on statistical issues, in particular on avoiding overfitting.Large neural nets can easily have millions of parameters, which makes them particularly prone to overfitting.","1351-early-stopping#13.5.1 Early stopping":"One of the easiest ways to perform regularization is to stop the training procedure when the validation error increases.This works because we restrain the optimization algorithm from further transferring information from the training example to the parameters.","1352-weight-decay#13.5.2 Weight decay":"A common approach to reduce overfitting is to impose a prior of the parameter and use MAP estimation.It is standard to use a Gaussian distribution on the weighs  and biases .This is equivalent to  regularization for SGD (but not for Adam, see the AdamW paper). In the neural networks literature, this is called weight decay since it encourages smaller weights.","1353-sparse-dnns#13.5.3 Sparse DNNs":"Since there are many weights in DNN, it is often helpful to encourage sparsity. This allows us to perform model compression to save memory and time.We can achieve this with a  regularization or ARD, for instance.In the example below, a 5-layer MLP fit on 1d regression with  regularization has a sparse topology.Despite their intuitive appeal, these methods are not widely used because GPUs are optimized for dense matrix multiplication.However, if we use group sparsity, we can prune out whole layers of the model. This results in block sparse weight matrices, which can result in speedup and memory saving.","1354-dropout#13.5.4 Dropout":"Suppose, for each example, we randomly turn off all the outgoing connections of a neuron with probability . This is called dropout.Dropout can dramatically reduce overfitting and is widely used. It works because it prevents complex co-adaptation of the hidden units. Each unit must learn to perform well when others are missing at random.We can view dropout as estimating a noisy version of the term  where . At test time, we usually turn the noise off.To ensure the weights have the same expectation during training and testing, we multiply them by .We can also use dropout during testing. This results in an ensemble of networks, each with a slightly different sparse graph structure. This is known as the Monte Carlo dropout:","1355-bayesian-neural-networks#13.5.5 Bayesian neural networks":"Modern DNNs are often trained using a penalized maximum likelihood to find a single setting of parameters. However, sometimes the number of parameters far exceeds the number of data points, and multiple models may fit the data equally well.It is therefore often useful to capture the uncertainty with a posterior predictive distribution, rather than optimizing on a single set of parameters. This can be done by marginalizing the parameters:Here the model is an ensemble over all possible models, weighted by the posterior probability of the parameters. This is known as Bayesian neural networks (BNN).BNN is challenging for large neural networks but can bring some performance gains.","1356-regularization-effects-of-sgd#13.5.6 Regularization effects of SGD":"Some optimization methods (in particular second-order ones) are able to find a narrow set of parameters with a very low loss, called a sharp minima.While this effectively reduces the empirical loss, the optimizer has overfitted, and we prefer finding instead flat minima. Such solutions are more robust and generalize better since there is a lot of posterior uncertainty and less data point memorization.SGD often finds flat minima by adding noise, preventing it to enter narrow landscapes. This is called implicit regularization. We can also explicitly encourage flat minima with entropy SGD, sharpness-aware minimization, and stochastic weight averaging (SWA).The loss landscape depends on the data and not only on the parameters. As we usually can‚Äôt afford to perform full-batch gradient descent, we will end up with a series of loss curves, one for each minibatch.If the variance of these curves is low, the model has found a flat minima and will generalize well.We can consider the continuous-time gradient flow, which has the form:where:where  is the original loss and second term penalizes large gradient.If we extend this analysis to SGD, we obtain the following flow:Therefore:The second term is the variance of the minibatch gradients, which is a measure of stability and hence, of generalization.This shows that SGD not only has computation advantage (since it is faster than full batch or second-order methods), but also statistical advantages."}},"/proba-ml/optimization/constrained-optimization":{"title":"8.5 Constrained Optimization","data":{"":"We consider the following problem:where the feasible set is:with  is the set of equality constraints and  the set of inequality constraints.","851-lagrange-multipliers#8.5.1 Lagrange Multipliers":"Let‚Äôs have a single constrained .We know that the gradient of the loss and the gradient of the constraint are (anti-?)parallel at some point  minimizing .\nproofIf we move by  on the constraint surface we have:As we have , we must have .We seek a point   on the constrained surface minimizing .  must also be orthogonal to the constraint surface, otherwise we could decrease the objective by just moving on the surface.\nfor some .This leads to the objective:At a stationary point, we have:This is called a critical point.If we have  constraints, we can add them:We end up with  equations for  variables and can use unconstrained optimization methods to solve this system.ExempleWe minimize , subject to the constraint .The Lagrangian is:Therefore we need to solve:","852-the-kkt-conditions#8.5.2 The KKT conditions":"We introduce inequality constraints :However, this is a discontinuous function that is hard to optimize.Instead, we can consider and our optimization problem becomes:The general Lagrangian becomes:When  and  are convex, then all critical points must satisfy the Karush-Kuhn-Tucker (KKT) conditions:\nAll constraints are satisfied (feasibility)\nThe solution is a stationary point:\nThe penalty for the inequality constraints points in the right direction (dual feasibility):\nThe Lagrange multiplies pick up any slack in the inequality constraints:\nIf the  is convex and the constraints define a convex set, then the KKT conditions are sufficient to ensure global optimality.","853-linear-programming#8.5.3 Linear Programming":"We optimize a linear function subject to linear constraints, represented as:The feasible set defines a convex polytope: a convex space defined as an intersection of half-spaces.8.5.3.1 The simplex algorithmIt can be shown that the optima of an LP occur at the vertex of the polytope defining the feasible set.The simplex algorithm solves LPs by moving from vertex to vertex, seeking the edge that most improve the objective.8.5.3.2 ApplicationsApplications range from business to machine learning, in robust linear regression settings. It is also useful for state estimation in graphical models.","854-quadratic-programming#8.5.4 Quadratic programming":"We optimize a quadratic function:If  is semi-positive, then this is a convex problem.ExampleLet‚Äôs consider:with  and Subject to We can decompose the above constraint into:We can write as , withThis is now in the standard QP form.We see that the 2 edges of the diamond to the left are inactive because the objective is in the opposite directions.This means  and  and by complementarity  From the KKT conditions, we know that:This gives us:The solution is 8.5.4.1 ApplicationsQP programming is used in the lasso method for sparse linear regression, which optimizes:It can also be used for SVM."}},"/proba-ml/optimization/first-order-methods":{"title":"8.2 First-Order Methods","data":{"":"First-order methods compute first-order derivatives of the objective function but they ignore the curvature information.All these algorithms required the user to choose a starting point  before iterating:where  is the learning rate and  is the descent direction, such as the negative of the gradient .These update steps are performed until reaching a stationary point.","822-step-size#8.2.2 Step size":"The sequence of steps  is the learning rate schedule. We describe some methods to pick this.","8221-constant-step-size#8.2.2.1 Constant step size":"The simplest method is to consider . However, if set too large the method will fail to converge, and if set too small, the method will converge very slowly.We can derive a theoretical upper bound for the step size. For a quadratic objective function:with   positive semi-definite, the steepest descent will converge iff the step size satisfies:This ensures that the step is lower than the slope of the steepest direction, which the eigenvalue measures.More generally:where  is the Lipschitz constant of the gradient ‚Äîbut this constant is usually unknown.","8222-line-search#8.2.2.2 Line search":"The optimal step size can be found by solving the optimization:If the loss is convex, this subproblem is also convex because the update term is affine.If we take the quadratic loss as an example, we solve:and find:However, it is often not necessary to be so precise; instead, we can start with the current step size and reduce it by a factor  at each step until it satisfies the Armijo-Goldstein test:where , typically .","823-convergence-rates#8.2.3 Convergence rates":"In some convex problems where the gradient is bounded by the Lipschitz constant, gradient descent converges at a linear rate:where  is the convergence rate.For simple problems like the quadratic objective function, we can derive  explicitly. If we use the steepest gradient descent with line search, the convergence rate is given by:Intuitively, the condition problem  represents the skewness of the space ‚Äîi.e. being far from a symmetrical bowl.More generally, the objective around the optimum is locally quadratic, hence the convergence rate depends on the condition number of the Hessian at that point: . We can improve the speed of convergence by using a surrogate model that has a Hessian close to the Hessian of the objective.Line search often exhibits zig-zag paths that are inefficient, instead, we can use conjugate gradient descent.","8231-conjugate-gradient-descent#8.2.3.1 Conjugate gradient descent":"are conjugate vectors w.r.t  iff:Let  is a set of mutually conjugate vectors, i.e.  for . forms a basis for , and we can write:The gradient of the quadratic objective function is:thus we choose our first vector as:and we will find other vectors as conjugates with the following update:","824-momentum-methods#8.2.4 Momentum methods":"","8241-momentum#8.2.4.1 Momentum":"This can be implemented as follow:with , usually .Where the momentum  plays the role of an exponentially weighted moving average of the gradient:","8242-nesterov-momentum#8.2.4.2 Nesterov momentum":"The standard momentum issue is that it may not slow down enough at the bottom of the valley and causes oscillations.To mitigate this, Nesterov accelerated gradient adds extrapolation steps to the gradient descent:This can be rewritten in the same format as the momentum:This method can be faster than standard momentum because it measures the gradient at the next location.In practice, using the Nesterov momentum can be unstable if  or  are misspecified."}},"/proba-ml/optimization/intro":{"title":"8.1 Intro","data":{"":"Optimization is at the core of machine learning: we try to find the values of a set of variable  that minimizes a scalar-valued loss function :with the parameter space .An algorithm minimizing this loss function is called a solver.","811-local-vs-global-optimization#8.1.1 Local vs Global Optimization":"The global optimum is the point that satisfies the general equation, but finding it is computationally intractable.Instead, we will try to find a local minimum. We say that  is a local minimum if:A local minimum can be flat (multiple points of equal objective value) or strict.A globally convergent algorithm will converge to a stationary point, although that point doesn‚Äôt have to be the global optimum.Let   and  hessian matrix.\nNecessary condition:   is a local minimum   and  is positive semi-definite\nSufficient condition:  and  positive definite   is a local minimum\nThe condition on  is necessary to avoid saddle points ‚Äîin this case, the eigenvalues of the Hessian will be both positive and negative, hence non-positive definite.","812-constrained-vs-unconstrained-optimization#8.1.2 Constrained vs Unconstrained Optimization":"In unconstrained optimization, we minimize the loss with any value in the parameter space .We can represent inequality constraints as  for  and equality constraints as  for The feasible set is then:And the constraint optimization problem becomes:Adding constraints can create new minima or maxima until the feasible set becomes empty.A common strategy for solving constrained optimization is to combine penalty terms, measuring how much we violated the constraints, with the unconstrained objective. The Lagrangian is a special case of this combined objective.","813-convex-vs-nonconvex-optimization#8.1.3 Convex vs Nonconvex Optimization":"In convex optimization, every local minimum is a global minimum, therefore many machine learning models are designed so that their training objective is convex. is a convex set if for any  and  we have:We say  is a convex function if its epigraph (the set of points above the function) defines a convex set.Equivalently,  is convex if it is defined on a convex set, and for any  and : is concave if   is convex.Suppose  is twice differentiable, then:\n is convex iff  is positive semi definite.\n is strictly convex iff  is positive definite\nFor example, let‚Äôs consider the quadratic form . It is convex if  is positive definite.Furthermore,  is strongly convex with parameter  if for any  in its subspace:If  is twice differentiable, then it is strongly convex with parameter  iff  ‚Äîmeaning   is positive definite.In the real line domain, this condition becomes .In conclusion:\n is convex iff \n is strictly convex if \n is strongly convex with parameter  iff","814-smooth-vs-non-smooth-optimization#8.1.4 Smooth vs Non-smooth Optimization":"In smooth optimization, the objective function is continuously differentiable, and we quantify the smoothness using the Lipschitz constant. In the 1d case:The function output can‚Äôt change by more than  if we change the input by  unit.Non-smooth functions have at least some functions in the gradient that are not defined:In some problems we can partition the objective into a part containing only smooth terms and a part with nonsmooth (rough) terms:In machine learning, these composite objectives are often formed with  the training loss and  a regularizer such as the  norm of .8.1.4.1 SubgradientsWe generalize the notion of derivative to function with local discontinuities.For ,  is the subgradient of at :For example, the subdifferential  of  is:"}},"/proba-ml/optimization/proximal-gradient-methods":{"title":"8.6 Proximal gradient method","data":{"":"We are often interested in optimizing an objective of the form:where:\n is a differentiable (smooth) loss, like the NLL\n is a convex loss but may not be differentiable, like the  norm or an indicator function that is infinite when constraints are violated.\nOne way to solve this is to use a Proximal gradient method: take a step size   in the gradient direction and then project the result in the constraints space respecting :where:We can rewrite the proximal operator as a constraint optimization problem:Thus the proximal function minimizes the rough loss while staying close to the current iterate .","861-projected-gradient-descent#8.6.1 Projected gradient descent":"We want to optimize:where  is a convex set.We can have box constraints, with In unconstrained settings, this becomes:with:We can solve that with the proximal gradient operator:In the box constraints example, the projection operator can be computed element-wise:","862-proximal-operator-for-the-l1-norm-regularizer#8.6.2 Proximal operator for the L1-norm regularizer":"A linear predictor has the form .We can foster model interpretability and limit overfitting by using feature selection, by encouraging weight to be zero by penalizing the  norm:This induces sparsity because if we consider 2 points:  and , we have  but .Therefore the sparse solution  is cheaper when using the  norm.If we combine our smooth loss with the regularizer we get:We can optimize this with the proximal operator, in a element-wise fashion:In section 11.4.3, we show that the solution is given by the soft threshold operator:","863-proximal-operator-for-quantization#8.6.3 Proximal operator for quantization":"In some applications like edge computing, we might want to ensure the parameters are quantized.We define a regularizer measuring the distance between the parameter and its nearest quantized version:In the case , this becomes:We solve this with proximal gradient descent, in which we treat quantization as a regularizer: this is called ProxQuant.The update becomes:","864-incremental-online-proximal-methods#8.6.4 Incremental (online) proximal methods":"Many ML problems have an objective function that is the sum of losses, one per example: these problems can be solved incrementally, with online learning.It is possible to extend proximal methods to these settings, the Kalman Filter is one example."}},"/proba-ml/optimization/second-order-methods":{"title":"8.3 Second-order methods","data":{"":"First-order methods only exploit the gradient of the loss function, which is cheap to compute but can be imprecise. Second-order methods leverage the curvature of the loss using the Hessian.","831-newtons-method#8.3.1 Newton‚Äôs method":"The classic second-order method is Newton‚Äôs method, whose update is:where  is assumed positive-definite. The intuition is that  ‚Äúundoes‚Äù the skewness in the local curvature.To see why, consider the Taylor series approximation of  around :The differential w.r.t  gives:","832-bfgs#8.3.2 BFGS":"Quasi-Newton methods iteratively build up an approximation to the Hessian, using information about the gradient at each step:with  and If  is positive definite and the step size  is chosen via line search satisfying the Armiko condition and the curvature condition:with , then  remains positive definite.Typically, .Alternatively, BFGS can iteratively update an approximate the inverse of the Hessian:Since storing the Hessian approximation still takes  space, for very large problems the limited memory (L-BFGS) is preferred.Rather than storing  , we store the  most recent  and then approximate  by performing a sequence of inner products with the stored  and  vectors. between 5-20 is often sufficient.","833-trust-region-methods#8.3.3 Trust region methods":"If the objective function is non-convex, then the Hessian may not be positive definite, so  may not be the descent direction. In general, any time the quadratic approximation made by Newton‚Äôs method is invalid, we are in trouble.However, there is usually a local region  around the current iterate where we can safely approximate the objective by a quadratic.Let  the approximation to the objective, where . At each step, we solve:this is called trust-region optimization.We usually assume  is a quadratic approximation:We assume that the region is a ball:The constraint problem becomes an unconstrained one:for some Lagrange multiplier .We can solve this using:This is called Tikhonov regularization, adding a sufficiently large  ensure that the resulting matrix is always positive definite.See Beyond auto-differentiation from Google AI using trust region and quadratic approximations."}},"/proba-ml/optimization/stochastic-gradient-descent":{"title":"8.4 Stochastic gradient descent","data":{"":"The goal is to minimize the average of the loss function:where  is a training sample. In other words, at each iteration:where .The resulting algorithm is the SGD:As long as the gradient estimate is unbiased, this will converge to a stationary point, providing a decay on .","841-finite-sum-problem#8.4.1 Finite sum problem":"SGD is widely used in ML, because many models are based on empirical risk minimization, which minimizes the following loss:This is called a finite-sum problem. Computing the gradient requires summing over all  examples and can be slow. Fortunately, we can approximate it by sampling a minibatch :where  is a random set of examples. Therefore, SGD is an unbiased estimate of the empirical average of the gradient.Although the rate of convergence of SGD is slower than in batch GD, the per-step time is lower for SGD. Besides, it is most of the time wasteful to compute the complete gradient, above all at the beginning when the parameters are not well estimated.","842-example-using-the-lms#8.4.2 Example using the LMS":"The objective of the linear regression is:And its gradient:And with SGD of batch size  the update is:with the index at each iteration .LMS and LGD might need multiple passes to the data to find the optimum.","843-choosing-the-step-size-learning-rate#8.4.3 Choosing the step size (learning rate)":"An overall small learning rate leads to underfitting, and a large one leads to instability of the model: both fail to converge to the optimum.Rather than using a single constant, we can use a learning rate schedule. A sufficient condition for SGD to converge is the schedule satisfying the Robbins-Monro condition:Some common examples of learning rate schedules:Piecewise constant schedule reduces the learning rate when a threshold is reached, or when the loss has plateaued, this is called reduce-on-plateau.Exponential decay is typically too fast, and polynomial decay is preferred, here with  and .Another scheduling strategy is to quickly increase the learning rate before reducing it again: we hope to escape a poorly-conditioned loss landscape with small steps, before making progress with larger steps: this is the one-cycle learning rate schedule.We can also repeat this pattern in a cyclical learning rate schedule to escape local minima.","844-iterate-averaging#8.4.4 Iterate averaging":"The estimate of the parameter can be unstable with SGD. To reduce the variance, we can compute the average with:This is called iterate averaging or Polyak-Ruppert averaging. It has been proven that this method yields the best asymptotic convergence for SGD, matching second-order methods.**Stochastic Weight Averaging (SWA)** exploits the flatness in objective to find solutions that provide better generalization rather than quick convergence. It does so using an equal average with a modified learning rate schedule.","845-variance-reduction#8.4.5 Variance reduction":"These methods reduce the variance of the gradients in SGD, rather than the parameters themselves, and can improve the convergence rate from sublinear to linear.8.4.5.1 Stochastic Variance Reduced Gradient (SVRG)For each epoch, we take a snapshot of our parameters, , and compute its full-batch gradient . This will act as a baseline, and we also compute the gradient for this snapshot  at each minibatch iteration .The gradient becomes:This is unbiased since:Iterations of SVRG are faster than those of full-batch GD, but SVRG can still match the theoretical convergence rate of GD.8.4.5.2 Stochastic Average Gradient AcceleratedUnlike SVRG, SAGA only needs to compute the full batch gradient once at the start of the algorithm. It ‚Äúpays‚Äù for this speed-up by keeping in memory  gradient vectors.We first initialize  and then compute .Then at iteration , we use the gradient estimate:we then update  and  by replacing the old local gradients by the new ones.If the features (and hence the gradients) are sparse, then the cost of storing the gradients is reasonable.8.4.5.3 Application to deep learningVariance reduction techniques are widely used for fitting ML models with convex objectives but are challenging in deep learning. This is because batch normalization, data augmentation, and dropout break the assumptions of SVGR since the loss will differ randomly in ways that do not only depend on parameters and data index.","846-preconditioned-sgd#8.4.6 Preconditioned SGD":"We consider the following update:where  is a preconditioned matrix,  positive definite. The noise in the gradient estimate makes the estimation of the Hessian difficult, on which second-order methods depend.Also, solving the update direction with the full preconditioned matrix is expensive, therefore practitioners often use diagonal .8.4.6.1 Adaptative Gradient (AdaGrad)AdaGrad was initially designed for convex objectives where many elements of the gradient vector are zero. These correspond to features that are rarely present in the input, like rare words.The update has the form:where  is the index of the dimension of the parameters,  controls the amount of adaptivity andEquivalently:where the root and divisions are computed element-wise.Here we consider We still have to choose the step size but AdaGrad is less sensitive to it than vanilla GD. We usually fix .8.4.6.2 RMSProp and AdaDeltaAdaGrad denominator can increase too quickly, leading to vanishing learning rates. An alternative is to use EWMA:usually with , which approximates the RMS:Therefore the update becomes:Adadelta is similar to RMSProp but adds an EWMA to the update of the parameter  as well:with:If we fix  as the adaptative learning rate is not guaranteed to decrease, the solution might not converge.8.4.6.3 Adaptative Moment Estimation (Adam)Let‚Äôs combine momentum with RMSProp:this gives the following update:with , ,  and If we initialize , we bias our update towards small gradient values. Instead, we can correct the bias using:8.4.6.4 Issues with adaptative learning ratesAdaptative learning rates  still rely on setting a . Since EWMA is computed using stochastic settings with noisy gradients, the optimization might not converge, even for convex problems, because the effective learning rate increases.To mitigate this, AMSGrad, Padam, or Yogi have been developed, the latter replacing:with:Because ADAM uses EWMA which is by nature multiplicative, its adaptative learning rate decays fairly quickly. In sparse settings, gradients are rarely non-zero so ADAM loses information quickly.In contrast, YOGI uses an additive adaptative method.8.4.6.5 Non-diagonal preconditioning matricesPreceding methods don‚Äôt solve the problem of ill-conditioning due to the correlation of the parameters, and hence do not always provide a speed boost over SGD as we might expect.One way to get faster convergence is to use the full-matrix Adagrad:where .Unfortunately, , which is expensive to store and invert.The Shampoo algorithm makes a block diagonal approximation to M, one per layer of the model, and then uses the Kronecker product structure to invert it.where  is the tensor-matrix product. This can be expressed as:\nif len(state) == 0:\n    for dim_id, dim in enumerate(grad.size()):\n        state[f\"precond_{dim_id}\"] = epsilon * torch.eye(dim)\nfor dim_id, dim in enumerate(grad.size()):\n    precond = state[f\"precond_{dim_id}\"]\n    grad = grad.transpose_(0, dim_id)\n    transposed_size = grad.size()\n    grad = grad.view(dim, -1)\n    \n    precond += grad @ grad.T  # (dim, dim)\n    inv_precond = _matrix_power(precond, -1 / order) # uses SVD\n    \n    grad = inv_precond @ grad  # (dim, -1)\n    grad = grad.view(transposed_size)\nsee full code.However, Shampoo is impracticable in large applications and is difficult to parallelize. Ani+20 introduces the following changes:\ndesign a pipelined version of the optimization, exploiting the heterogeneity of the CPU-accelerators coupled architecture\nextend Shampoo to train very large layers (such as embeddings layers)\nreplace the expensive SVD handling preconditioners with iterative methods to compute the roots of PSD matrices"}},"/proba-ml/recommender-systems/explicit-feedback":{"title":"22.1 Explicit feedback","data":{"":"We consider the simplest setting in which the user gives explicit feedback in terms of a rating, such as +1 or -1 (for like / dislike) or a score from 1 to 5.Let  be the rating matrix, for  users and   movies. Typically the matrix will be very large but very sparse since most users will not provide a feedbacks on most items.We can also view the matrix as a bipartite graph, where the weight of the  edge is . This reflects that we are dealing with relational data, i.e. the values of  and   don‚Äôt have intrinsic meaning (they are just indices), it is the fact that  and  are connected that matters.If  is missing, it could be that user  didn‚Äôt interact with item , or that they knew they wouldn‚Äôt like it and choose not to engage with it. In the former case, the data is missing at random whereas in the second the missingness is informative about the true value of .We will assume that data is missing at random for simplicity.","2211-datasets#22.1.1 Datasets":"In 2006, Netflix released a large dataset of 100m movie ratings (from a scale of 1 to 5) from 480k users on 18k movies. Despite its large size, the matrix is 99% sparse (unknown).The winning team used a combination of techniques, see Lessons from the Netflix Challenge.The Netflix dataset is not available anymore, but the MovieLens group has released an anonymized public dataset of movie ratings, on a scale of 1-5, that can be used for research. There are also the Jester jokes and the BookCrossing datasets.","2212-collaborative-filtering#22.1.2 Collaborative filtering":"The original approach to the recommendation problem is called collaborative filtering. The idea is that users collaborate on recommending items by sharing their ratings with other users.Then, if  wants to know if they interact with , they can see what ratings other users  have given to , and take a weighted average:where we assume  if the entry is unknown.The traditional approach measured the similarity of two users by comparing the sets  and , where  is the set of items.However, this can suffer from data sparsity. Below, we discuss about learning dense embedding for users and items, so that we are able to compute similarity in a low dimensional feature space.","2213-matrix-factorization#22.1.3 Matrix factorization":"We can view the recommender problem as one of matrix completion, in which we wish to predict all the missing entries of . We can formulate this as the following optimization problem:However, this is an under-specified problem, since there are an infinite number of ways of filling in the missing entries of .We need to add some constraints. Suppose we assume that  is low-rank. Then we can write it in the form:where  and ,  is the rank of the matrix.This corresponds to a prediction of the form:This is called matrix factorization.If we observe all  entries, we can find the optimal  using SVD. However, when  has missing entries, the corresponding objective is no longer convex, and doesn‚Äôt have a unique optimum.We can fit this using alternative least square (ALS), where we estimate  given  and then   given . Alternatively, we can use SGD.In practice, it is important to also follow for user-specific and item-specific baselines, by writing:This can capture the fact that some users tend to give low or high ratings, and some items may have unusually high ratings.In addition, we can add some  regularization to the parameters to get the objective:We can optimize using SGD by sampling an entry  from the set of observed values, and performing the update:where  is the error term, and  is the learning rate.This approach was first introduced by Simon Funk, who was one of the first to do well in the early days of the Netflix challenge.","22131-probabilistic-matrix-factorization-pmf#22.1.3.1 Probabilistic matrix factorization (PMF)":"We can convert matrix factorization into a probabilistic model by defining:This is known as probabilistic matrix factorization (PMF).The NLL is equivalent to the matrix factorization objective. However, the probabilistic perspective allows us to generalize the model more easily.For example, we can capture the fact that ratings are positive integers (mostly zeros) instead of reals by using a Poisson or negative Binomial likelihood.","22132-example-netflix#22.1.3.2 Example: Netflix":"We apply PMF to the Netflix dataset using  latent factors. The figure below visualizes some learning embedding vectors .Left is humor and horror movies, while movies on the right are more serious. Top is critically acclaimed independent movie and on the bottom mainstream Hollywood blockbusters.Users are embedded into the same space as movies, we can then predict the rating for any user-video pair using proximity in the latent embedding space.","22133-example-movielens#22.1.3.3 Example: MovieLens":"We apply PMF to the MovieLens-1M dataset with 6k users, 3k movies and 1m ratings. We use  factors.For simplicity, we fit this using SVD applied to the dense ratings matrix, where we replace the missing values with 0. We truncate the predictions so that they lie in the range .We see that the model is not particularly accurate but it captures some structure in the data.Furthermore, it seems to behave in a qualitatively sensible way. Below, we show the top 10 movies rated by a user and show the top 10 predictions for movies they had not seen.Without giving explicit informations, the model captured the underlying preference for action and film-noir.","2214-autoencoders#22.1.4 Autoencoders":"Matrix factorization is a (bi)linear model. We can make it nonlinear using autoencoders.Let  be the th column of the rating matrix, where unknown ratings are set to 0.We can predict this ratings vector using an autoencoder of the form:where  maps the ratings to the embedding space,  maps the embedding space to a distribution over ratings,  are the biases of the hidden units,  are the biases of the output units and  is the sigmoid activation function.This is called the item-based version of the AutoRec model. This has  parameters.There is also a user-based version with  parameters, but on both MovieLens and Netflix, the authors find the item-based version works better.We can fit this by only updating parameters that are associated with the observed entries of , and add a  regularizer to get the objective:Despite the simplicity of the method, authors find that this does better than restricted Boltzmann machines (RBMs) and local low-rank matrix (LLORMA)."}},"/proba-ml/statistics/bayesian-statistics":{"title":"4.6 Bayesian statistics","data":{"":"Inference is modeling uncertainty about parameters using a probability distribution (instead of a point estimate). In Bayesian statistics we use the posterior distribution.Once we have computed the posterior over , we compute the posterior predictive distribution by marginalizing out . In the supervised setting:This is Bayes model averaging (BMA), since we use an infinite amount set of models, weighted by how likely their parameters are.","461-conjugate-prior#4.6.1 Conjugate prior":"A prior  is conjugate for a likelihood function  if the posterior . The family  is closed under bayesian updating.For simplicity, we consider a model without covariates.","462-beta-binomial-model#4.6.2 Beta-binomial model":"4.6.2.1 Bernoulli likelihoodWe toss a coin  times to estimate the head probability . We want to compute .Under the iid hypothesis:4.6.2.2 Binomial likelihoodInstead of observing a sequence of coin toss, we can count the number of heads using a binomial likelihoodSince these likelihood are proportional, we will use the same inference about  for both models.4.6.2.3 PriorThe Beta distribution is the conjugate prior to Bernoulli or Binomial likelihood.4.6.2.4 PosteriorIf we multiply the Bernoulli likelihood with the Beta prior, we obtained the following posterior:Here the hyper-parameters of the prior  play an similar role to the sufficient statistics . We call  pseudo counts and  observed counts.4.6.2.5 ExampleIf we set , it would means that we have already observed 2 heads and 2 tails before we see the actual data. This is a weak preference for the value of  (small update in figure (a) below).If we set , the prior become . The prior is uninformative, so there is no update.4.2.2.6 Posterior mode (MAP estimate)Similarly to the MAP estimate already we saw in the regularization section, we haveIf we use , we find back the add-one smoothing.If we use , we have 4.6.2.7 Posterior meanThe posterior mean is a more robust estimation than the posterior mode since it integrates over the whole space (instead of a single point).If , the posterior mean is:with  the strength of the posterior.The posterior mean is a convex combination of the prior mean,  and the MLE: 4.6.2.8 Posterior varianceThe variance of a Beta posterior is given by:Thus, the standard error of our estimate (aka the posterior variance) is:4.6.2.9 Posterior predictiveFor the Bernoulli distribution, the posterior predictive distribution has the form:For the Binomial distribution, the posterior predictive distribution has the form of the Beta-Binomial distribution:We plot in figure (a) below the posterior predictive distribution for , with a uniform prior .(Remember that  and )We plot in figure (b) the plugin approximation, where  is directly injected into the likelihood: The long tail of the Bayesian approach is less prone to overfitting and Black Swan paradoxes. In both case, the prior is uniform, so this Bayesian property is due to the integration over unknown parameters.4.6.2.10 Marginal likelihoodThe marginal likelihood for a model  is defined as:Since it is constant in , it is not useful for parameter inference, but can help for model or hyperparameters selection (aka Empirical Bayes)The marginal likelihood for the Beta Binomial is:soThe marginal likelihood of the Beta Bernoulli is the same as above without the  term.4.6.2.11 Mixture of conjugate priorsOur current prior is rather restrictive. If we want to represent a coin that may be fair but also be equally biased towards head. We can use a Mixture of beta distributions:We introduce  meaning that  comes from mixture , so the mixture prior is:We can show that the posterior can also be written as a mixture of conjugate distributions:where the posterior mixing weights are:We can compute this quantity using the equation (40) of the marginal likelihood. If we observe  heads and  tails, the posterior is:We can compute the probability that the coin is biased towards head:","463-the-dirichlet-multinomial-model#4.6.3 The Dirichlet-multinomial model":"We generalize the results from binary variable to categories (e.g. dice).4.6.3.1 LikelihoodLet  be a discrete random variable drawn from categorical distribution. The likelihood has the form:4.6.3.2 PriorThe conjugate prior is a Dirichlet distribution, which is a multivariate of a beta distribution. It has support over the probability simplex:The pdf of Dirichlet:with the multivariate Beta function:4.6.3.3 PosteriorThe posterior is:The posterior mean is:The posterior mode (which corresponds to the MAP) is:If , corresponding to a uniform prior, the MAP becomes the MLE:4.6.3.4 Posterior predictiveThe posterior predictive distribution is given by:For a batch of data to predict (instead of a single point), it becomesWhere the denominator and the numerator are marginal likelihoods on the training and training + future test data.4.6.3.5 Marginal likelihoodThe marginal likelihood of the Dirichlet-categorical are given by:","464-gaussian-gaussian-model#4.6.4 Gaussian-Gaussian model":"We derive the posterior for the parameters for a Gaussian distribution, assuming that the variance is known for simplicity.4.6.4.1 Univariate caseWe can show that the conjugate prior is also Gaussian. We then apply the Bayes rule for Gaussian, with the observed precision  and the prior precision . The posterior is:The posterior mean is a convex combination of the empirical mean and the prior mean.To gain more intuition, after seeing  point, our posterior mean is:The second equation is the prior mean adjusted towards the data y.The third equation is the data adjusted towards the prior mean, called a shrinkage estimate.For a Gaussian estimate the posterior mean and posterior mode are the same, thus we can use the above equations to perform MAP estimation.If we set an uninformative prior  and approximate the observed variance  bythen the posterior variance of our mean estimate is:We see that uncertainty in  reduces at the rate .Because 95% of the Gaussian distribution is contained within 2 standard deviations of the mean, the 95% credible interval for  is:4.6.4.2 Multivariate caseFor -dimensional data, the likelihood has the form:Thus we replace the set of observations with their mean and scale down the covariance.We obtain the same expression for the posterior of the mean than in the univariate case.","465-beyond-conjugate-priors#4.6.5 Beyond conjugate priors":"For most models, there is no prior in the exponential family that conjugates to the likelihood. We briefly discuss various other kinds of prior.4.6.5.1 Non informative priorWhen we have little to no domain specific knowledge, we prefer ‚Äúlet the data speak for itself‚Äù (we get closer to the frequentist statistics).We can for example use a flat prior , which can be viewed as an infinitely wide Gaussian.4.6.5.2 Hierarchical priorThe parameters of the prior  are called hyper-parameters . If there are unknown, we can put a prior on them: this defines a hierarchical Bayesian model.If we set a non informative prior on , the joint distribution has the form:We aim at learning  by treating the parameters as data points. This is useful when we want to estimate multiple related parameters from different subpopulations.4.6.5.3 Empirical priorThe joint distribution of the hierarchical prior can be challenging to compute.Instead, we make a point wise estimation of the hyper-parameters by maximizing the marginal likelihood:We then compute the conditional posterior  in the usual way.This violates the principle that the prior should be chosen independently of the data, but we can view it as a computationally cheap approximation of the hierarchical prior.The more integral one performs, the ‚Äúmore Bayesian‚Äù one becomes:Note that ML-II is less likely to overfit than ML since there are often fewer hyper-parameters  than parameters .","466-credible-intervals#4.6.6 Credible intervals":"The posterior distribution is hard to represent in high dimension. We can summarize it via a point estimate (posterior mean or mode) and compute the associated credible interval (which is different from the frequentist confidence interval).The Credible Interval for  is:If the posterior has a known functional form, we can compute its inverse cdf to find the interval:  and .In general, finding the inverse cdf is hard, so instead we rank samples from the posterior distribution and select our target percentiles.Sometimes, points that are outside the interval have higher density probability than inner points.We use the highest posterior density (HPD) to counter this, by finding the threshold :and then defines the HPD as:","467-bayesian-machine-learning#4.6.7 Bayesian machine learning":"So far, we have focused on unconditional models of the form .In supervised machine learning, we focus on conditional models of the form .Our posterior becomes  where . This approach is called Bayesian machine learning.4.6.7.1 Plugin approximationOnce we have computed the posterior over the , we can compute the posterior predictive distribution over  given :Computing this integral is often intractable. Instead, we approximate that there is a single best model , such as the MLE.Using the plugin approximation, the predictive distribution is now:This is the standard machine learning approach of fitting a model  then making prediction.However, as this approximation can overfit, we average over a few plausible parameters values (instead of the fully Bayesian approach above). Here are 2 examples:4.6.7.2 Example: scalar input, binary outputTo perform binary classification, we can use a logistic regression of the form:In other words:We find the MLE  for this 1d logistic regression and use the plugin approximation to get the posterior predictive The decision boundary is defined as:We capture the uncertainty by approximating the posterior .Given this, we can approximate the mean and the 95% credible interval of the posterior predictive distribution using a Monte Carlo approximation:where  is a posterior sample.We can also compute a distribution over the decision boundary:where 4.6.7.3 Exemple: binary input, scalar outputWe now predict the delivery time  for a package from company A () or company B().Our model is:The parameters are  so we can fit this model using MLE.However, the issue with the plug-in approximation  is that in the case of one single point of observation, we don‚Äôt capture uncertainty, our MLE for standard deviations are .Instead, the Bayesian approach using the Bayes rule for Gaussian allows to compute the posterior variance and the credible interval.","468-computational-issues#4.6.8 Computational issues":"Usually, performing the Bayesian update is intractable except for simple cases like conjugate models or latent variables with few possible values.We perform approximate posterior inference for a Beta-Bernoulli model:with  consisting in 10 heads and 1 tails, with a uniform prior.4.6.8.1 Grid approximationThe simplest approach is to partition  in a finite set  and approximate the posterior by brute-force enumeration:This captures the skewness of our posterior, but the number of grid points scales exponentially in high dimension.4.6.8.2 Quadratic (Laplace) approximationWe approximate the posterior using a multivariate Gaussian as follow:where  is the normalization constant and  is called an energy function.Performing a Taylor series around the mode  (i.e. the lowest energy state), we get:where  is the gradient at the mode and  the Hessian. Since  is the mode, the gradient is zero.Hence:The last line follows from the normalization constant of the MVN.This is easy to apply, since after computing the MAP estimate, we just need to compute the Hessian at the mode (in high dimensional space, we use a diagonal approximation).However we see that the Laplace approximation fits poorly, because the posterior is skewed whereas Gaussian is symmetric.Additionally, the parameter of interest lies in the constraint interval  whereas the Gaussian assumes an unconstrained space . This can be solved later by a change of variable .4.6.8.3 Variational inference (VI)VI is another optimization base posterior approximation. It aims at minimizing some discrepancy  between our posterior distribution  and a distribution  from some tractable family (like MVN):If  is the KL divergence, we can derivate the evidence lower bound (ELBO) of the marginal likelihood. Maximizing the ELBO improves the posterior approximation.4.6.8.4 Markov Chain Monte Carlo (MCMC)Although VI is fast, it can give a biased approximation since it is restricted to a specific function form .Monte Carlo is a more flexible, non-parametric approach:The key issue is to create posterior samples  without having to evaluate the normalization constant MCMC ‚Äîand Hamiltonian Monte Carlo (HMC)‚Äî speeds up this method by augmenting this algorithm with gradient based information derived from ."}},"/proba-ml/recommender-systems/intro":{"title":"22. Recommender Systems","data":{"":"Recommender systems recommend items (such as movies, books, ads) to users based on various information, such as their past viewing / purchasing behavior (e.g. which movies they rated high or low, which ads they clicked on), as well as optional ‚Äúside information‚Äù such as demographics about the user, or information about the content of the item (e.g., its title, genre or price)."}},"/proba-ml/recommender-systems/implicit-feedback":{"title":"22.2 Implicit feedback","data":{"":"So far, we have assumed that the user gives explicit ratings for each item they interact with, which is very restrictive.More generally, we would like to learn from the implicit feedback that users give by interacting with a system. For instance, we could treat all movies watched by a user as positive, and all the others as negative. We end-up with a sparse, positive-only ratings matrix.Alternatively, we can consider a user watching item  but not item  as an implicit signal that they prefer   over \n. The resulting data can be represented as a set of tuples , where  is a positive pair, and  is a negative, unlabeled pair.","2221-bayesian-personalized-ranking#22.2.1 Bayesian personalized ranking":"To fit a model of the form , we need to use a ranking loss, so that the model ranks  ahead of  for user .A simple way to do it is to use a Bernoulli form of the data:If we combine this with a Gaussian prior for , we get the following MAP estimation problem:where , where  is the set of items user  selected, and  are all the items.This is known as Bayesian personalized ranking (BPR).As an example, if  and the user  chose to interact with , we get the following implicit item-item preference matrix:where  means user  prefer item  over , and  means that we can‚Äôt determine its preferences.When the set of possible items is large, the number of negative in  can be large. Fortunately, we can approximate the loss by subsampling negatives.Note that an alternative to the log-loss above is to use the hinge loss, similar to the approach used for SVMs:where   is the safety margin. This ensures the negative items   never scores more than  higher than the positive items .","2222-factorization-machines#22.2.2 Factorization machines":"The AutoRec approach of section 22.1.4 is nonlinear but treats users and items asymmetrically.We start with a linear and symmetrical discriminative approach, by predicting the output (such as a rating) of a user-item one-hot-encoded pair:where ,  is the number of inputs,  is a weight matrix,  is a weight vector, and  is a global offset.This is known as a factorization machine (FM), and is a generalization of the equation in section 22.1.3:since  can handle information beyond user and item.Computing the generalized equation takes  time since it considers all pairwise interactions between users and items.Fortunately, we can rewrite this to compute it in  as follows:For sparse vectors, the complexity is linear in the number of nonzero components, so here it is just .We can fit this model to minimize any loss we want, e.g., MSE loss if we have explicit feedbacks, ranking loss if we have implicit feedbacks.Deep factorization machines combine the above with an MLP applied to a concatenation of the embedding vectors, instead of the inner product. The model has the form:The idea is that the bilinear FM model captures explicit interactions between users and features, while the MLP captures implicit relation between user features and item features, which allows the model to generalize.","2223-neural-matrix-factorization#22.2.3 Neural matrix factorization":"The **Neural matrix factorization** model is another way to combine bilinear model with neural nets.The bilinear part is used to define the generalized matrix factorization (GMF) pathway, which computes the following feature for user  and item :where  and  are user and item embedding matrices.The DNN part is a MLP applied to the concatenation of other embedding vectors:Finally, the model combines these to get:The model is trained on implicit feedback, where  if the interaction is observed,  otherwise. However it could be trained to minimize BPR loss."}},"/proba-ml/recommender-systems/leveraging-side-information":{"title":"22.3 Leveraging side information","data":{"":"So far, we have assumed that the only information available to the predictor are the id of the user and the id of the item.This is an extremely impoverished representation, and will fail to work if we encounter a new user or a new item (the cold start problem). To overcome this, we need to leverage ‚Äúside information‚Äù.For items, we often have rich meta-data, such as text (titles, descriptions), images, high-dimensional categorical variables (e.g. location), or scalar (e.g. price).For users, the side information depends on the specific form of the interactive system.\nFor search engines, it is the list of queries the user has issued, and information derived from websites they have visited (tracked by cookies).\nFor online shopping sites, it is the list of searches plus past viewing and purchasing behavior.\nFor social network, there is information about the friendship graph of each user.\nIt is very easy to capture this side information in the factorization machines framework, by expanding our definition of  beyond the two one-hot vectors.In addition to features about the user and item, there may be other contextual features, such as the time of interaction. The sequence of the most recently viewed items is often also a useful signal.The Covolutional Sequence Embedding Recommendation (Caser) captures this by embedding the last  items, and then treating the  input as an image, by using a covolutional layer as part of the model.See this review."}},"/proba-ml/statistics/frequentist-statistics":{"title":"4.7 Frequentist statistics","data":{"471-sampling-distribution#4.7.1 Sampling distribution":"In frequentist statistics, uncertainty is represented by the sampling distribution on an estimator.\nAn estimator is a decision procedure mapping observed data to an action (here the action is a parameter vector).We denote it by , where  can be the MLE, MAP estimate or MOM estimate.\nThe sampling distribution is the distribution of results if we applied the estimator multiple times to different datasets from some distributionWe sample  different datasets of size  from some true model :For brevity, we denote it \nIf we apply the estimator to each :We typically need to approximate it with Monte Carlo.","472-gaussian-approximation-of-the-sampling-distribution-of-the-mle#4.7.2 Gaussian approximation of the sampling distribution of the MLE":"The most common estimator is the MLE.When the sample size becomes large, the sampling distribution of the MLE becomes Gaussian: is the Fisher information matrix (FIM). It measures the amount of curvature of the log-likelihood at its peak.One can show that the FIM is also the Hessian of the NLL:A log-likelihood function with high curvature (a large Hessian) will result in a low variance estimate since the parameters are well determined by the data.","473-bootstrap-approximation-of-the-sampling-distribution-of-any-estimator#4.7.3 Bootstrap approximation of the sampling distribution of any estimator":"When the estimator is a complex function of the data (not jus the MLE) or when the sample size is small, we can approximate the sampling distribution using a Monte Carlo technique called the bootstrap.\nThe parametric bootstrap\nCompute \nUse it as plugin to create  datasets of size :\nCompute our estimator for each sample, . This empirical distribution is our estimate of the sampling distribution\nThe non-parametric bootstrap\nSample  points from  with replacement, this create \nCompute our estimator as for each sample and draw the empirical distribution.\nNote that on average, a sample only has 63.2% of unique data point, since the probability that an item is picked at least once is , which converges to \nThe bootstrap is a ‚Äúpoor man‚Äôs‚Äù posterior. In the common case where the estimator is a MLE and the prior is uniform, they are similar.","474-confidence-intervals#4.7.4 Confidence intervals":"We use the variability induced by the sampling distribution to estimate the uncertainty of an a parameter estimate. We define a -confidence interval as:where the hypothetical data  is used to derives the interval .If , this means that if we repeatedly sample data and compute , 95% of such intervals would contains the parameter Suppose   is the unknown true parameter but we know :By rearranging, we find  is a   CI.In most cases, we assume a Gaussian approximation to the sampling distribution:and thus we can compute and approximate CI using:where  is the  quantile of the Gaussian CDF and  is the estimate standard error.If the Gaussian approximation is not satisfactory, we can bootstrap the empirical distribution  as an approximation to .","475-confidence-intervals-are-not-credible#4.7.5 Confidence intervals are not credible":"A frequentist 95% CI is defined as an interval such that .If I repeat the experiment over and over, then 95% of the time the CI contains the true mean.It doesn‚Äôt mean that the parameter is 95% likely to live in the interval given by the observed data.That quantity is instead given by the credible interval These concepts are different: in the frequentist view,  is treated as a unknown, fixed constant, and the data as random. In the Bayesian view, the data is fixed (as it is known) and  is random.","476-the-bias-variance-tradeoff#4.7.6 The bias-variance tradeoff":"In frequentist, data is a random variable drawn from some true but unknown distribution . So the estimator has a sampling distribution The bias is defined as:The MLE of the Gaussian mean is unbiased , but if  is not known, the MLE of the Gaussian variance is biased:Intuitively, this is because we use up one point to get the mean. The unbiased estimator for the Gaussian variance is:The bias variance tradeoff is given by:It can be wise to use a biased estimator as long as it reduces the variance by more than the square of the bias.MAP estimator for a Gaussian meanSuppose we want to estimate the mean of a Gaussian from .The MLE is unbiased and has a variance of The MAP under a prior of the form  is:The bias and variance are given by:MAP estimator for linear regressionMAP estimator for classificationIf we use a 0-1 loss instead of the MSE, the frequentist risk is now . If the estimate is on the correct side of the classification, then the bias is negative, and decreasing the variance will decrease the misclassification rate.However, if the estimate is wrong, the bias is positive and it pays to increase the variance. This illustrates that it is better to focus on the expected loss in classification, not on the bias and variance."}},"/proba-ml/statistics/other-estimation-methods":{"title":"4.4 Other estimation methods","data":{"441-the-methods-of-moment-mom#4.4.1 The methods of moment (MOM)":"MOM is a simpler approach than computing MLE. Solve  number of equations ( is the number of parameters).\nThe theoretical moments are given by \nThe empirical moments are given by \nWe solve  for every k.Example with the univariate Gaussian: (since )so:So in this case, this is similar to MLE.Example with the uniform distribution:Inverting these equations to get  and , we see that these estimators sometimes give incorrect results.To compute the MLE, we sort the data values. The likelihood is:Then, if we set This is minimized by  and , as one would expect.","442-online-recursive-estimation#4.4.2 Online (recursive) estimation":"When data arrives sequentially we perform online learning.Let  our estimate (e.g. MLE) given . To ensure our algorithms takes a constant time per update, our solution is in the form:Example for the mean of Gaussian:This is a moving average, the size of the correction diminish over time. However if the distribution is changing, we might want to give more weight to recent data points.This is solved by exponentially-weighted moving average (EWMA)with The contribution of a data point in k steps is .Since the initial estimate starts from  there is an initial bias, corrected by scaling as"}},"/proba-ml/statistics/empirical-risk-minimization":{"title":"4.3 Empirical risk minimization (ERM)","data":{"":"We can generalize the MLE by replacing the loss loss term with any other loss to get:","431-misclassification-rate#4.3.1 Misclassification rate":"Let  our true label and  our predictionWe can define the loss as the misclassification rate","432-surrogate-loss#4.3.2 Surrogate loss":"Unfortunately, the 0-1 loss is non smooth, making it NP-hard to optimize. Here we use a surrogate loss function, a maximally tight convex upper bound, easy to minimize.For exemple, letwhere  is the logs oddsThe log loss isThis is a smooth upper bound to the 0-1 loss where  is the margin (it defines a ‚Äúmargin of safety‚Äù away from the threshold at 0)"}},"/proba-ml/statistics/intro":{"title":"4. Statistics","data":{"41-intro#4.1 Intro":"We estimate probability models parameters  from data . Most methods are optimizations of the form:"}},"/proba-ml/statistics/maximum-likelihood-estimation":{"title":"4.2 Maximum likelihood estimation (MLE)","data":{"421-definition#4.2.1 Definition":"Pick the parameter estimation assigning the highest probability to the training data, defined as:Wit the i.i.d. assumptions this becomes:And the Negative Log Likelihood (since most optimization algorithms are designed to minimize cost functions)With","422-justification-for-mle#4.2.2 Justification for MLE":"MLE can be viewed as a point estimation to the Bayesian posterior So  if the prior is uniform.Another way to see the MLE is that the resulting predictive distribution  is as close as possible to the empirical distribution of the data.If we defined the empirical distribution byMinimizing the KL divergence between the empirical  and an estimated distribution  is equivalent to minimizing the NLL and therefore computing the MLE.The same logic applies for supervised settings, with:","423-mle-for-the-bernoulli-distribution#4.2.3 MLE for the Bernoulli distribution":"Let  be the probability of heads in a coin toss.The MLE can be found by:","424-mle-for-the-categorical-distribution#4.2.4 MLE for the categorical distribution":"Let .To compute the MLE, we have to minimize the NLL subject the constraint  using the following Lagrangian:We get the MLE by taking the derivative of  for  and","425-mle-for-the-univariate-gaussian#4.2.5 MLE for the univariate Gaussian":"Suppose . We estimate here again the parameters using the MLE:We find the stationary point by  and  and  are the sufficient statistics of the data, since they are sufficient to compute the MLE.The unbiased estimator for variance (not the MLE) is:","426-mle-for-mvn#4.2.6 MLE for MVN":"with  the precision matrix.Using :HenceSo the MLE of  is just the empirical mean.Using the trace trick:With  the scatter matrix centered on Resolving the derivative for  gives","427-mle-for-linear-regression#4.2.7 MLE for linear regression":"Let suppose the model corresponds to . If we fix  to focus on :Dropping the irrelevant additive constants:Note that  and Writing the  in matrix notation:And the equation of OLS is:"}},"/proba-ml/trees-forests-boosting/bagging":{"title":"18.3 Bagging","data":{"":"Bagging stands for ‚Äúbootstrap aggregating‚Äù . This is a simple form of ensemble learning in which we fit  base models to different randomly sampled version of the data. This encourages the different models to make diverse predictions.The datasets are sampled with replacement (this is bootstrap sampling), so a given example may appear multiple time until we have a total of  examples per model ( being the size of the original dataset).The disadvantage of bootstrap is that on average, base models only see 63% of the unique dataset. Indeed, the probability for a point to not be selected from a set of size , in  draws iswhich means only  data points are selected.The remaining training instances are called out-of-bag (OOB). We can use the predicted performances of the model on the OOB set as an estimate of test set performances, as an alternative to cross-validation.The main advantage of bootstrap is preventing the ensemble to rely too much on individual train point, which enhances robustness and generalization.This advantage increases with the size of the ensemble, with the tradeoff of taking more time and memory to train.Bagging doesn‚Äôt always improve performance, and stable models like nearest neighbors classifiers won‚Äôt benefit much from it.For neural networks, things are more nuanced. They are unstable wrt the training set, however they will underperform if they only see 63% of the training set. So, bagged DNNs do not usually work well."}},"/proba-ml/statistics/regularization":{"title":"4.5 Regularization","data":{"":"The main issue with MLE and ERM is that they pick parameters by minimizing the loss on the training set, which can result in overfitting.With enough parameters, a model can match any empirical distribution. However, in most cases the empirical distribution is not the same as the true distribution. Regularization add a penalty term to the NLL.by taking the log loss and the log prior for penalty term.This is equivalent to minimizing the MAP:","451-map-for-bernoulli-distribution#4.5.1 MAP for Bernoulli distribution":"If we observe just one head, . We add a penalty to  to discourage extreme values.With , and , we encourage values of  near to the mean With the same method as the MLE, we find:By setting , we resolve the zero-count problem.Regularization combines empirical data and prior knowledge.","452-map-for-mvn#4.5.2 MAP for MVN":"We showed that the MLE for covariance of a MVN is . In high dimensions, the scatter matrix can easily become singular. One solution is to perform MAP estimation.A convenient prior to use for  is the Wishart distribution (generalization of the Gamma distribution). The resulting MAP is:where  controls the amount of regularization.If we set :Off-diagonal elements are shrunk toward 0.  can be chosen via cross-validation of via close form (implemented in scikit-learn LedoitWolf).The benefits of the MAP is to better behaved than MLE, with eigenvalues of the estimate closer to that of the true covariance matrix (the eigenvectors however are unaffected).","453-weight-decay#4.5.3 Weight decay":"We saw that reducing the degree of the polynomial help the regression task. A more general solution is to add regularization to the regression coefficients.where a higher  means more pressure toward low coefficients and thus less flexibility for the model. This is ridge regression.","454-picking-ùù∫-by-cross-validation#4.5.4 Picking ùù∫ by cross validation":"We split the training data into  folds and iteratively fit on all fold except one, used for validation.We haveSoFinallyThe standard error of our estimate is:where","457-using-more-data#4.5.7 Using more data":"When a model is too simplistic, it fails to improve as the data grow. A overly complex model will end up converging toward the right solution if enough data is provided."}},"/proba-ml/trees-forests-boosting/ensemble-learning":{"title":"18.2 Ensemble learning","data":{"":"We saw that decision tree can be quite unstable, in the sense that their predictions might vary a lot with a small perturbation in the input data. They are high variance estimators.A simple way to reduce the variance is to average over multiple models. This is called ensemble learning. The result model has the form:where  is the th base model.The ensemble will have similar bias to the base models, but lower variance, generally resulting in better overall performances.Averaging is a sensible way to combine predictions from regression models. For classifiers, we take a majority vote of the outputs (called committee method)To see why this can help, suppose each base model is a binary classifier with accuracy , and suppose 1 is the correct class. Let  be the prediction for the th model and  the number of class of vote for class 1.We define the final predictor to be the majority vote, i.e. class 1 if  and class 0 otherwise. The probability that the ensemble will pick class 1 is:where  is the cdf of the Binomial distribution evaluated in .For  and , we get . With  we get .The performance of the voting approach is dramatically improved because we assume each predictor made independent errors. In practice, their mistakes might be correlated, but as long as we ensemble sufficiently diverse models, we still can come ahead.","1821-stacking#18.2.1 Stacking":"An alternative to using unweighted average or majority vote is to learn how to combine the base models, using stacking or ‚Äústacked generalization‚Äù:We need to learn the combination weight on a separated dataset, otherwise all their mass will be put on the best performing base model.","1822-ensembling-is-not-bayes-model-averaging#18.2.2 Ensembling is not Bayes model averaging":"Note that an ensemble of models is not the same as using BMA. An ensemble considers a larger hypothesis class of the form:whereas BMA uses:The key difference is that the BMA weights  sum to one, and in the limit of infinite data, only a single model will be chosen (the MAP model). In contrary, the ensemble weights  are arbitrary and don‚Äôt collapse in this way in a single model."}},"/proba-ml/trees-forests-boosting/classification-and-regression-tree":{"title":"18.1 Classification and regression tree (CART)","data":{"":"Classification and regression trees (also called CART models or decision trees) are defined by recursively partitioning the input space and defining a local model on each resulting region of the input space.The overall model can be represented by a tree, with one leaf per region.","1811-model-definition#18.1.1 Model definition":"We start by considering regression tree.Trees consist of a set of nested decision rules. At each node , the feature  of the input vector  is compared to a threshold : if it‚Äôs lower, the input flows to the left branch, otherwise to the right.At the leaves of the tree, the model specify the predicted output for any input that falls in this region.In the above example, the first region of space is defined by:The overall result is that we partition the 2d input space into 5 regions. We can now associate a mean response with each of these regions, resulting in a piecewise constant surface.The output from region 1 can be estimated using:Formally, a regression tree can be defined as:where  is the predicted output for the node  and  where  is the number of nodes. We can express the regions as .For classification problems, the leaves contains a distribution over the class labels rather than the mean response.","1812-model-fitting#18.1.2 Model fitting":"To fit the model, we need to minimize the following loss:Unfortunately, this is not differentiable because of the need to learn the tree structure. Finding the optimal partitioning of the data is NP-complete.The standard practice is to use a greedy procedure to grow the tree one node at a time. This approach is used by CART, C4.5 and ID3 which are three popular implementations of the method. Scikit-learn uses an optimized version of CART.The idea is the following. Suppose we are at node ; let  be the set of examples that reach this node. We consider how to split this node into a left branch and a right branch to minimize the error in each child subtree.If the th feature is a real-valued scalar, we can partition  by comparing to a threshold . The set of possible thresholds  for feature  can be obtained by sorting the unique values of .For each possible threshold we define the left and right splits:If the th feature is categorical, with  possible values, we check if the feature if the feature is equal to each of those values or not. This defines a  set of possible binary splits:Alternatively, we could allow multi-way split, but this could lead to data fragmentation with too little data falling into each subtree, leading to overfitting. Therefore it is more common to use binary splits.Once we have computed  and  for each  and  at node , we choose the best feature  to split on, and the best value for this feature  as follows:We then partition the data and call the fitting algorithm recursively on each subset of the data.We now discuss the cost function  which is used to evaluate the cost of node .For regression, we could use the mean squared error:where  is the mean of the response variable for examples reaching node .For classification, we begin by computing the empirical distribution over class labels for this node:Given this, we can compute the Gini index:This is the expected error rate, since  is the probability for a random entry in the leaf belong to class , and  the probability it would be misclassified.Alternatively we can define the cost as the entropy or deviance of the node:A node that is pure (i.e. only has example of one class) will have 0 entropy.","1813-regularization#18.1.3 Regularization":"If we let the tree become deep enough, it can reach an error 0 (assuming no label noise) by partitioning the input space is sufficiently small regions. This leads to overfitting, and there are two main regularization approaches:\nStop the tree growing process using some heuristic, like reaching a maximum depth or having too few sample at a node.\nGrow the tree to its maximum depth and prune it back, by merging split subtree into their parent. This can partially overcome the greedy nature of top-down tree growing. However, forward growing and backward pruning is slower than the greedy top-down approach.","1814-handling-missing-input-features#18.1.4 Handling missing input features":"In general, it is hard for discriminative models to handle missing input features. However, for trees, simple heuristics can work well.\nSurrogate splits leverages highly correlated features, inducing similar partition when the chosen variable is missing at test time. This can be thought as learning a local joint model of the input. This has the advantage over a generative model to not modeling the entire joint distribution of inputs, but has the disadvantage of being completely ad-hoc\nA simpler approach is to create a new ‚Äúmissing‚Äù category, and then treat data as fully observed.","1815-pros-and-cons#18.1.5 Pros and cons":"Tree models are popular for several reasons:\nThey are easy to interpret\nThey can easily handle mixed discrete and continuous inputs\nThey are insensitive to monotone transformations of the inputs (since splits are based on ranking the data points) so there is no need to standardize the data\nThey perform automatic variable selection\nThey are relatively robust to outliers\nThey are fast to fit, and scale well to large data sets.\nThey can handle missing value features\nHowever, the primary disadvantage of trees is that they don‚Äôt predict very accurately compared to other kind of model, due to the greedy nature of tree construction.A related problem is that they are unstable: small changes of the input data can have large effects on the structure of the tree, due to its hierarchical nature.Below, omitting a single training point can dramatically change the decision surface, due to the use of axis parallel split.We‚Äôll later see how this instability can be turned into a virtue."}},"/proba-ml/trees-forests-boosting/random-forests":{"title":"18.4 Random Forests","data":{"":"Bagging relies on the assumption that re-running the algorithme on different subsets of the data will result in sufficiently diverse base models.The random forests approach tries to decorrelate the base learners even further by learning trees based on a randomly chosen subset of input variables (at each node of the tree) , as well as a randomly chosen subset of data case.The figure above shows that random forest work much better than bagged decision trees, because in this spam classification dataset, many input features are irrelevant.We also see that boosting works even better, but relies on a sequential fitting of trees, whereas random forests can be fit in parallel.When it comes to aggregating the predictions of base models, note that the scikit-learn random forests implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.See scikit-learn comparison of random forests and gradient boosting trees."}},"/apply-2022/fire-chat-1":{"title":"27. Fire chat: Is ML a subset or superset of programming? Mike & Martin","data":{"":"https://www.youtube.com/watch?v=urx00Wfm4dw&ab_channel=Tecton\nWhere is ML delivering and undelivering?Continuum of views:\nThe most bearish: ‚ÄúAI/ML is nothing new, just expensive regression‚Äù\nThe most bullish: ‚ÄúSo powerful we won‚Äôt need theories anymore‚Äù\nWhere in this continuum does reality lies?There‚Äôs absolutely something innovative and new. Implications on consumption layer and world understanding.What we don‚Äôt know is how far that goes. This doesn‚Äôt cut humans out of the loop, not the end of the theory.\nIntermediate steps?Some companies come in and tell the ‚Äúend of theory‚Äù story. Try to use AI to find PMF. It‚Äôs too bullish.More pragmatic approach: documents are tremendously unstructured, some rule-based approaches to solve this problem. And we‚Äôre going to apply this domain where they‚Äôre buyers.\nIs ML fundamentally different from software eng?How do you break down the AI market? Analytics (computer-aided decision making) and operational MLAnalytics is a natural extension of traditional analytics. The assumption is that the output of dashboards and BI is to be consumed by humanOn the operational one, AI can be anywhere in the program being used.Otherwise, ML is fundamentally different. When it comes to traditional system building, we‚Äôre dealing with finite state machines, and control it, control the complexity. For ML it is different, complexity of the natural universe. The state spaces are huge. No tool can manage that, new skillsets and new tools are required.\nDoes ML system handle continuous state?Computers are the right architecture for everything. The nature of problems we‚Äôre solving requires more than software. AI is part of the system, so ML is a superset.Some set of approaches to handle data that is very messy, you‚Äôre supposed to put structure on that.\nAre there different layers on the ML stack?You need people to understand this stuff. The role for someone that can extract features to build software. Skillset much matches a set of tooling to help them. Insights from these teams translate to traditional system building\nFor the average MLE or DS, what does it mean for them, does that boil down to adding structure to datasets where there is no structure? What skills to be great at?You never really have a raw data source. Adding structure to unstructured data is not refining raw data. Being really good at figuring out how to extract info from a dataset, also higher-level reasoning ‚Äúwhat dataset could be out there, and what could I extract‚Äù, and this skill won‚Äôt come from an AutoML tool. Having a solid mental of causality.Ultimately you‚Äôre trying to build predictions, there are some domains where it doesn't apply like sub-similarity. Some companies claim to build chatbots on some domains, and some realizes that they can answer less than 50% of queries.Data centers don‚Äôt handle traffic smartly but send it to random destinations. No matter the zoom level, the stochastic probabilities are the same. Before starting an ML project, ask whether ML makes sense in the first place.There are some problems where linear regression is fine. Some can‚Äôt be solved with ML. Other problems actually have the data and seem a good fit, but you don‚Äôt know until you know the distribution. Think more like scientists than engineers.\nAre there other signs to look for when we don‚Äôt know the distribution of the data, that can save us the 6 months of pain?In a practical perspective, when ML is not off the right start? When is it doom to fail?\nWhen you don‚Äôt have the right data of right skillset on your team\nDo you have level of organizational/executive support. Putting ML in prod today takes a lot of work, tool and team. Doing that is an investment. Make sure it‚Äôs both doable and worth it. The business value might not be high enough for ML.\nFor seed-stage and A-stage, building the ML infrastructure is a real challenge.\nDifficulty to put operational ML in production on the long term if often underestimate\nData products often allow companies to stand out from competition though"}},"/proba-ml/trees-forests-boosting/boosting":{"title":"18.5 Boosting","data":{"":"Ensemble of trees, whether fit using bagging or the random forest algorithm, corresponds to a model of the form:where  is the th tree and  is the corresponding weight, often set to .We can generalize this by allowing the  functions to be general function approximators, such as neural nets. The result is called additive model.We can think of this as a linear function with adaptative basis functions. The goal, as usual, is to minimize the empirical loss function.The original boosting algorithm sequentially fits additive models where each  is a binary classifiers that returns .\nWe first fit  on the original dataset, then we weight the data samples by the errors made by , so misclassified example get more weight.\nNext, we fit  on this weighted dataset.\nWe keep repeating this process until  number of components have been fitted (where  is a hyper-parameter that controls the complexity of the overall-model, and can be chosen by monitoring the performance on a validation dataset, via early-stopping).\nIt can be shown that as long as each  has an accuracy above chance level (even slightly above 50%), then the final ensemble of classifiers will have a higher accuracy that any individual .In other words, we can boost a weak learner  into a strong learner.Note that boosting reduces the bias of strong learner by fitting trees that depends on each other, whereas bagging and RF reduces the variance by fitting independent trees.In the rest of this section, we focus on boosting model with arbitrary loss function, suitable for regression, multiclass classification or ranking.","1851-forward-stagewise-additive-modeling#18.5.1 Forward stagewise additive modeling":"We sequentially optimize the empirical objective for differentiable loss functions. At iteration , we compute:where  is an additive model.We then set:Note that we don‚Äôt adjust the weight of the previously added models.This optimization step depends on the loss function that we choose, and (in some cases) on the form of the weak learner .","1852-quadratic-loss-and-least-square-boosting#18.5.2 Quadratic loss and least square boosting":"If we use a squared error loss, the th term in the objective at step  becomes:where  is the residual at step  of the th observation.We can minimize this objective by simply setting  and fitting  to the residual errors. This is called least square boosting.We see how each new weak learner that is added to the ensemble corrects the error made by the earlier version of the model.","1853-exponential-loss-and-adaboost#18.5.3 Exponential loss and AdaBoost":"Suppose we are interested in binary classification, i.e predicting . Let assume the weak learner computes:The negative log likelihood from section 10.2.3 gives us:We can minimize this by ensuring the margin  is as large as possible.We see in the figure below that the log loss is an upper bound on the 0-1 loss.In this section, we consider the exponential loss  instead, since it has the same optimal solution as the log loss when the sample size grows to infinity, but it is easier to optimize in the boosting setting.To see the equivalence with the log loss, we can set the derivative of the expected exponential loss to zero:We will now discuss how to solve for the th weak learner  when we use the exponential loss.\nWhen  returns a binary label, it is called discrete Adaboost\nWhen  returns a probability instead, it is called real Adaboost\nAt step , we have to minimize:whereis a weight applied to the th sample.We can rewrite the objective as follows:Consequently the optimal function to add is:This can be found by applying the weak learner to a weighted version of the dataset, with weights .All that remains is to solve for the size of the update , by substituting  into :whereAfter updating the strong learner, we recompute the weight for the next iteration:Since , we finally have:Since  is constant across all examples it can be dropped.If we then define  we have:We then see that we exponentially increase the weights of misclassified examples. This algorithm is known as Adaboost.A multiclass generalization of exponential loss, and an adaboost-like algorithm to minimize it is known as SAMME (stagewise additive modeling using a multiclass exponential loss function) ‚Äî this is the implementation of the Adaboost classifier in scikit-learn.","1854-logitboost#18.5.4 LogitBoost":"The trouble with exponential loss is that it puts a lot of weight on misclassified examples.In addition,  is not the logarithm of any pmf for binary variables , hence we can‚Äôt recover probabilities from .A natural alternative is to use log-loss, which only punishes mistakes linearly, and we‚Äôll be able to extract probability using:The goal is to minimize the expected log-loss:By performing Newton update on this update, similarly to IRLS, one can derive the algorithm below, known as logitBoost.The key subroutine is the ability for the weak learner to solve a weighted least square problem.","1855-gradient-boosting#18.5.5 Gradient Boosting":"Rather than deriving a new boosting algorithm for each loss function, we can derive a generic version known as gradient boosting.We solve  by performing gradient descent in the space of functions. Since functions are infinite dimensional objects, we will represent them by their values on the training set .At step , let  be the gradient of  evaluated at :Gradient of some common loss functions are given by:We then make the update:where  is the step length, chosen by:In its current form, this algorithm is not useful because it only optimize  on a fixed set of points, so we don‚Äôt learn a function that can generalize.Instead, we can modify this algorithm by fitting a weak learner to approximate of the negative  gradient signal. We use the update:The overall algorithm is:We have introduced a learning rate  to control the size of the updates, for regularization purposes.If we apply this algorithm using squared loss, we recover L2Boosting since For classification, we can use log-loss. In this case, we get an algorithm known as BinomialBoost. Its advantage over LogitBoost is that is doesn‚Äôt need to apply weighted fitting, it just applies any black-box regression model to the gradient vector.To apply this to multiclass classification, we can fit  separate regression trees, using the pseudo residual of the form:Although the trees are fit separately, their predictions are combined using softmax:When we have large datasets, we can use stochastic variant in which we subsample (without replacement) a random fraction of the data to pass the regression tree at each iteration.This is called stochastic gradient boosting. Not only is it faster, but it also better generalizes because subsampling the data is a form of regularization.18.5.5.1 Gradient tree boostingIn practice, gradient boosting nearly always assume the weak learner is a regression tree, which is a model of the form:where  is the predicted output for region  (in general  can be a vector). This combination is called gradient boosting regression trees. A relative version is known as MART (multivariate additive regression trees).To use this in gradient boosting, we first find good regions  for tree  using standard regresion tree learning on the residuals.Then, we (re)solve for the weight of each leaf by solving:For squared error, the optimal weight is just the mean of the residuals in that leaf.18.5.5.2 XGBoostXGBoost, which stands for extreme gradient boosting, is a very efficient and widely used implementation of gradient boosted trees, and add a few more improvements:\nIt adds a regularizer to the tree complexity\nIt uses a second-order approximation of the loss instead of a linear approximation\nIt samples features at internal nodes (as in random forests)\nIt ensures scalability on large datasets by using out-of-core methods\nOther popular implementations are LightGBM and CatBoost.XGBoost optimizes the following objective:where:is the regularizer,  is the number of leaves and  and  are regularization coefficients.At the th step, the loss is given by:We compute a second order Taylor expansion as follows:where  is the hessian:In the case of regression trees, we have , where  specifies which node the input  belongs to, and  are the leaf weights.We can rewrite the loss by removing elements that are independent of :with  is the set of indices of points assigned to leaf .This is quadratic in each , so the optimal weights are given by:The loss for evaluating different tree structures  then becomes:We can greedily optimize this using a recursive node splitting procedure. For a given leaf , we consider splitting it into a left and right partition .We can compute the gain (i.e. the reduction in loss) of this split:where  and Thus, we see that it is not worth splitting if the gain is negative (the first term is below"}},"/xhec":{"title":"Xhec","data":{"xhec-python-for-data-science#X/HEC Python for Data Science":"Full course","exercices#Exercices":"1. Numpy2. Pandas and scikit-learn3. skrub4. Classification trees","final-project#Final project":"Kaggle Challenge"}},"/proba-ml/trees-forests-boosting/interpreting-tree-ensembles":{"title":"18.6 Interpreting tree ensembles","data":{"":"Trees are popular because they are interpretable. Unfortunately, ensembles of trees lose that property.Fortunately, there are some simple methods to interpret what function has been learned.","1861-feature-importance#18.6.1 Feature importance":"For a single decision tree , we can consider the following measure of feature importance for feature :where the sum is over all the non-leaf (internal) nodes,  is the gain in accuracy (reduction in cost) at node , and  if node  uses feature .We can get a more accurate estimate by averaging over all trees of the ensemble:We can then normalize the scores so that the highest is 100%.However, there are two limitations of impurity-based feature importances:\nimpurity-based importances are biased towards high cardinality features;\nimpurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).\nInstead, scikit-learn suggests using permutation importance.","1862-partial-dependency-plot-pdp#18.6.2 Partial dependency plot (PDP)":"After we have identified the most relevant input features, we can try to assess the impact they have on the output.A partial dependency plot for feature  has the form:We plot  vs .Thus we marginalize out all features except . In the case of binary classifier we can convert the output in log odds before plotting.In figure a), we see that the probability of the spam increases when the frequency of ‚Äú!‚Äù and ‚Äúremove‚Äù increases.Conversely, this probability decreases when the frequency of ‚Äúedu‚Äù or ‚Äúhp‚Äù increases.We can also try to capture interaction effects between feature  and  by computing:In figure b), we see that the probability of spam increases when the frequency of ‚Äú!‚Äù increases, but much more so when the word ‚Äúhp‚Äù is missing.See scikit-learn documentation for computation methods."}}}